diff --git a/configs/common/CacheConfig.py b/configs/common/CacheConfig.py
index 3fa3676..7be1d02 100644
--- a/configs/common/CacheConfig.py
+++ b/configs/common/CacheConfig.py
@@ -41,15 +41,13 @@
 # Configure the M5 cache hierarchy config in one place
 #
 
-from __future__ import print_function
-
 import m5
 from m5.objects import *
 from Caches import *
 
 def config_cache(options, system):
     if options.external_memory_system and (options.caches or options.l2cache):
-        print("External caches and internal caches are exclusive options.\n")
+        print "External caches and internal caches are exclusive options.\n"
         sys.exit(1)
 
     if options.external_memory_system:
@@ -59,15 +57,15 @@ def config_cache(options, system):
         try:
             from cores.arm.O3_ARM_v7a import *
         except:
-            print("O3_ARM_v7a_3 is unavailable. Did you compile the O3 model?")
+            print "O3_ARM_v7a_3 is unavailable. Did you compile the O3 model?"
             sys.exit(1)
 
-        dcache_class, icache_class, l2_cache_class, walk_cache_class = \
-            O3_ARM_v7a_DCache, O3_ARM_v7a_ICache, O3_ARM_v7aL2, \
+        dcache_class, icache_class, l2_cache_class, l3_cache_class, walk_cache_class = \
+            O3_ARM_v7a_DCache, O3_ARM_v7a_ICache, O3_ARM_v7aL2, O3_ARM_v7aL3, \
             O3_ARM_v7aWalkCache
     else:
-        dcache_class, icache_class, l2_cache_class, walk_cache_class = \
-            L1_DCache, L1_ICache, L2Cache, None
+        dcache_class, icache_class, l2_cache_class, l3_cache_class, walk_cache_class = \
+            L1_DCache, L1_ICache, L2Cache, L3Cache, None
 
         if buildEnv['TARGET_ISA'] == 'x86':
             walk_cache_class = PageTableWalkerCache
@@ -81,8 +79,38 @@ def config_cache(options, system):
     # any more caches.
     if options.l2cache and options.elastic_trace_en:
         fatal("When elastic trace is enabled, do not configure L2 caches.")
+    if options.l3cache:
+        system.l3 = l3_cache_class(clk_domain=system.cpu_clk_domain,
+                                   size=options.l3_size,
+                                   assoc=options.l3_assoc)
+        system.tol3bus = L3XBar(clk_domain = system.cpu_clk_domain,
+                                     width = 32)
+        system.l3.cpu_side = system.tol3bus.master
+        system.l3.mem_side = system.membus.slave
+
+
+
+
+    #if options.l2cache and options.l3cache:
+
+     #   system.l2 = l2_cache_class(clk_domain=system.cpu_clk_domain,
+      #                             size=options.l2_size,
+       #                            assoc=options.l2_assoc)
 
-    if options.l2cache:
+      #  system.l3 = l3_cache_class(clk_domain=system.cpu_clk_domain,
+       #                            size=options.l3_size,
+       #                            assoc=options.l3_assoc)
+
+     #   system.tol2bus = L2XBar(clk_domain = system.cpu_clk_domain)
+     #   system.tol3bus = L3XBar(clk_domain = system.cpu_clk_domain)
+#
+ #       system.l2.cpu_side = system.tol2bus.master
+  #      system.l2.mem_side = system.tol3bus.slave
+
+   #     system.l3.cpu_side = system.tol3bus.master
+   #     system.l3.mem_side = system.membus.slave
+
+    elif options.l2cache:
         # Provide a clock for the L2 and the L1-to-L2 bus here as they
         # are not connected using addTwoLevelCacheHierarchy. Use the
         # same clock as the CPUs.
@@ -103,6 +131,14 @@ def config_cache(options, system):
                                   assoc=options.l1i_assoc)
             dcache = dcache_class(size=options.l1d_size,
                                   assoc=options.l1d_assoc)
+            if options.l3cache:
+                system.cpu[i].l2 = l2_cache_class(size=options.l2_size,
+                                                  assoc=options.l2_assoc)
+                system.cpu[i].tol2bus = L2XBar(clk_domain = system.cpu_clk_domain)
+                system.cpu[i].l2.cpu_side = system.cpu[i].tol2bus.master
+                system.cpu[i].l2.mem_side = system.tol3bus.slave
+        
+
 
             # If we have a walker cache specified, instantiate two
             # instances here
@@ -157,7 +193,10 @@ def config_cache(options, system):
                         ExternalCache("cpu%d.dcache" % i))
 
         system.cpu[i].createInterruptController()
-        if options.l2cache:
+        if options.l3cache:
+            system.cpu[i].connectAllPorts(system.cpu[i].tol2bus, system.membus)
+#            system.cpu[i].connectAllPorts(system.tol3bus, system.membus)
+        elif options.l2cache:
             system.cpu[i].connectAllPorts(system.tol2bus, system.membus)
         elif options.external_memory_system:
             system.cpu[i].connectUncachedPorts(system.membus)
diff --git a/configs/common/Caches.py b/configs/common/Caches.py
index 926a41d..a89666b 100644
--- a/configs/common/Caches.py
+++ b/configs/common/Caches.py
@@ -72,6 +72,16 @@ class L2Cache(Cache):
     tgts_per_mshr = 12
     write_buffers = 8
 
+class L3Cache(Cache):
+    size = '8192kB'
+    assoc = 64  #8
+    tag_latency = 32
+    data_latency = 32
+    response_latency = 32
+    mshrs = 32
+    tgts_per_mshr = 24
+    write_buffers = 16
+
 class IOCache(Cache):
     assoc = 8
     tag_latency = 50
diff --git a/configs/common/FSConfig.py b/configs/common/FSConfig.py
index aa00efe..eb5ac1a 100644
--- a/configs/common/FSConfig.py
+++ b/configs/common/FSConfig.py
@@ -560,7 +560,8 @@ def makeX86System(mem_mode, numCPUs=1, mdesc=None, self=None, Ruby=False):
     disk0 = CowIdeDisk(driveID='master')
     disk2 = CowIdeDisk(driveID='master')
     disk0.childImage(mdesc.disk())
-    disk2.childImage(disk('linux-bigswap2.img'))
+   # disk2.childImage(disk('linux-bigswap2.img'))
+    disk2.childImage(mdesc.disk())
     self.pc.south_bridge.ide.disks = [disk0, disk2]
 
     # Add in a Bios information structure.
diff --git a/configs/common/Options.py b/configs/common/Options.py
index aed8881..0bf0a40 100644
--- a/configs/common/Options.py
+++ b/configs/common/Options.py
@@ -85,10 +85,13 @@ def addNoISAOptions(parser):
     parser.add_option("--mem-ranks", type="int", default=None,
                       help = "number of memory ranks per channel")
     parser.add_option("--mem-size", action="store", type="string",
-                      default="512MB",
+                      default="256MB",
                       help="Specify the physical memory size (single memory)")
-
-
+    parser.add_option("--Ccache", type="string", help = "use countercache")
+    parser.add_option("--Ccache_size", type="string", default="8kB", help = "use countercache")
+    parser.add_option("--Ccache_assoc", type="int", default=2, help = "use countercache")
+    parser.add_option("--Ccache_blksize", type="string", default="64Byte", help = "use countercache")
+    parser.add_option("--Ccache_majpgsize", type="string", default="4KB", help = "use countercache")
     parser.add_option("--memchecker", action="store_true")
 
     # Cache Options
@@ -101,15 +104,18 @@ def addNoISAOptions(parser):
     parser.add_option("--num-dirs", type="int", default=1)
     parser.add_option("--num-l2caches", type="int", default=1)
     parser.add_option("--num-l3caches", type="int", default=1)
-    parser.add_option("--l1d_size", type="string", default="64kB")
+    #parser.add_option("--l1d_size", type="string", default="64kB") #default 
+    parser.add_option("--l1d_size", type="string", default="32kB")
     parser.add_option("--l1i_size", type="string", default="32kB")
-    parser.add_option("--l2_size", type="string", default="2MB")
+    #parser.add_option("--l2_size", type="string", default="2MB") #default
+    parser.add_option("--l2_size", type="string", default="512kB")
     parser.add_option("--l3_size", type="string", default="16MB")
     parser.add_option("--l1d_assoc", type="int", default=2)
     parser.add_option("--l1i_assoc", type="int", default=2)
     parser.add_option("--l2_assoc", type="int", default=8)
     parser.add_option("--l3_assoc", type="int", default=16)
     parser.add_option("--cacheline_size", type="int", default=64)
+    parser.add_option("--l3cache", action="store_true")
 
     # Enable Ruby
     parser.add_option("--ruby", action="store_true")
@@ -164,6 +170,8 @@ def addCommonOptions(parser):
     parser.add_option("-l", "--lpae", action="store_true")
     parser.add_option("-V", "--virtualisation", action="store_true")
 
+    parser.add_option("--fastmem", action="store_true")
+
     # dist-gem5 options
     parser.add_option("--dist", action="store_true",
                       help="Parallel distributed gem5 simulation.")
@@ -341,8 +349,6 @@ def addFSOptions(parser):
         parser.add_option("--enable-context-switch-stats-dump", \
                 action="store_true", help="Enable stats dump at context "\
                 "switches and dump tasks file (required for Streamline)")
-        parser.add_option("--generate-dtb", action="store_true", default=False,
-                    help="Automatically generate a dtb file")
 
     # Benchmark options
     parser.add_option("--dual", action="store_true",
diff --git a/src/mem/DRAMCtrl.py b/src/mem/DRAMCtrl.py
index fa04c9f..6d49e71 100644
--- a/src/mem/DRAMCtrl.py
+++ b/src/mem/DRAMCtrl.py
@@ -87,11 +87,11 @@ class DRAMCtrl(QoSMemCtrl):
 
     # threshold in percent for when to forcefully trigger writes and
     # start emptying the write buffer
-    write_high_thresh_perc = Param.Percent(85, "Threshold to force writes")
+    write_high_thresh_perc = Param.Percent(15, "Threshold to force writes")
 
     # threshold in percentage for when to start writes if the read
     # queue is empty
-    write_low_thresh_perc = Param.Percent(50, "Threshold to start writes")
+    write_low_thresh_perc = Param.Percent(0, "Threshold to start writes")
 
     # minimum write bursts to schedule before switching back to reads
     min_writes_per_switch = Param.Unsigned(16, "Minimum write bursts before "
@@ -235,6 +235,27 @@ class DRAMCtrl(QoSMemCtrl):
     # time to exit self-refresh mode with locked DLL
     tXSDLL = Param.Latency("0ns", "Self-refresh exit latency DLL")
 
+	# enable encryption
+    enable_encryption = Param.Unsigned(0, "enable encryption")
+    
+    # encryption method
+    encryption_method = Param.Unsigned(0, "method to perform encryption")
+
+	# tsecme encryption threshold
+	#tsecme_encrption_thresh = Param.Unsigned(0, "tsecme encrption threshold")
+	
+    # deuce method
+    deuce_epoch = Param.Unsigned(0, "deuce epoch length")
+
+    # non-volatile memory
+    nvm = Param.Unsigned(0, "emulates nvm")
+    nvm_write_buffer_size = Param.Unsigned(0, "nvm write buffer size")
+    nvm_wb_max_flush_threshold = Param.Unsigned(0, "wb max flush threshold")
+    nvm_wb_min_flush_threshold = Param.Unsigned(0, "wb min flush threshold")
+    bit_flips_delay = Param.Unsigned(0, "delay due to bit flips")
+    reram = Param.Unsigned(0, "reram nvm memory")
+    dynamic_row_remapping = Param.Unsigned(0, "reram nvm memory - dynamic row remapping")
+    
     # Currently rolled into other params
     ######################################################################
 
@@ -385,6 +406,21 @@ class DDR3_1600_8x8(DRAMCtrl):
     # self refresh exit time
     tXS = '270ns'
 
+    enable_encryption = 0
+    # encryption method
+    encryption_method = 0
+	#tsecme_encrption_thresh = 0;
+    deuce_epoch = 0
+
+    # non-volatile memory
+    nvm = 0
+    nvm_write_buffer_size = 0
+    nvm_wb_max_flush_threshold = 0
+    nvm_wb_min_flush_threshold = 0
+    bit_flips_delay = 0;
+    reram = 0;
+    dynamic_row_remapping = 0
+	
     # Current values from datasheet Die Rev E,J
     IDD0 = '55mA'
     IDD2N = '32mA'
@@ -397,6 +433,356 @@ class DDR3_1600_8x8(DRAMCtrl):
     IDD6 = '20mA'
     VDD = '1.5V'
 
+# NVM memory
+class NVM_DDR3_1600_8x8(DDR3_1600_8x8):
+    enable_encryption = 1
+    nvm = 1
+    nvm_write_buffer_size = 32
+    nvm_wb_max_flush_threshold = 30
+    nvm_wb_min_flush_threshold = 6
+    bit_flips_delay = 1
+    reram = 0
+    dynamic_row_remapping = 0
+	
+    tRCD = '60ns'
+    tWR = '160ns'
+
+# ReRAM
+class NVM_ReRAM(NVM_DDR3_1600_8x8):
+    enable_encryption = 0
+    bit_flips_delay = 0
+    nvm = 0
+    nvm_write_buffer_size = 0
+    nvm_wb_max_flush_threshold = 0
+    nvm_wb_min_flush_threshold = 0
+
+    reram = 1
+    dynamic_row_remapping = 0
+
+    write_buffer_size = 8
+    read_buffer_size = 8
+    min_writes_per_switch = 1
+
+    tRCD = '18ns'
+    tWR = '50ns'
+	
+# ReRAM
+class NVM_ReRAM1(NVM_DDR3_1600_8x8):
+    enable_encryption = 0
+    bit_flips_delay = 0
+    nvm = 0
+    nvm_write_buffer_size = 0
+    nvm_wb_max_flush_threshold = 0
+    nvm_wb_min_flush_threshold = 0    
+    reram = 1
+    dynamic_row_remapping = 1
+	
+    tRCD = '18ns'
+    tWR = '50ns'
+	
+# DCW without encryption
+class NVM_DCW(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 0
+
+# DCW with encryption
+class NVM_DCWECRPT(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 1
+
+# FNW without encryption
+class NVM_FNW(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 2
+
+# FNW with encryption
+class NVM_FNWECRPT(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 3
+
+# With TSECME encryption
+class NVM_TSECME(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 4
+    #tsecme_encrption_thresh = 200
+
+# With TSECME1 encryption
+class NVM_TSECME1(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 5
+
+# With TSECME1 encryption
+class NVM_TSECME2(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 6
+
+# With TSECME1 encryption
+class NVM_TSECME3(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 7
+
+# With TSECME1 encryption
+class NVM_TSECME4(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 8
+
+# With TSECME1 encryption
+class NVM_TSECME5(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 14
+
+# With TSECME1 encryption
+class NVM_TSECME6(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 15
+
+# With PSECME encryption
+class NVM_DEUCE_4(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 9
+    deuce_epoch = 4
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME_4(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 10
+    deuce_epoch = 4
+
+# With PSECME encryption
+class NVM_DEUCE_8(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 9
+    deuce_epoch = 8
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME_8(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 10
+    deuce_epoch = 8
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME1_8(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 11
+    deuce_epoch = 8
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME2_8(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 12
+    deuce_epoch = 8
+
+# With PSECME encryption
+class NVM_DEUCE_16(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 9
+    deuce_epoch = 16
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME_16(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 10
+    deuce_epoch = 16
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME1_16(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 11
+    deuce_epoch = 16
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME2_16(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 12
+    deuce_epoch = 16
+
+# With PSECME encryption
+class NVM_DEUCE_32(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 9
+    deuce_epoch = 32
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME_32(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 10
+    deuce_epoch = 32
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME1_32(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 11
+    deuce_epoch = 32
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME2_32(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 12
+    deuce_epoch = 32
+
+# With PSECME encryption
+class NVM_DEUCE_64(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 9
+    deuce_epoch = 64
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME_64(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 10
+    deuce_epoch = 64
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME1_64(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 11
+    deuce_epoch = 64
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME2_64(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 12
+    deuce_epoch = 64
+
+# With PSECME encryption
+class NVM_DEUCE_128(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 9
+    deuce_epoch = 128
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME_128(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 10
+    deuce_epoch = 128
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME1_128(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 11
+    deuce_epoch = 128
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME2_128(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 12
+    deuce_epoch = 128
+
+# With PSECME encryption
+class NVM_DEUCE_256(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 9
+    deuce_epoch = 256
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME_256(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 10
+    deuce_epoch = 256
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME1_256(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 11
+    deuce_epoch = 256
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME2_256(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 12
+    deuce_epoch = 256
+
+# With PSECME encryption
+class NVM_DEUCE1_256(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 13
+    deuce_epoch = 256
+
+# With PSECME encryption
+class NVM_DEUCE1_128(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 13
+    deuce_epoch = 128
+
+# With PSECME encryption
+class NVM_DEUCE_512(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 9
+    deuce_epoch = 512
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME_512(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 10
+    deuce_epoch = 512
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME1_512(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 11
+    deuce_epoch = 512
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME2_512(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 12
+    deuce_epoch = 512
+
+# With PSECME encryption
+class NVM_DEUCE1_512(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 13
+    deuce_epoch = 512
+
+# With PSECME encryption
+class NVM_DEUCE_1024(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 9
+    deuce_epoch = 1024
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME_1024(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 10
+    deuce_epoch = 1024
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME1_1024(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 11
+    deuce_epoch = 1024
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME2_1024(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 12
+    deuce_epoch = 1024
+
+# With PSECME encryption
+class NVM_DEUCE_2048(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 9
+    deuce_epoch = 2048
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME_2048(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 10
+    deuce_epoch = 2048
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME1_2048(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 11
+    deuce_epoch = 2048
+
+# With PSECME encryption
+class NVM_DEUCE_TSECME2_2048(NVM_DDR3_1600_8x8):
+    # encryption method
+    encryption_method = 12
+    deuce_epoch = 2048
+
+    
 # A single HMC-2500 x32 model based on:
 # [1] DRAMSpec: a high-level DRAM bank modelling tool
 # developed at the University of Kaiserslautern. This high level tool
diff --git a/src/mem/XBar.py b/src/mem/XBar.py
index 655d980..9dd80ec 100644
--- a/src/mem/XBar.py
+++ b/src/mem/XBar.py
@@ -151,6 +151,16 @@ class L2XBar(CoherentXBar):
 # One of the key coherent crossbar instances is the system
 # interconnect, tying together the CPU clusters, GPUs, and any I/O
 # coherent masters, and DRAM controllers.
+
+class L3XBar(CoherentXBar):
+    width = 32
+    frontend_latency = 1
+    forward_latency = 0
+    response_latency = 1
+    snoop_response_latency = 1
+    snoop_filter = SnoopFilter(lookup_latency = 0)
+    point_of_unification = True
+
 class SystemXBar(CoherentXBar):
     # 128-bit crossbar by default
     width = 16
diff --git a/src/mem/abstract_mem.cc b/src/mem/abstract_mem.cc
index 01817bb..93f398b 100644
--- a/src/mem/abstract_mem.cc
+++ b/src/mem/abstract_mem.cc
@@ -73,9 +73,10 @@ AbstractMemory::init()
 }
 
 void
-AbstractMemory::setBackingStore(uint8_t* pmem_addr)
+AbstractMemory::setBackingStore(uint8_t* pmem_addr, uint8_t* pmemEncrypt_addr)
 {
     pmemAddr = pmem_addr;
+    pmemEncryptAddr = pmemEncrypt_addr;
 }
 
 void
@@ -409,6 +410,9 @@ AbstractMemory::access(PacketPtr pkt)
         if (writeOK(pkt)) {
             if (pmemAddr) {
                 memcpy(hostAddr, pkt->getConstPtr<uint8_t>(), pkt->getSize());
+                //for(int i=0; i<pkt->getSize(); i++)
+				//		cout << " writing previous data: " << hostAddr[i];
+                //cout << endl;
                 DPRINTF(MemoryAccess, "%s wrote %i bytes to address %x\n",
                         __func__, pkt->getSize(), pkt->getAddr());
             }
@@ -458,3 +462,59 @@ AbstractMemory::functionalAccess(PacketPtr pkt)
               pkt->cmdString());
     }
 }
+
+void
+AbstractMemory::getMemoryData(PacketPtr pkt, uint8_t *p, uint8_t *s)
+{
+    assert(AddrRange(pkt->getAddr(),
+                     pkt->getAddr() + pkt->getSize() - 1).isSubset(range));
+
+    uint8_t *hostAddr = pmemAddr + pkt->getAddr() - range.start();
+
+    uint8_t *hostEncryptAddr = pmemEncryptAddr + pkt->getAddr() - range.start();
+
+    if (pkt->isWrite() || pkt->isRead()) {
+    	if (pmemAddr) {
+			memcpy(p, hostAddr, pkt->getSize());
+			//cout << " getting hostEncryptAddr: " << std::hex << *s << endl;
+		}
+		else
+			cout << " pmem addr is NULL" << endl;
+
+        if (pmemEncryptAddr) {
+			memcpy(s, hostEncryptAddr, pkt->getSize());
+			//cout << " getting hostEncryptAddr: " << std::hex << *s << endl;
+		}
+		else
+			cout << " pmem encrypt addr is NULL" << endl;
+
+    } else {
+        panic("AbstractMemory: unimplemented get memory command %s",
+              pkt->cmdString());
+    }
+}
+
+void
+AbstractMemory::storeEncryptedData(PacketPtr pkt, uint8_t *p)
+{
+    assert(AddrRange(pkt->getAddr(),
+                     pkt->getAddr() + pkt->getSize() - 1).isSubset(range));
+
+    uint8_t *hostEncryptAddr = pmemEncryptAddr + pkt->getAddr() - range.start();
+
+	if (pkt->isRead()) {
+		assert(!pkt->isWrite());
+		if (pmemEncryptAddr)
+			memcpy(p, hostEncryptAddr, pkt->getSize());
+	}
+	else if (pkt->isWrite()) {
+		if (pmemEncryptAddr) {
+			memcpy(hostEncryptAddr, p, pkt->getSize());
+			//cout << "storing hostEncryptAddr: " << std::hex << *hostEncryptAddr << endl;
+		}
+	} else {
+		panic("Unexpected packet %s", pkt->print());
+	}
+
+}
+
diff --git a/src/mem/abstract_mem.hh b/src/mem/abstract_mem.hh
index 4dd255f..9a99ac9 100644
--- a/src/mem/abstract_mem.hh
+++ b/src/mem/abstract_mem.hh
@@ -110,6 +110,9 @@ class AbstractMemory : public MemObject
     // Pointer to host memory used to implement this memory
     uint8_t* pmemAddr;
 
+    // Pointer to host encrypted memory
+    uint8_t* pmemEncryptAddr;
+
     // Enable specific memories to be reported to the configuration table
     const bool confTableReported;
 
@@ -216,7 +219,7 @@ class AbstractMemory : public MemObject
      *
      * @param pmem_addr Pointer to a segment of host memory
      */
-    void setBackingStore(uint8_t* pmem_addr);
+    void setBackingStore(uint8_t* pmem_addr, uint8_t* pmemEncrypt_addr);
 
     /**
      * Get the list of locked addresses to allow checkpointing.
@@ -313,6 +316,17 @@ class AbstractMemory : public MemObject
     void functionalAccess(PacketPtr pkt);
 
     /**
+     * Gets the stored data
+     *
+     */
+    void getMemoryData(PacketPtr pkt, uint8_t *p, uint8_t *s);
+
+    /*
+     * Stores encrypted data
+     */
+    void storeEncryptedData(PacketPtr pkt, uint8_t *p);
+
+    /**
      * Register Statistics
      */
     void regStats() override;
@@ -320,3 +334,4 @@ class AbstractMemory : public MemObject
 };
 
 #endif //__MEM_ABSTRACT_MEMORY_HH__
+
diff --git a/src/mem/cache/base.cc b/src/mem/cache/base.cc
index a061410..01686f5 100644
--- a/src/mem/cache/base.cc
+++ b/src/mem/cache/base.cc
@@ -227,8 +227,8 @@ BaseCache::handleTimingReqHit(PacketPtr pkt, CacheBlk *blk, Tick request_time)
         // by access(), that calls accessBlock() function.
         cpuSidePort.schedTimingResp(pkt, request_time, true);
     } else {
-        DPRINTF(Cache, "%s satisfied %s, no response needed\n", __func__,
-                pkt->print());
+        //DPRINTF(Cache, "%s satisfied %s, no response needed\n", __func__,
+        //        pkt->print());
 
         // queue the packet for deletion, as the sending cache is
         // still relying on it; if the block is found in access(),
@@ -264,8 +264,8 @@ BaseCache::handleTimingReqMiss(PacketPtr pkt, MSHR *mshr, CacheBlk *blk,
                 // uncached memory write, forwarded to WriteBuffer.
                 allocateWriteBuffer(pkt, forward_time);
             } else {
-                DPRINTF(Cache, "%s coalescing MSHR for %s\n", __func__,
-                        pkt->print());
+                //DPRINTF(Cache, "%s coalescing MSHR for %s\n", __func__,
+                //        pkt->print());
 
                 assert(pkt->req->masterId() < system->maxMasters());
                 mshr_hits[pkt->cmdToIndex()][pkt->req->masterId()]++;
@@ -429,18 +429,21 @@ BaseCache::recvTimingResp(PacketPtr pkt)
     const bool is_error = pkt->isError();
 
     if (is_error) {
-        DPRINTF(Cache, "%s: Cache received %s with error\n", __func__,
-                pkt->print());
+        //DPRINTF(Cache, "%s: Cache received %s with error\n", __func__,
+        //        pkt->print());
     }
 
-    DPRINTF(Cache, "%s: Handling response %s\n", __func__,
-            pkt->print());
+    //DPRINTF(Cache, "%s: Handling response %s\n", __func__,
+    //        pkt->print());
 
     // if this is a write, we should be looking at an uncacheable
     // write
     if (pkt->isWrite()) {
         assert(pkt->req->isUncacheable());
+        //if(pkt->req->isUncacheable())
         handleUncacheableWriteResp(pkt);
+        //else
+        //DPRINTF(Cache, "recvTimingResp write response %s\n",pkt->getBlockAddr(blkSize));
         return;
     }
 
@@ -478,8 +481,8 @@ BaseCache::recvTimingResp(PacketPtr pkt)
     CacheBlk *blk = tags->findBlock(pkt->getAddr(), pkt->isSecure());
 
     if (is_fill && !is_error) {
-        DPRINTF(Cache, "Block for addr %#llx being updated in Cache\n",
-                pkt->getAddr());
+        //DPRINTF(Cache, "Block for addr %#llx being updated in Cache\n",
+        //        pkt->getAddr());
 
         blk = handleFill(pkt, blk, writebacks, mshr->allocOnFill());
         assert(blk != nullptr);
@@ -558,8 +561,8 @@ BaseCache::recvAtomic(PacketPtr pkt)
     // above us is responding
     if (pkt->cacheResponding() && !pkt->isClean()) {
         assert(!pkt->req->isCacheInvalidate());
-        DPRINTF(Cache, "Cache above responding to %s: not responding\n",
-                pkt->print());
+        //DPRINTF(Cache, "Cache above responding to %s: not responding\n",
+        //        pkt->print());
 
         // if a cache is responding, and it had the line in Owned
         // rather than Modified state, we need to invalidate any
@@ -864,7 +867,58 @@ BaseCache::satisfyRequest(PacketPtr pkt, CacheBlk *blk, bool, bool)
         assert(blk->isWritable());
         // Write or WriteLine at the first cache with block in writable state
         if (blk->checkWrite(pkt)) {
-            pkt->writeDataToBlock(blk->data, blkSize);
+        	pkt->writeModifiedWordsFlagToBlock(blk->data,blkSize,blk->wordsModifiedFlags);
+        	/*std::string buffer;
+			buffer.append("address: "); buffer.append(std::to_string(pkt->getBlockAddr(blkSize)));
+			for(int i=0; i<32; i++) {buffer.append(":");buffer.append(std::to_string(blk->wordsModifiedFlags[i]));}buffer.append(" blk->tag: ");buffer.append(std::to_string(blk->tag));
+			buffer.append(" satisfyRequest: write: writeModifiedWordsFlagToBlock");
+			DPRINTF(Cache, "%s\n", buffer);
+			std::string buffer1;
+			std::string buffer2;
+			std::string buffer3;
+			buffer1.append("address: "); buffer1.append(std::to_string(pkt->getBlockAddr(blkSize)));
+			buffer1.append("(");buffer1.append(std::to_string(pkt->getAddr()));buffer1.append(")");
+	    	std::vector<uint8_t> pkt_data_words(pkt->getSize());
+	    	pkt->writeData(&pkt_data_words[0]);
+	    	for (int i=0; i<pkt->getSize(); i++) {
+				buffer1.append(":");
+				buffer1.append(std::to_string(pkt_data_words[i]));
+			}
+	    	//cout << buffer1 << endl;
+	    	DPRINTF(Cache, "%s\n", buffer1);
+			buffer2.append("address: "); buffer2.append(std::to_string(pkt->getBlockAddr(blkSize)));
+			buffer2.append("(");buffer2.append(std::to_string(pkt->getAddr()));buffer2.append(")");
+			buffer2.append("(");buffer2.append(std::to_string(pkt->getOffset(blkSize)));buffer2.append(")");
+	    	std::vector<uint8_t> blk_data_words(blkSize);
+	    	std::memcpy(&blk_data_words[0], blk->data, blkSize);
+	    	for (int i=0; i<blkSize; i++) {
+				buffer2.append(":");
+				buffer2.append(std::to_string(blk_data_words[i]));
+			}
+	    	//cout << buffer2 << endl;
+	    	DPRINTF(Cache, "%s\n", buffer2);
+			//buffer1.append(" satisfyRequest: write: writeModifiedWordsFlagToBlock");
+
+			//cout << "address: " << pkt->getBlockAddr(getSize()) <<"("<< pkt->getAddr()<<")"<<"("<< getSize()<<")";
+			//cout << buffer1 << endl;
+	    	//DPRINTF(Cache,"blk: %x\n",(uint512_t)(blk->data));
+	    	//DPRINTF(Cache,"pkt: %x\n",(uint512_t)(pkt));
+        	//cout << "address: " << pkt->getBlockAddr(blkSize); for(int i=0; i<32; i++) cout << ":"<<blk->wordsModifiedFlags[i]; cout<<" blk->tag: " << blk->tag << " satisfyRequest: writeModifiedWordsFlagToBlock"<<endl;
+			*/
+        	pkt->writeDataToBlock(blk->data, blkSize);
+			/*buffer3.append("address: "); buffer3.append(std::to_string(pkt->getBlockAddr(blkSize)));
+			buffer3.append("(");buffer3.append(std::to_string(pkt->getAddr()));buffer3.append(")");
+			buffer3.append("(");buffer3.append(std::to_string(pkt->getOffset(blkSize)));buffer3.append(")");
+            std::vector<uint8_t> blk_data_words1(blkSize);
+			std::memcpy(&blk_data_words1[0], blk->data, blkSize);
+			for (int i=0; i<blkSize; i++) {
+				buffer3.append(":");
+				buffer3.append(std::to_string(blk_data_words1[i]));
+			}
+			//buffer1.append(" satisfyRequest: write: writeModifiedWordsFlagToBlock");
+			//cout << buffer3 << endl;
+			DPRINTF(Cache, "%s\n", buffer3);
+			*/
         }
         // Always mark the line as dirty (and thus transition to the
         // Modified state) even if we are a failed StoreCond so we
@@ -880,6 +934,15 @@ BaseCache::satisfyRequest(PacketPtr pkt, CacheBlk *blk, bool, bool)
         // all read responses have a data payload
         assert(pkt->hasRespData());
         pkt->setDataFromBlock(blk->data, blkSize);
+        pkt->setModifiedWordsFlagFromBlock(blk->wordsModifiedFlags);
+        //std::ostringstream s;
+        //s << "address: " << pkt->getBlockAddr(blkSize); for(int i=0; i<32; i++) s << ":"<<blk->wordsModifiedFlags[i]; s <<" blk->tag: " << blk->tag << " satisfyRequest: writeModifiedWordsFlagToBlock"<<endl;
+        //std::string buffer(s.str());
+        //std::string buffer;
+		//buffer.append("address: "); buffer.append(std::to_string(pkt->getBlockAddr(blkSize)));
+        //for(int i=0; i<32; i++) {buffer.append(":");buffer.append(std::to_string(blk->wordsModifiedFlags[i]));}buffer.append(" blk->tag: ");buffer.append(std::to_string(blk->tag));
+        //buffer.append(" satisfyRequest: read: setModifiedWordsFlagFromBlock");
+        //DPRINTF(Cache, "%s\n", buffer);
     } else if (pkt->isUpgrade()) {
         // sanity check
         assert(!pkt->hasSharers());
@@ -922,8 +985,8 @@ BaseCache::access(PacketPtr pkt, CacheBlk *&blk, Cycles &lat,
     // that can modify its value.
     blk = tags->accessBlock(pkt->getAddr(), pkt->isSecure(), lat);
 
-    DPRINTF(Cache, "%s for %s %s\n", __func__, pkt->print(),
-            blk ? "hit " + blk->print() : "miss");
+    //DPRINTF(Cache, "%s for %s %s\n", __func__, pkt->print(),
+    //        blk ? "hit " + blk->print() : "miss");
 
     if (pkt->req->isCacheMaintenance()) {
         // A cache maintenance operation is always forwarded to the
@@ -984,8 +1047,8 @@ BaseCache::access(PacketPtr pkt, CacheBlk *&blk, Cycles &lat,
         // any ordering/decisions about ownership already taken
         if (pkt->cmd == MemCmd::WritebackClean &&
             mshrQueue.findMatch(pkt->getAddr(), pkt->isSecure())) {
-            DPRINTF(Cache, "Clean writeback %#llx to block with MSHR, "
-                    "dropping\n", pkt->getAddr());
+            //DPRINTF(Cache, "Clean writeback %#llx to block with MSHR, "
+            //        "dropping\n", pkt->getAddr());
             return true;
         }
 
@@ -1014,8 +1077,17 @@ BaseCache::access(PacketPtr pkt, CacheBlk *&blk, Cycles &lat,
         }
         // nothing else to do; writeback doesn't expect response
         assert(!pkt->needsResponse());
+        //pkt->writeModifiedWordsFlagToBlock(blk->data,blkSize,blk->wordsModifiedFlags);
+        pkt->setModifiedWordsFlagInBlock(blk->wordsModifiedFlags);
+        //std::string buffer;
+		//buffer.append("address: "); buffer.append(std::to_string(pkt->getBlockAddr(blkSize)));
+		//for(int i=0; i<32; i++) {buffer.append(":");buffer.append(std::to_string(blk->wordsModifiedFlags[i]));}buffer.append(" blk->tag: ");buffer.append(std::to_string(blk->tag));
+		//buffer.append(" access: writeback: setModifiedWordsFlagInBlock");
+		//DPRINTF(Cache, "%s\n", buffer);
+
+        //cout << "address: " << pkt->getBlockAddr(blkSize); for(int i=0; i<32; i++) cout << ":"<<blk->wordsModifiedFlags[i]; cout<<" blk->tag: " << blk->tag << " access: writeModifiedWordsFlagToBlock"<<endl;
         pkt->writeDataToBlock(blk->data, blkSize);
-        DPRINTF(Cache, "%s new state is %s\n", __func__, blk->print());
+        //DPRINTF(Cache, "%s new state is %s\n", __func__, blk->print());
         incHitCount(pkt);
         // populate the time when the block will be ready to access.
         blk->whenReady = clockEdge(fillLatency) + pkt->headerDelay +
@@ -1070,8 +1142,16 @@ BaseCache::access(PacketPtr pkt, CacheBlk *&blk, Cycles &lat,
         }
         // nothing else to do; writeback doesn't expect response
         assert(!pkt->needsResponse());
+        //pkt->writeModifiedWordsFlagToBlock(blk->data,blkSize,blk->wordsModifiedFlags);
+        pkt->setModifiedWordsFlagInBlock(blk->wordsModifiedFlags);
+        //std::string buffer;
+		//buffer.append("address: "); buffer.append(std::to_string(pkt->getBlockAddr(blkSize)));
+		//for(int i=0; i<32; i++) {buffer.append(":");buffer.append(std::to_string(blk->wordsModifiedFlags[i]));}buffer.append(" blk->tag: ");buffer.append(std::to_string(blk->tag));
+		//buffer.append(" access: writeclean: setModifiedWordsFlagInBlock");
+		//DPRINTF(Cache, "%s\n", buffer);
+        //cout << "address: " << pkt->getBlockAddr(blkSize); for(int i=0; i<32; i++) cout << ":"<<blk->wordsModifiedFlags[i]; cout<<" blk->tag: " << blk->tag << " access: writeModifiedWordsFlagToBlock"<<endl;
         pkt->writeDataToBlock(blk->data, blkSize);
-        DPRINTF(Cache, "%s new state is %s\n", __func__, blk->print());
+        //DPRINTF(Cache, "%s new state is %s\n", __func__, blk->print());
 
         incHitCount(pkt);
         // populate the time when the block will be ready to access.
@@ -1123,9 +1203,9 @@ BaseCache::handleFill(PacketPtr pkt, CacheBlk *blk, PacketList &writebacks,
     assert(pkt->isResponse() || pkt->cmd == MemCmd::WriteLineReq);
     Addr addr = pkt->getAddr();
     bool is_secure = pkt->isSecure();
-#if TRACING_ON
-    CacheBlk::State old_state = blk ? blk->status : 0;
-#endif
+//#if TRACING_ON
+    //CacheBlk::State old_state = blk ? blk->status : 0;
+//#endif
 
     // When handling a fill, we should have no writes to this line.
     assert(addr == pkt->getBlockAddr(blkSize));
@@ -1151,8 +1231,8 @@ BaseCache::handleFill(PacketPtr pkt, CacheBlk *blk, PacketList &writebacks,
             assert(!tempBlock->isValid());
             blk = tempBlock;
             tempBlock->insert(addr, is_secure);
-            DPRINTF(Cache, "using temp block for %#llx (%s)\n", addr,
-                    is_secure ? "s" : "ns");
+            //DPRINTF(Cache, "using temp block for %#llx (%s)\n", addr,
+            //        is_secure ? "s" : "ns");
         }
 
         // we should never be overwriting a valid block
@@ -1201,8 +1281,8 @@ BaseCache::handleFill(PacketPtr pkt, CacheBlk *blk, PacketList &writebacks,
         }
     }
 
-    DPRINTF(Cache, "Block addr %#llx (%s) moving from state %x to %s\n",
-            addr, is_secure ? "s" : "ns", old_state, blk->print());
+    //DPRINTF(Cache, "Block addr %#llx (%s) moving from state %x to %s\n",
+    //        addr, is_secure ? "s" : "ns", old_state, blk->print());
 
     // if we got new data, copy it in (checking for a read response
     // and a response that has data is the same in the end)
@@ -1212,6 +1292,14 @@ BaseCache::handleFill(PacketPtr pkt, CacheBlk *blk, PacketList &writebacks,
         assert(pkt->getSize() == blkSize);
 
         pkt->writeDataToBlock(blk->data, blkSize);
+        //pkt->writeModifiedWordsFlagToBlock(blk->data);
+        pkt->setModifiedWordsFlagInBlock(blk->wordsModifiedFlags);
+        //std::string buffer;
+		//buffer.append("address: "); buffer.append(std::to_string(pkt->getBlockAddr(blkSize)));
+		//for(int i=0; i<32; i++) {buffer.append(":");buffer.append(std::to_string(blk->wordsModifiedFlags[i]));}buffer.append(" blk->tag: ");buffer.append(std::to_string(blk->tag));
+		//buffer.append(" handleFill: read: setModifiedWordsFlagInBlock");
+		//DPRINTF(Cache, "%s\n", buffer);
+        //cout << "address: " << pkt->getBlockAddr(blkSize); for(int i=0; i<32; i++) cout << ":"<<blk->wordsModifiedFlags[i]; cout<<" blk->tag: " << blk->tag << " handleFill: setModifiedWordsFlagInBlock"<<endl;
     }
     // We pay for fillLatency here.
     blk->whenReady = clockEdge() + fillLatency * clockPeriod() +
@@ -1259,11 +1347,11 @@ BaseCache::allocateBlock(const PacketPtr pkt, PacketList &writebacks)
     // The victim will be replaced by a new entry, so increase the replacement
     // counter if a valid block is being replaced
     if (victim->isValid()) {
-        DPRINTF(Cache, "replacement: replacing %#llx (%s) with %#llx "
-                "(%s): %s\n", regenerateBlkAddr(victim),
-                victim->isSecure() ? "s" : "ns",
-                addr, is_secure ? "s" : "ns",
-                victim->isDirty() ? "writeback" : "clean");
+        //DPRINTF(Cache, "replacement: replacing %#llx (%s) with %#llx "
+        //        "(%s): %s\n", regenerateBlkAddr(victim),
+        //        victim->isSecure() ? "s" : "ns",
+        //        addr, is_secure ? "s" : "ns",
+        //        victim->isDirty() ? "writeback" : "clean");
 
         replacements++;
     }
@@ -1314,8 +1402,8 @@ BaseCache::writebackBlk(CacheBlk *blk)
         new Packet(req, blk->isDirty() ?
                    MemCmd::WritebackDirty : MemCmd::WritebackClean);
 
-    DPRINTF(Cache, "Create Writeback %s writable: %d, dirty: %d\n",
-            pkt->print(), blk->isWritable(), blk->isDirty());
+    //DPRINTF(Cache, "Create Writeback %s writable: %d, dirty: %d\n",
+     //       pkt->print(), blk->isWritable(), blk->isDirty());
 
     if (blk->isWritable()) {
         // not asserting shared means we pass the block in modified
@@ -1331,7 +1419,13 @@ BaseCache::writebackBlk(CacheBlk *blk)
 
     pkt->allocate();
     pkt->setDataFromBlock(blk->data, blkSize);
-
+    pkt->setModifiedWordsFlagFromBlock(blk->wordsModifiedFlags);
+    //std::string buffer;
+	//buffer.append("address: "); buffer.append(std::to_string(pkt->getBlockAddr(blkSize)));
+	//for(int i=0; i<32; i++) {buffer.append(":");buffer.append(std::to_string(blk->wordsModifiedFlags[i]));}buffer.append(" blk->tag: ");buffer.append(std::to_string(blk->tag));
+	//buffer.append(" writebackBlk: setModifiedWordsFlagFromBlock");
+	//DPRINTF(Cache, "%s\n", buffer);
+    //cout << "address: " << pkt->getBlockAddr(blkSize); for(int i=0; i<32; i++) cout << ":"<<blk->wordsModifiedFlags[i]; cout<<" blk->tag: " << blk->tag << " writebackBlk: setModifiedWordsFlagFromBlock"<<endl;
     return pkt;
 }
 
@@ -1353,8 +1447,8 @@ BaseCache::writecleanBlk(CacheBlk *blk, Request::Flags dest, PacketId id)
         pkt->setWriteThrough();
     }
 
-    DPRINTF(Cache, "Create %s writable: %d, dirty: %d\n", pkt->print(),
-            blk->isWritable(), blk->isDirty());
+    //DPRINTF(Cache, "Create %s writable: %d, dirty: %d\n", pkt->print(),
+    //        blk->isWritable(), blk->isDirty());
 
     if (blk->isWritable()) {
         // not asserting shared means we pass the block in modified
@@ -1370,7 +1464,13 @@ BaseCache::writecleanBlk(CacheBlk *blk, Request::Flags dest, PacketId id)
 
     pkt->allocate();
     pkt->setDataFromBlock(blk->data, blkSize);
-
+    pkt->setModifiedWordsFlagFromBlock(blk->wordsModifiedFlags);
+    //std::string buffer;
+	//buffer.append("address: "); buffer.append(std::to_string(pkt->getBlockAddr(blkSize)));
+	//for(int i=0; i<32; i++) {buffer.append(":");buffer.append(std::to_string(blk->wordsModifiedFlags[i]));}buffer.append(" blk->tag: ");buffer.append(std::to_string(blk->tag));
+	//buffer.append(" writecleanBlk: setModifiedWordsFlagFromBlock");
+	//DPRINTF(Cache, "%s\n", buffer);
+    //cout << "address: " << pkt->getBlockAddr(blkSize); for(int i=0; i<32; i++) cout << ":"<<blk->wordsModifiedFlags[i]; cout<<" blk->tag: " << blk->tag << " writecleanBlk: setModifiedWordsFlagFromBlock"<<endl;
     return pkt;
 }
 
@@ -1454,7 +1554,7 @@ BaseCache::sendMSHRQueuePacket(MSHR* mshr)
     // use request from 1st target
     PacketPtr tgt_pkt = mshr->getTarget()->pkt;
 
-    DPRINTF(Cache, "%s: MSHR %s\n", __func__, tgt_pkt->print());
+    //DPRINTF(Cache, "%s: MSHR %s\n", __func__, tgt_pkt->print());
 
     CacheBlk *blk = tags->findBlock(mshr->blkAddr, mshr->isSecure);
 
@@ -1533,7 +1633,7 @@ BaseCache::sendWriteQueuePacket(WriteQueueEntry* wq_entry)
     // always a single target for write queue entries
     PacketPtr tgt_pkt = wq_entry->getTarget()->pkt;
 
-    DPRINTF(Cache, "%s: write %s\n", __func__, tgt_pkt->print());
+    //DPRINTF(Cache, "%s: write %s\n", __func__, tgt_pkt->print());
 
     // forward as is, both for evictions and uncacheable writes
     if (!memSidePort.sendTimingReq(tgt_pkt)) {
@@ -2351,3 +2451,4 @@ BaseCache::MemSidePort::MemSidePort(const std::string &_name,
       _snoopRespQueue(*_cache, *this, _label), cache(_cache)
 {
 }
+
diff --git a/src/mem/cache/base.hh b/src/mem/cache/base.hh
index 4ebc524..53b1faa 100644
--- a/src/mem/cache/base.hh
+++ b/src/mem/cache/base.hh
@@ -1020,7 +1020,7 @@ class BaseCache : public MemObject
         WriteQueueEntry *wq_entry =
             writeBuffer.findMatch(blk_addr, pkt->isSecure());
         if (wq_entry && !wq_entry->inService) {
-            DPRINTF(Cache, "Potential to merge writeback %s", pkt->print());
+            //DPRINTF(Cache, "Potential to merge writeback %s", pkt->print());
         }
 
         writeBuffer.allocate(blk_addr, blkSize, pkt, time, order++);
@@ -1055,7 +1055,7 @@ class BaseCache : public MemObject
             cpuSidePort.setBlocked();
         }
         blocked |= flag;
-        DPRINTF(Cache,"Blocking for cause %d, mask=%d\n", cause, blocked);
+        //DPRINTF(Cache,"Blocking for cause %d, mask=%d\n", cause, blocked);
     }
 
     /**
@@ -1069,7 +1069,7 @@ class BaseCache : public MemObject
     {
         uint8_t flag = 1 << cause;
         blocked &= ~flag;
-        DPRINTF(Cache,"Unblocking for cause %d, mask=%d\n", cause, blocked);
+        //DPRINTF(Cache,"Unblocking for cause %d, mask=%d\n", cause, blocked);
         if (blocked == 0) {
             blocked_cycles[cause] += curCycle() - blockedCycle;
             cpuSidePort.clearBlocked();
@@ -1159,3 +1159,4 @@ class BaseCache : public MemObject
 };
 
 #endif //__MEM_CACHE_BASE_HH__
+
diff --git a/src/mem/cache/blk.hh b/src/mem/cache/blk.hh
index 3bb0317..2f7eae0 100644
--- a/src/mem/cache/blk.hh
+++ b/src/mem/cache/blk.hh
@@ -123,6 +123,8 @@ class CacheBlk : public ReplaceableEntry
     /** Tick on which the block was inserted in the cache. */
     Tick tickInserted;
 
+    bool wordsModifiedFlags[32];
+
   protected:
     /**
      * Represents that the indicated thread context has a "lock" on
diff --git a/src/mem/cache/cache.cc b/src/mem/cache/cache.cc
index 1b53163..72159a4 100644
--- a/src/mem/cache/cache.cc
+++ b/src/mem/cache/cache.cc
@@ -172,7 +172,7 @@ Cache::access(PacketPtr pkt, CacheBlk *&blk, Cycles &lat,
                       "Should never see a write in a read-only cache %s\n",
                       name());
 
-        DPRINTF(Cache, "%s for %s\n", __func__, pkt->print());
+        //DPRINTF(Cache, "%s for %s\n", __func__, pkt->print());
 
         // flush and invalidate any existing block
         CacheBlk *old_blk(tags->findBlock(pkt->getAddr(), pkt->isSecure()));
@@ -269,7 +269,7 @@ Cache::doWritebacksAtomic(PacketList& writebacks)
 void
 Cache::recvTimingSnoopResp(PacketPtr pkt)
 {
-    DPRINTF(Cache, "%s for %s\n", __func__, pkt->print());
+    //DPRINTF(Cache, "%s for %s\n", __func__, pkt->print());
 
     // determine if the response is from a snoop request we created
     // (in which case it should be in the outstandingSnoop), or if we
@@ -284,8 +284,8 @@ Cache::recvTimingSnoopResp(PacketPtr pkt)
 
         outstandingSnoop.erase(pkt->req);
 
-        DPRINTF(Cache, "Got prefetch response from above for addr "
-                "%#llx (%s)\n", pkt->getAddr(), pkt->isSecure() ? "s" : "ns");
+        //DPRINTF(Cache, "Got prefetch response from above for addr "
+        //        "%#llx (%s)\n", pkt->getAddr(), pkt->isSecure() ? "s" : "ns");
         recvTimingResp(pkt);
         return;
     }
@@ -307,7 +307,7 @@ Cache::promoteWholeLineWrites(PacketPtr pkt)
     if (doFastWrites && (pkt->cmd == MemCmd::WriteReq) &&
         (pkt->getSize() == blkSize) && (pkt->getOffset(blkSize) == 0)) {
         pkt->cmd = MemCmd::WriteLineReq;
-        DPRINTF(Cache, "packet promoted from Write to WriteLineReq\n");
+        //DPRINTF(Cache, "packet promoted from Write to WriteLineReq\n");
     }
 }
 
@@ -412,8 +412,8 @@ Cache::recvTimingReq(PacketPtr pkt)
         // a cache above us (but not where the packet came from) is
         // responding to the request, in other words it has the line
         // in Modified or Owned state
-        DPRINTF(Cache, "Cache above responding to %s: not responding\n",
-                pkt->print());
+        //DPRINTF(Cache, "Cache above responding to %s: not responding\n",
+        //        pkt->print());
 
         // if the packet needs the block to be writable, and the cache
         // that has promised to respond (setting the cache responding
@@ -545,16 +545,16 @@ Cache::createMissPacket(PacketPtr cpu_pkt, CacheBlk *blk,
         // of date, however, there is no harm in conservatively
         // assuming the block has sharers
         pkt->setHasSharers();
-        DPRINTF(Cache, "%s: passing hasSharers from %s to %s\n",
-                __func__, cpu_pkt->print(), pkt->print());
+        //DPRINTF(Cache, "%s: passing hasSharers from %s to %s\n",
+        //        __func__, cpu_pkt->print(), pkt->print());
     }
 
     // the packet should be block aligned
     assert(pkt->getAddr() == pkt->getBlockAddr(blkSize));
 
     pkt->allocate();
-    DPRINTF(Cache, "%s: created %s from %s\n", __func__, pkt->print(),
-            cpu_pkt->print());
+    //DPRINTF(Cache, "%s: created %s from %s\n", __func__, pkt->print(),
+    //        cpu_pkt->print());
     return pkt;
 }
 
@@ -590,20 +590,20 @@ Cache::handleAtomicReqMiss(PacketPtr pkt, CacheBlk *&blk,
         bus_pkt = pkt;
     }
 
-    DPRINTF(Cache, "%s: Sending an atomic %s\n", __func__,
-            bus_pkt->print());
+    //DPRINTF(Cache, "%s: Sending an atomic %s\n", __func__,
+    //        bus_pkt->print());
 
-#if TRACING_ON
-    CacheBlk::State old_state = blk ? blk->status : 0;
-#endif
+//#if TRACING_ON
+//    CacheBlk::State old_state = blk ? blk->status : 0;
+//#endif
 
     Cycles latency = ticksToCycles(memSidePort.sendAtomic(bus_pkt));
 
     bool is_invalidate = bus_pkt->isInvalidate();
 
     // We are now dealing with the response handling
-    DPRINTF(Cache, "%s: Receive response: %s in state %i\n", __func__,
-            bus_pkt->print(), old_state);
+    //DPRINTF(Cache, "%s: Receive response: %s in state %i\n", __func__,
+    //        bus_pkt->print(), old_state);
 
     // If packet was a forward, the response (if any) is already
     // in place in the bus_pkt == pkt structure, so we don't need
@@ -786,8 +786,8 @@ Cache::serviceMSHRTargets(MSHR *mshr, const PacketPtr pkt, CacheBlk *blk,
                 // propagate that.  Response should not have
                 // isInvalidate() set otherwise.
                 tgt_pkt->cmd = MemCmd::ReadRespWithInvalidate;
-                DPRINTF(Cache, "%s: updated cmd to %s\n", __func__,
-                        tgt_pkt->print());
+                //DPRINTF(Cache, "%s: updated cmd to %s\n", __func__,
+                //        tgt_pkt->print());
             }
             // Reset the bus additional time as it is now accounted for
             tgt_pkt->headerDelay = tgt_pkt->payloadDelay = 0;
@@ -805,7 +805,7 @@ Cache::serviceMSHRTargets(MSHR *mshr, const PacketPtr pkt, CacheBlk *blk,
             // I don't believe that a snoop can be in an error state
             assert(!is_error);
             // response to snoop request
-            DPRINTF(Cache, "processing deferred snoop...\n");
+            //DPRINTF(Cache, "processing deferred snoop...\n");
             // If the response is invalidating, a snooping target can
             // be satisfied if it is also invalidating. If the reponse is, not
             // only invalidating, but more specifically an InvalidateResp and
@@ -877,7 +877,7 @@ Cache::cleanEvictBlk(CacheBlk *blk)
 
     PacketPtr pkt = new Packet(req, MemCmd::CleanEvict);
     pkt->allocate();
-    DPRINTF(Cache, "Create CleanEvict %s\n", pkt->print());
+    //DPRINTF(Cache, "Create CleanEvict %s\n", pkt->print());
 
     return pkt;
 }
@@ -896,7 +896,7 @@ Cache::doTimingSupplyResponse(PacketPtr req_pkt, const uint8_t *blk_data,
     assert(req_pkt->isRequest());
     assert(req_pkt->needsResponse());
 
-    DPRINTF(Cache, "%s: for %s\n", __func__, req_pkt->print());
+    //DPRINTF(Cache, "%s: for %s\n", __func__, req_pkt->print());
     // timing-mode snoop responses require a new packet, unless we
     // already made a copy...
     PacketPtr pkt = req_pkt;
@@ -1050,8 +1050,8 @@ Cache::handleSnoop(PacketPtr pkt, CacheBlk *blk, bool is_timing,
         }
         return snoop_delay;
     } else {
-        DPRINTF(Cache, "%s: snoop hit for %s, old state is %s\n", __func__,
-                pkt->print(), blk->print());
+        //DPRINTF(Cache, "%s: snoop hit for %s, old state is %s\n", __func__,
+        //        pkt->print(), blk->print());
 
         // We may end up modifying both the block state and the packet (if
         // we respond in atomic mode), so just figure out what to do now
@@ -1071,8 +1071,8 @@ Cache::handleSnoop(PacketPtr pkt, CacheBlk *blk, bool is_timing,
     // above and in it's own cache, a new MemCmd::ReadReq is created that
     // downstream caches observe.
     if (pkt->mustCheckAbove()) {
-        DPRINTF(Cache, "Found addr %#llx in upper level cache for snoop %s "
-                "from lower cache\n", pkt->getAddr(), pkt->print());
+        //DPRINTF(Cache, "Found addr %#llx in upper level cache for snoop %s "
+        //        "from lower cache\n", pkt->getAddr(), pkt->print());
         pkt->setBlockCached();
         return snoop_delay;
     }
@@ -1089,7 +1089,7 @@ Cache::handleSnoop(PacketPtr pkt, CacheBlk *blk, bool is_timing,
         // Exclusive to Shared, or remain in Shared
         if (!pkt->req->isUncacheable())
             blk->status &= ~BlkWritable;
-        DPRINTF(Cache, "new state is %s\n", blk->print());
+        //DPRINTF(Cache, "new state is %s\n", blk->print());
     }
 
     if (respond) {
@@ -1139,7 +1139,7 @@ Cache::handleSnoop(PacketPtr pkt, CacheBlk *blk, bool is_timing,
     // like that
     if (blk_valid && invalidate) {
         invalidateBlock(blk);
-        DPRINTF(Cache, "new state is %s\n", blk->print());
+        //DPRINTF(Cache, "new state is %s\n", blk->print());
     }
 
     return snoop_delay;
@@ -1173,8 +1173,8 @@ Cache::recvTimingSnoopReq(PacketPtr pkt)
     // Inform request(Prefetch, CleanEvict or Writeback) from below of
     // MSHR hit, set setBlockCached.
     if (mshr && pkt->mustCheckAbove()) {
-        DPRINTF(Cache, "Setting block cached for %s from lower cache on "
-                "mshr hit\n", pkt->print());
+        //DPRINTF(Cache, "Setting block cached for %s from lower cache on "
+        //        "mshr hit\n", pkt->print());
         pkt->setBlockCached();
         return;
     }
@@ -1189,9 +1189,9 @@ Cache::recvTimingSnoopReq(PacketPtr pkt)
     // Let the MSHR itself track the snoop and decide whether we want
     // to go ahead and do the regular cache snoop
     if (mshr && mshr->handleSnoop(pkt, order++)) {
-        DPRINTF(Cache, "Deferring snoop on in-service MSHR to blk %#llx (%s)."
-                "mshrs: %s\n", blk_addr, is_secure ? "s" : "ns",
-                mshr->print());
+        //DPRINTF(Cache, "Deferring snoop on in-service MSHR to blk %#llx (%s)."
+        //        "mshrs: %s\n", blk_addr, is_secure ? "s" : "ns",
+        //        mshr->print());
 
         if (mshr->getNumTargets() > numTarget)
             warn("allocating bonus target for snoop"); //handle later
@@ -1201,8 +1201,8 @@ Cache::recvTimingSnoopReq(PacketPtr pkt)
     //We also need to check the writeback buffers and handle those
     WriteQueueEntry *wb_entry = writeBuffer.findMatch(blk_addr, is_secure);
     if (wb_entry) {
-        DPRINTF(Cache, "Snoop hit in writeback to addr %#llx (%s)\n",
-                pkt->getAddr(), is_secure ? "s" : "ns");
+        //DPRINTF(Cache, "Snoop hit in writeback to addr %#llx (%s)\n",
+        //        pkt->getAddr(), is_secure ? "s" : "ns");
         // Expect to see only Writebacks and/or CleanEvicts here, both of
         // which should not be generated for uncacheable data.
         assert(!wb_entry->isUncacheable());
@@ -1218,8 +1218,8 @@ Cache::recvTimingSnoopReq(PacketPtr pkt)
             // propagate the BLOCK_CACHED flag in Writeback packets and prevent
             // any CleanEvicts from travelling down the memory hierarchy.
             pkt->setBlockCached();
-            DPRINTF(Cache, "%s: Squashing %s from lower cache on writequeue "
-                    "hit\n", __func__, pkt->print());
+            //DPRINTF(Cache, "%s: Squashing %s from lower cache on writequeue "
+            //        "hit\n", __func__, pkt->print());
             return;
         }
 
@@ -1321,7 +1321,7 @@ Cache::sendMSHRQueuePacket(MSHR* mshr)
     PacketPtr tgt_pkt = mshr->getTarget()->pkt;
 
     if (tgt_pkt->cmd == MemCmd::HardPFReq && forwardSnoops) {
-        DPRINTF(Cache, "%s: MSHR %s\n", __func__, tgt_pkt->print());
+        //DPRINTF(Cache, "%s: MSHR %s\n", __func__, tgt_pkt->print());
 
         // we should never have hardware prefetches to allocated
         // blocks
@@ -1362,16 +1362,16 @@ Cache::sendMSHRQueuePacket(MSHR* mshr)
             bool pending_modified_resp = !snoop_pkt.hasSharers();
             markInService(mshr, pending_modified_resp);
 
-            DPRINTF(Cache, "Upward snoop of prefetch for addr"
-                    " %#x (%s) hit\n",
-                    tgt_pkt->getAddr(), tgt_pkt->isSecure()? "s": "ns");
+            //DPRINTF(Cache, "Upward snoop of prefetch for addr"
+            //        " %#x (%s) hit\n",
+            //        tgt_pkt->getAddr(), tgt_pkt->isSecure()? "s": "ns");
             return false;
         }
 
         if (snoop_pkt.isBlockCached()) {
-            DPRINTF(Cache, "Block present, prefetch squashed by cache.  "
-                    "Deallocating mshr target %#x.\n",
-                    mshr->blkAddr);
+            //DPRINTF(Cache, "Block present, prefetch squashed by cache.  "
+            //        "Deallocating mshr target %#x.\n",
+            //        mshr->blkAddr);
 
             // Deallocate the mshr target
             if (mshrQueue.forceDeallocateTarget(mshr)) {
@@ -1398,3 +1398,4 @@ CacheParams::create()
 
     return new Cache(this);
 }
+
diff --git a/src/mem/cache/mshr.cc b/src/mem/cache/mshr.cc
index ccaec7b..e28cc9e 100644
--- a/src/mem/cache/mshr.cc
+++ b/src/mem/cache/mshr.cc
@@ -140,13 +140,13 @@ replaceUpgrade(PacketPtr pkt)
 
     if (pkt->cmd == MemCmd::UpgradeReq) {
         pkt->cmd = MemCmd::ReadExReq;
-        DPRINTF(Cache, "Replacing UpgradeReq with ReadExReq\n");
+        //DPRINTF(Cache, "Replacing UpgradeReq with ReadExReq\n");
     } else if (pkt->cmd == MemCmd::SCUpgradeReq) {
         pkt->cmd = MemCmd::SCUpgradeFailReq;
-        DPRINTF(Cache, "Replacing SCUpgradeReq with SCUpgradeFailReq\n");
+        //DPRINTF(Cache, "Replacing SCUpgradeReq with SCUpgradeFailReq\n");
     } else if (pkt->cmd == MemCmd::StoreCondReq) {
         pkt->cmd = MemCmd::StoreCondFailReq;
-        DPRINTF(Cache, "Replacing StoreCondReq with StoreCondFailReq\n");
+        //DPRINTF(Cache, "Replacing StoreCondReq with StoreCondFailReq\n");
     }
 
     if (!has_data) {
@@ -357,7 +357,7 @@ MSHR::allocateTarget(PacketPtr pkt, Tick whenReady, Counter _order,
 bool
 MSHR::handleSnoop(PacketPtr pkt, Counter _order)
 {
-    DPRINTF(Cache, "%s for %s\n", __func__, pkt->print());
+    //DPRINTF(Cache, "%s for %s\n", __func__, pkt->print());
 
     // when we snoop packets the needsWritable and isInvalidate flags
     // should always be the same, however, this assumes that we never
@@ -671,3 +671,4 @@ MSHR::print() const
     print(str);
     return str.str();
 }
+
diff --git a/src/mem/coherent_xbar.cc b/src/mem/coherent_xbar.cc
index 2bee5bc..a7daa18 100644
--- a/src/mem/coherent_xbar.cc
+++ b/src/mem/coherent_xbar.cc
@@ -439,6 +439,9 @@ CoherentXBar::recvTimingResp(PacketPtr pkt, PortID master_port_id)
     // determine the source port based on the id
     MasterPort *src_port = masterPorts[master_port_id];
 
+if(pkt->getAddr()>=17179607040){
+        return true;
+    }
     // determine the destination
     const auto route_lookup = routeTo.find(pkt->req);
     assert(route_lookup != routeTo.end());
diff --git a/src/mem/dram_ctrl.cc b/src/mem/dram_ctrl.cc
index e7af75b..e11036b 100644
--- a/src/mem/dram_ctrl.cc
+++ b/src/mem/dram_ctrl.cc
@@ -55,14 +55,73 @@
 #include "debug/Drain.hh"
 #include "debug/QOS.hh"
 #include "sim/system.hh"
+#include "base/random.hh"
+#include "cache/C_Cache.h"
+#include "cache/MT_Cachex.h"
+#include <boost/multiprecision/cpp_int.hpp>
+#include <boost/random.hpp>
 
 using namespace std;
 using namespace Data;
+using namespace boost::multiprecision;
+using namespace boost::random;
+
+//const uint64_t MainMemorySize = 17179869184; //16GB 
+
+//const uint64_t CacheSize = 256*1024;
+//const uint64_t Associativity = 8;
+//const uint64_t BlockSize = 64;
+//const uint64_t NoOfSets = (uint32_t)CacheSize/(BlockSize*Associativity);
+//const uint64_t NoOfCounters = (int)MainMemorySize/64;
+
+
+//static map< uint64_t , int > counters;
+//static map< uint64_t , bool > shadowPlus; 
+list<PacketPtr> dummyPacketQ;
+uint8_t *pkt_data_d = new uint8_t[1];
+
+CounterCache *cc= new CounterCache();
+MTCache *mtc=new MTCache();
+
+uint64_t hit_count=0;
+uint64_t miss_count=0;
+uint64_t ocounter=0;
+uint64_t scounter=0;
+uint64_t mtc_scounter=0;
+uint64_t sppcounter=0;
+uint64_t shdPPcounter=0;
+uint64_t mtread=0;
+uint64_t mtwrite=0;
+uint64_t mthit=0;
+uint64_t mtmiss=0;
+
+uint64_t AES=0;
+map<uint64_t,list<uint32_t>* > prev_modified_words;			// used to store each cacheline previous modified words to perform encryption
+map<uint64_t,uint32_t> cacheline_epoch; 	// used to store each cacheline encryption status
+map<uint64_t,uint32_t> cacheline_stats; 	// used to store each cacheline encryption status
+map<uint64_t,uint32_t> cacheline_stats_4; 	// used to store each cacheline encryption status
+map<uint64_t,uint32_t> cacheline_stats_8; 	// used to store each cacheline encryption status
+map<uint64_t,uint32_t> cacheline_stats_16; 	// used to store each cacheline encryption status
+map<uint64_t,uint32_t> cacheline_stats_32; 	// used to store each cacheline encryption status
+map<uint64_t,uint32_t> cacheline_stats_64; 	// used to store each cacheline encryption status
+uint64_t numCacheLinesWritten1_4=0;
+uint64_t numCacheLinesWritten1_8=0;
+uint64_t numCacheLinesWritten1_16=0;
+uint64_t numCacheLinesWritten1_32=0;
+uint64_t numCacheLinesWritten1_64=0;
+uint64_t num_epoch = 0;
+map<uint64_t,uint512_t> trailPAD;
+map<uint64_t,uint512_t> leadPAD;
+map<uint64_t,vector<bool>* > modified_words_flags;
+
+#define DEBUG
 
 DRAMCtrl::DRAMCtrl(const DRAMCtrlParams* p) :
     QoS::MemCtrl(p),
     port(name() + ".port", *this), isTimingMode(false),
     retryRdReq(false), retryWrReq(false),
+    XbarSendPacketEvent([this]{ processSendPacketEvent(); }, name()),
+    XbarFakeEvent([this]{ processFakeEvent(); }, name()),
     nextReqEvent([this]{ processNextReqEvent(); }, name()),
     respondEvent([this]{ processRespondEvent(); }, name()),
     deviceSize(p->device_size),
@@ -109,6 +168,39 @@ DRAMCtrl::DRAMCtrl(const DRAMCtrlParams* p) :
     readQueue.resize(p->qos_priorities);
     writeQueue.resize(p->qos_priorities);
 
+    nvm = p->nvm;
+    enableEncryption = 0;
+    bitFlipsDelay = 0;
+    if(p->nvm) {
+    	nvm = p->nvm;
+    	nvm_write_buffer_size = p->nvm_write_buffer_size;
+        nvm_totalWriteBufferSize = 0;
+        nvm_wb_flushing = false;
+        nvm_wb_max_threshold = p->nvm_wb_max_flush_threshold;
+        nvm_wb_min_threshold = p->nvm_wb_min_flush_threshold;
+
+        enableEncryption = p->enable_encryption;
+        if(enableEncryption) {
+        	bitFlipsDelay = p->bit_flips_delay;
+        	if(bitFlipsDelay) {
+            	encryptionMethod = p->encryption_method;
+            	cout << "Encryption method: " << encryptionMethod << endl;
+        		deuce_epoch = p->deuce_epoch;
+        		cout << "NVM: " << nvm << " write buffer size: " << nvm_write_buffer_size << " wb max theshold: " << nvm_wb_max_threshold << " wb_min_threshold: " << nvm_wb_min_threshold <<
+        				" deuce epoch: " << deuce_epoch << endl;
+        	}
+        }
+    }
+        reram = p->reram;
+        if(reram) {
+        	dynamicRowReMapping = p->dynamic_row_remapping;
+        	cout << "NVM: " << nvm << " write buffer size: " << nvm_write_buffer_size << " wb max theshold: " << nvm_wb_max_threshold << " wb_min_threshold: " << nvm_wb_min_threshold <<
+        		" reram: " << reram << " dynamic_row_remapping" << dynamicRowReMapping << endl;
+
+        }
+    //}
+    cout << "tRCD: " << tRCD << " tWR: " << tWR << endl;
+
 
     for (int i = 0; i < ranksPerChannel; i++) {
         Rank* rank = new Rank(*this, p, i);
@@ -134,14 +226,14 @@ DRAMCtrl::DRAMCtrl(const DRAMCtrlParams* p) :
              "address range assigned (%d Mbytes)\n", deviceCapacity,
              capacity / (1024 * 1024));
 
-    DPRINTF(DRAM, "Memory capacity %lld (%lld) bytes\n", capacity,
-            AbstractMemory::size());
-
-    DPRINTF(DRAM, "Row buffer size %d bytes with %d columns per row buffer\n",
-            rowBufferSize, columnsPerRowBuffer);
+    //DPRINTF(DRAM, "Memory capacity %lld (%lld) bytes\n", capacity,
+    //        AbstractMemory::size());
 
     rowsPerBank = capacity / (rowBufferSize * banksPerRank * ranksPerChannel);
 
+    //DPRINTF(DRAM, "Row buffer size %d bytes with %d columns per row buffer. Rows per bank: %d\n",
+    //        rowBufferSize, columnsPerRowBuffer, rowsPerBank);
+
     // some basic sanity checks
     if (tREFI <= tRP || tREFI <= tRFC) {
         fatal("tREFI (%d) must be larger than tRP (%d) and tRFC (%d)\n",
@@ -243,6 +335,11 @@ DRAMCtrl::startup()
     // remember the memory system mode of operation
     isTimingMode = system()->isTimingMode();
 
+    //for(int i=0;i<=268436;i++){
+    //    counters.insert ( std::pair<int ,int>(i,0) );
+	//shadowPlus.insert(std::pair<int , bool>(i,false));
+    //}
+
     if (isTimingMode) {
         // timestamp offset should be in clock cycles for DRAMPower
         timeStampOffset = divCeil(curTick(), tCK);
@@ -258,13 +355,16 @@ DRAMCtrl::startup()
         // the next request, this will add an insignificant bubble at the
         // start of simulation
         nextBurstAt = curTick() + tRP + tRCD;
+        if(!XbarSendPacketEvent.scheduled()){
+        schedule(XbarSendPacketEvent, curTick()+1500);
+    }
     }
 }
 
 Tick
 DRAMCtrl::recvAtomic(PacketPtr pkt)
 {
-    DPRINTF(DRAM, "recvAtomic: %s 0x%x\n", pkt->cmdString(), pkt->getAddr());
+    ////DPRINTF(DRAM, "recvAtomic: %s 0x%x\n", pkt->cmdString(), pkt->getAddr());
 
     panic_if(pkt->cacheResponding(), "Should not see packets where cache "
              "is responding");
@@ -284,9 +384,9 @@ DRAMCtrl::recvAtomic(PacketPtr pkt)
 bool
 DRAMCtrl::readQueueFull(unsigned int neededEntries) const
 {
-    DPRINTF(DRAM, "Read queue limit %d, current size %d, entries needed %d\n",
-            readBufferSize, totalReadQueueSize + respQueue.size(),
-            neededEntries);
+    //DPRINTF(DRAM, "Read queue limit %d, current size %d, entries needed %d\n",
+    //        readBufferSize, totalReadQueueSize + respQueue.size(),
+    //        neededEntries);
 
     auto rdsize_new = totalReadQueueSize + respQueue.size() + neededEntries;
     return rdsize_new > readBufferSize;
@@ -295,8 +395,8 @@ DRAMCtrl::readQueueFull(unsigned int neededEntries) const
 bool
 DRAMCtrl::writeQueueFull(unsigned int neededEntries) const
 {
-    DPRINTF(DRAM, "Write queue limit %d, current size %d, entries needed %d\n",
-            writeBufferSize, totalWriteQueueSize, neededEntries);
+    //DPRINTF(DRAM, "Write queue limit %d, current size %d, entries needed %d\n",
+    //        writeBufferSize, totalWriteQueueSize, neededEntries);
 
     auto wrsize_new = (totalWriteQueueSize + neededEntries);
     return  wrsize_new > writeBufferSize;
@@ -397,8 +497,8 @@ DRAMCtrl::decodeAddr(PacketPtr pkt, Addr dramPktAddr, unsigned size,
     assert(row < rowsPerBank);
     assert(row < Bank::NO_ROW);
 
-    DPRINTF(DRAM, "Address: %lld Rank %d Bank %d Row %d\n",
-            dramPktAddr, rank, bank, row);
+    //DPRINTF(DRAM, "Address: %lld Rank %d Bank %d Row %d\n",
+    //        dramPktAddr, rank, bank, row);
 
     // create the corresponding DRAM packet with the entry time and
     // ready time set to the current tick, the latter will be updated
@@ -436,30 +536,30 @@ DRAMCtrl::addToReadQueue(PacketPtr pkt, unsigned int pktCount)
         // First check write buffer to see if the data is already at
         // the controller
         bool foundInWrQ = false;
-        Addr burst_addr = burstAlign(addr);
+        //Addr burst_addr = burstAlign(addr);
         // if the burst address is not present then there is no need
         // looking any further
-        if (isInWriteQueue.find(burst_addr) != isInWriteQueue.end()) {
-            for (const auto& vec : writeQueue) {
-                for (const auto& p : vec) {
+        //if (isInWriteQueue.find(burst_addr) != isInWriteQueue.end()) {
+        //    for (const auto& vec : writeQueue) {
+        //        for (const auto& p : vec) {
                     // check if the read is subsumed in the write queue
                     // packet we are looking at
-                    if (p->addr <= addr &&
-                       ((addr + size) <= (p->addr + p->size))) {
-
-                        foundInWrQ = true;
-                        servicedByWrQ++;
-                        pktsServicedByWrQ++;
-                        DPRINTF(DRAM,
-                                "Read to addr %lld with size %d serviced by "
-                                "write queue\n",
-                                addr, size);
-                        bytesReadWrQ += burstSize;
-                        break;
-                    }
-                }
-            }
-        }
+        //            if (p->addr <= addr &&
+        //               ((addr + size) <= (p->addr + p->size))) {
+
+        //                foundInWrQ = true;
+        //                servicedByWrQ++;
+        //                pktsServicedByWrQ++;
+        //                DPRINTF(DRAM,
+        //                        "Read to addr %lld with size %d serviced by "
+        //                        "write queue\n",
+        //                        addr, size);
+        //                bytesReadWrQ += burstSize;
+        //                break;
+        //            }
+        //        }
+        //    }
+        //}
 
         // If not found in the write q, make a DRAM packet and
         // push it onto the read queue
@@ -467,18 +567,20 @@ DRAMCtrl::addToReadQueue(PacketPtr pkt, unsigned int pktCount)
 
             // Make the burst helper for split packets
             if (pktCount > 1 && burst_helper == NULL) {
-                DPRINTF(DRAM, "Read to addr %lld translates to %d "
-                        "dram requests\n", pkt->getAddr(), pktCount);
+                //DPRINTF(DRAM, "Read to addr %lld translates to %d "
+                //        "dram requests\n", pkt->getAddr(), pktCount);
                 burst_helper = new BurstHelper(pktCount);
             }
 
             DRAMPacket* dram_pkt = decodeAddr(pkt, addr, size, true);
             dram_pkt->burstHelper = burst_helper;
+            DPRINTF(DRAM, "Read Request to addr %lld, rank/bank/row %d %d %d readQueue size: %d\n",
+                    dram_pkt->addr, dram_pkt->rank, dram_pkt->bank, dram_pkt->row, readQueue[dram_pkt->qosValue()].size());
 
             assert(!readQueueFull(1));
             rdQLenPdf[totalReadQueueSize + respQueue.size()]++;
 
-            DPRINTF(DRAM, "Adding to read queue\n");
+            //DPRINTF(DRAM, "Adding to read queue\n");
 
             readQueue[dram_pkt->qosValue()].push_back(dram_pkt);
 
@@ -509,7 +611,7 @@ DRAMCtrl::addToReadQueue(PacketPtr pkt, unsigned int pktCount)
     // If we are not already scheduled to get a request out of the
     // queue, do so now
     if (!nextReqEvent.scheduled()) {
-        DPRINTF(DRAM, "Request scheduled immediately\n");
+        //DPRINTF(DRAM, "Request scheduled immediately\n");
         schedule(nextReqEvent, curTick());
     }
 }
@@ -538,17 +640,20 @@ DRAMCtrl::addToWriteQueue(PacketPtr pkt, unsigned int pktCount)
 
         // if the item was not merged we need to create a new write
         // and enqueue it
-        if (!merged) {
+        if (!merged || 1) {
             DRAMPacket* dram_pkt = decodeAddr(pkt, addr, size, false);
 
+            dram_pkt->dly_due_to_bit_flips = pkt->dlyDueToBitFips;
+
             assert(totalWriteQueueSize < writeBufferSize);
             wrQLenPdf[totalWriteQueueSize]++;
 
-            DPRINTF(DRAM, "Adding to write queue\n");
-
             writeQueue[dram_pkt->qosValue()].push_back(dram_pkt);
             isInWriteQueue.insert(burstAlign(addr));
 
+            DPRINTF(DRAM, "Write Request to addr %lld, rank/bank/row %d %d %d writeQueueSize: %d\n",
+                    dram_pkt->addr, dram_pkt->rank, dram_pkt->bank, dram_pkt->row, isInWriteQueue.size());
+
             // log packet
             logRequest(MemCtrl::WRITE, pkt->masterId(), pkt->qosValue(),
                        dram_pkt->addr, 1);
@@ -582,7 +687,7 @@ DRAMCtrl::addToWriteQueue(PacketPtr pkt, unsigned int pktCount)
     // If we are not already scheduled to get a request out of the
     // queue, do so now
     if (!nextReqEvent.scheduled()) {
-        DPRINTF(DRAM, "Request scheduled immediately\n");
+        //DPRINTF(DRAM, "Request scheduled immediately\n");
         schedule(nextReqEvent, curTick());
     }
 }
@@ -591,33 +696,207 @@ void
 DRAMCtrl::printQs() const
 {
 #if TRACING_ON
-    DPRINTF(DRAM, "===READ QUEUE===\n\n");
-    for (const auto& queue : readQueue) {
-        for (const auto& packet : queue) {
-            DPRINTF(DRAM, "Read %lu\n", packet->addr);
-        }
-    }
+    //DPRINTF(DRAM, "===READ QUEUE===\n\n");
+    //for (const auto& queue : readQueue) {
+    //    for (const auto& packet : queue) {
+    //        //DPRINTF(DRAM, "Read %lu\n", packet->addr);
+    //    }
+    //}
+
+    //DPRINTF(DRAM, "\n===RESP QUEUE===\n\n");
+    //for (const auto& packet : respQueue) {
+        //DPRINTF(DRAM, "Response %lu\n", packet->addr);
+    //}
+
+    //DPRINTF(DRAM, "\n===WRITE QUEUE===\n\n");
+    //for (const auto& queue : writeQueue) {
+    //    for (const auto& packet : queue) {
+            //DPRINTF(DRAM, "Write %lu\n", packet->addr);
+    //    }
+    //}
+#endif // TRACING_ON
+}
 
-    DPRINTF(DRAM, "\n===RESP QUEUE===\n\n");
-    for (const auto& packet : respQueue) {
-        DPRINTF(DRAM, "Response %lu\n", packet->addr);
-    }
+void
+DRAMCtrl::processFakeEvent(){
+    //cout << "before calling fake" << endl;
+    if(XbarSendPacketEvent.scheduled())
+    deschedule(XbarSendPacketEvent);
+    schedule(XbarSendPacketEvent,curTick()+100);
+}
 
-    DPRINTF(DRAM, "\n===WRITE QUEUE===\n\n");
-    for (const auto& queue : writeQueue) {
-        for (const auto& packet : queue) {
-            DPRINTF(DRAM, "Write %lu\n", packet->addr);
-        }
+
+void
+DRAMCtrl::processSendPacketEvent(){
+
+	uint64_t to_wait=1000;
+
+    //cout << "sending dummy " << endl;
+
+    if(!dummyPacketQ.empty()){
+        //cout << " sending " << dummyPacketQ.front()->getAddr();
+    	to_wait=dummyPacketQ.front()->payloadDelay;
+    	//if(dummyPacketQ.front()->isRead()) cout << " r " << endl;
+    	//else if(dummyPacketQ.front()->isWrite()) cout << " w " << endl;
+        //cout << " -- " << dummyPacketQ.front()->payloadDelay << endl;
+    	if(actualRcvTiming(dummyPacketQ.front())) {
+    		if((dummyPacketQ.front())->isWrite())
+    			DPRINTF(DRAM, "processSendPacketEvent: write queue size: %d\n", totalWriteQueueSize);
+    		dummyPacketQ.pop_front();
+    	}
+    	else
+    		if((dummyPacketQ.front())->isWrite())
+    			DPRINTF(DRAM, "processSendPacketEvent: write queue full: %d\n", totalWriteQueueSize);
     }
-#endif // TRACING_ON
+    //cout << " -- " << dummyPacketQ.front()->payloadDelay << endl;
+    if(XbarFakeEvent.scheduled())
+    	deschedule(XbarFakeEvent);
+	//cout << "next schedule at " << to_wait << " ticks" <<endl;
+    schedule(XbarFakeEvent,curTick()+to_wait+2000 );   
+
+
 }
 
-bool
-DRAMCtrl::recvTimingReq(PacketPtr pkt)
+
+void 
+DRAMCtrl::sendDummyRead(){
+     //unsigned long long address =0;
+
+//cout << "event called : dummy read" << endl;
+
+    unsigned ofst = (64*(int)(random_mt.random(1, 10)));
+//AddrRange(pkt->getAddr(),pkt->getAddr() + (pkt->getSize() - 1)).isSubset(range)
+//cout << "sending dummy request "  << ofst <<  endl;   
+    PacketPtr dummy_pkt = nullptr;
+    
+    Request::Flags flags;
+
+    RequestPtr req = std::make_shared<Request>((unsigned long long int) 16*1024*1024*1024-ofst ,64, flags, Request::funcMasterId);
+    req->setContext(1);
+    dummy_pkt = new Packet(req, MemCmd::ReadReq);
+    dummy_pkt->dataDynamic(pkt_data_d);
+
+    //AddrRange addr_range = RangeSize(dummy_pkt->getAddr(), dummy_pkt->getSize());
+    //PortID master_port_id = findPort(addr_range);
+
+    //Tick old_header_delay = dummy_pkt->headerDelay;
+
+    // a request sees the frontend and forward latency
+    
+
+    // set the packet header and payload delay
+    
+
+    // determine how long to be crossbar layer is busy
+    // Tick packetFinishTime = clockEdge(Cycles(1)) + dummy_pkt->payloadDelay;
+
+    dummy_pkt->payloadDelay+=60000;
+    //dummy_pkt->payloadDelay+=10000000;
+
+    dummyPacketQ.push_back(dummy_pkt);
+    
+
+    //counter_read++;
+
+    //masterPorts[master_port_id]->sendTimingReq(dummy_pkt);
+
+
+    //recvTimingReq(pkt_data,slavePorts[1]);
+    //dummy_pkt->makeResponse();
+            
+    //unsigned size = dummy_pkt->getSize();
+    //unsigned offset = dummy_pkt->getAddr() & (burstSize - 1);
+    //unsigned int dram_pkt_count = divCeil(offset + size, burstSize);
+    //packet->dataDynamic(pkt_data);
+    //addToReadQueue(packet, dram_pkt_count);
+    //readReqs++;
+    //bytesReadSys += size;
+    //writeReqs++;
+    //bytesWrittenSys += size;
+
+    //port.schedTimingResp(dummy_pkt, curTick() + 100);
+
+/*if(!XbardummyReadEvent.scheduled()){
+    schedule(XbardummyReadEvent,curTick()+2000);
+}*/
+
+//schedule(XbarSendPacketEvent,curTick()+1000);
+
+    
+}
+
+void 
+DRAMCtrl::sendDummyWrite(){
+     //unsigned long long address =0;
+
+    //cout << "event called : dummy write--" << endl;
+
+    unsigned ofst = (64*(int)(random_mt.random(1, 10)));
+//AddrRange(pkt->getAddr(),pkt->getAddr() + (pkt->getSize() - 1)).isSubset(range)
+//cout << "sending dummy request "  << ofst <<  endl;   
+    PacketPtr dummy_pkt = nullptr;
+    
+    Request::Flags flags;
+
+    RequestPtr req = std::make_shared<Request>((unsigned long long int) 16*1024*1024*1024-ofst ,64, flags, Request::funcMasterId);
+    req->setContext(1);
+    dummy_pkt = new Packet(req, MemCmd::WritebackDirty);
+    dummy_pkt->dataDynamic(pkt_data_d);
+
+    //AddrRange addr_range = RangeSize(dummy_pkt->getAddr(), dummy_pkt->getSize());
+    //PortID master_port_id = findPort(addr_range);
+
+    //Tick old_header_delay = dummy_pkt->headerDelay;
+
+    // a request sees the frontend and forward latency
+    //Tick xbar_delay = (frontendLatency + forwardLatency) * clockPeriod();
+
+    // set the packet header and payload delay
+    //calcPacketTiming(dummy_pkt, xbar_delay);
+
+    // determine how long to be crossbar layer is busy
+    // Tick packetFinishTime = clockEdge(Cycles(1)) + dummy_pkt->payloadDelay;
+
+    dummy_pkt->payloadDelay+=150000;
+    //dummy_pkt->payloadDelay+=10000000;
+
+    dummyPacketQ.push_back(dummy_pkt);
+    //portQueue.push_back(portId);
+    //counter_write++;
+
+    //masterPorts[master_port_id]->sendTimingReq(dummy_pkt);
+
+
+    //recvTimingReq(pkt_data,slavePorts[1]);
+    //dummy_pkt->makeResponse();
+            
+    //unsigned size = dummy_pkt->getSize();
+    //unsigned offset = dummy_pkt->getAddr() & (burstSize - 1);
+    //unsigned int dram_pkt_count = divCeil(offset + size, burstSize);
+    //packet->dataDynamic(pkt_data);
+    //addToReadQueue(packet, dram_pkt_count);
+    //readReqs++;
+    //bytesReadSys += size;
+    //writeReqs++;
+    //bytesWrittenSys += size;
+
+    //port.schedTimingResp(dummy_pkt, curTick() + 100);
+
+/*
+if(!XbardummyWriteEvent.scheduled()){
+    schedule(XbardummyWriteEvent,curTick()+2000);
+}*/
+
+//schedule(XbarSendPacketEvent,curTick()+1010);
+    
+}
+
+bool 
+DRAMCtrl::actualRcvTiming(PacketPtr pkt)
 {
-    // This is where we enter from the outside world
-    DPRINTF(DRAM, "recvTimingReq: request %s addr %lld size %d\n",
-            pkt->cmdString(), pkt->getAddr(), pkt->getSize());
+
+    //DPRINTF(DRAM, "recvTimingReq: request %s addr %lld size %d\n",
+    //        pkt->cmdString(), pkt->getAddr(), pkt->getSize());
 
     panic_if(pkt->cacheResponding(), "Should not see packets where cache "
              "is responding");
@@ -643,11 +922,15 @@ DRAMCtrl::recvTimingReq(PacketPtr pkt)
     // run the QoS scheduler and assign a QoS priority value to the packet
     qosSchedule( { &readQueue, &writeQueue }, burstSize, pkt);
 
+    //cout << pkt->getAddr() << endl;
+
+    
+
     // check local buffers and do not accept if full
     if (pkt->isRead()) {
         assert(size != 0);
         if (readQueueFull(dram_pkt_count)) {
-            DPRINTF(DRAM, "Read queue full, not accepting\n");
+            //DPRINTF(DRAM, "Read queue full, not accepting\n");
             // remember that we have to retry this port
             retryRdReq = true;
             numRdRetry++;
@@ -661,7 +944,7 @@ DRAMCtrl::recvTimingReq(PacketPtr pkt)
         assert(pkt->isWrite());
         assert(size != 0);
         if (writeQueueFull(dram_pkt_count)) {
-            DPRINTF(DRAM, "Write queue full, not accepting\n");
+            //DPRINTF(DRAM, "Write queue full, not accepting\n");
             // remember that we have to retry this port
             retryWrReq = true;
             numWrRetry++;
@@ -676,75 +959,212 @@ DRAMCtrl::recvTimingReq(PacketPtr pkt)
     return true;
 }
 
-void
-DRAMCtrl::processRespondEvent()
+
+bool
+DRAMCtrl::recvTimingReq(PacketPtr pkt)
 {
-    DPRINTF(DRAM,
-            "processRespondEvent(): Some req has reached its readyTime\n");
 
-    DRAMPacket* dram_pkt = respQueue.front();
+	//actualRcvTiming(pkt,slave_port_id);
+
+	uint64_t evicted = 0xffffffffff;
+
+	//// counter cache part
+	if(pkt->isRead()){
+		DPRINTF(DRAM, "recvTimingReq: read: %s\n",pkt->getBlockAddr(64));
+		if(enableEncryption)
+		{
+			if(cc->hasCounter(pkt->getAddr())){
+				hit_count++;
+			}
+			else{
+				miss_count++;
+				sendDummyRead();										// read a packet to memory to get the counter value
+				evicted = cc->PushInCache(pkt->getAddr(),'w');			// replace the received counter with the existing counter in the counter cache
+				if(evicted!=0xffffffffff){
+					sendDummyWrite();
+				}
+				else{
+				}
+			}
+
+			// receive the counter value. Perform actual decryption
+
+			//pkt->clearModifiedWordsFlag();
+
+			if(bitFlipsDelay) {
+				if(modified_words_flags.find(pkt->getBlockAddr(64)) == modified_words_flags.end())
+				{
+					modified_words_flags[pkt->getBlockAddr(64)] = new vector<bool>;
+					for(int i=0; i<32; i++)
+						modified_words_flags[pkt->getBlockAddr(64)]->push_back(0);
+				}
+
+				for(int i=0; i<32; i++) {
+					pkt->modifiedWordsFlag[i] = 0; //(*(modified_words_flags[pkt->getBlockAddr(64)]))[i];
+				}
+			}
+		}
+		pkt->payloadDelay+=60000;		// add delay for reading
+	}
+	else if(pkt->isWrite()){
+		DPRINTF(DRAM, "recvTimingReq: write: %s\n",pkt->getBlockAddr(64));
+		/*if(enableEncryption)
+		{
+			//sendDummyWrite();
+			if(cc->hasCounter(pkt->getAddr())){
+				hit_count++;
+				//cout << "cache hit while writing" << endl;
+			}
+			else{
+				//cout << "miss while writing-- fetching from memory" << endl;
+				miss_count++;
+				sendDummyRead();
+				evicted = cc->PushInCache(pkt->getAddr(),'w');
+				if(evicted!=0xffffffffff){
+					sendDummyWrite();
+				}
+				else{
+				}
+			}
+
+			//DPRINTF(DRAM, "recvTimingReq: write: %s\n",pkt->getBlockAddr(64));
+
+			if(bitFlipsDelay) {
+				if(encryptionMethod == 10) sendDummyRead();
+
+				//if(encryptionMethod == 12 || encryptionMethod == 13) {
+				if(!getDelayDueToBitFlips(pkt)) {
+					llcMissrate += 1;
+					accessAndRespond(pkt, frontendLatency);
+
+					// If we are not already scheduled to get a request out of the
+					// queue, do so now
+					if (!nextReqEvent.scheduled()) {
+						//DPRINTF(DRAM, "Request scheduled immediately\n");
+						schedule(nextReqEvent, curTick());
+					}
+					return true;
+				}
+			}
+		}*/
+		//pkt->payloadDelay+=150000;
+	}
+
+	//cout << "CacheMissRate " << (int)((float)miss_count/(float)(hit_count+miss_count)*100.0) << endl;
+
+	llcMissrate += 1;
+
+
+	dummyPacketQ.push_back(pkt);
 
-    // if a read has reached its ready-time, decrement the number of reads
-    // At this point the packet has been handled and there is a possibility
-    // to switch to low-power mode if no other packet is available
-    --dram_pkt->rankRef.readEntries;
-    DPRINTF(DRAM, "number of read entries for rank %d is %d\n",
-            dram_pkt->rank, dram_pkt->rankRef.readEntries);
+    return true;
 
-    // counter should at least indicate one outstanding request
-    // for this read
-    assert(dram_pkt->rankRef.outstandingEvents > 0);
-    // read response received, decrement count
-    --dram_pkt->rankRef.outstandingEvents;
-
-    // at this moment should not have transitioned to a low-power state
-    assert((dram_pkt->rankRef.pwrState != PWR_SREF) &&
-           (dram_pkt->rankRef.pwrState != PWR_PRE_PDN) &&
-           (dram_pkt->rankRef.pwrState != PWR_ACT_PDN));
-
-    // track if this is the last packet before idling
-    // and that there are no outstanding commands to this rank
-    if (dram_pkt->rankRef.isQueueEmpty() &&
-        dram_pkt->rankRef.outstandingEvents == 0) {
-        // verify that there are no events scheduled
-        assert(!dram_pkt->rankRef.activateEvent.scheduled());
-        assert(!dram_pkt->rankRef.prechargeEvent.scheduled());
-
-        // if coming from active state, schedule power event to
-        // active power-down else go to precharge power-down
-        DPRINTF(DRAMState, "Rank %d sleep at tick %d; current power state is "
-                "%d\n", dram_pkt->rank, curTick(), dram_pkt->rankRef.pwrState);
-
-        // default to ACT power-down unless already in IDLE state
-        // could be in IDLE if PRE issued before data returned
-        PowerState next_pwr_state = PWR_ACT_PDN;
-        if (dram_pkt->rankRef.pwrState == PWR_IDLE) {
-            next_pwr_state = PWR_PRE_PDN;
-        }
+}
 
-        dram_pkt->rankRef.powerDownSleep(next_pwr_state, curTick());
-    }
-
-    if (dram_pkt->burstHelper) {
-        // it is a split packet
-        dram_pkt->burstHelper->burstsServiced++;
-        if (dram_pkt->burstHelper->burstsServiced ==
-            dram_pkt->burstHelper->burstCount) {
-            // we have now serviced all children packets of a system packet
-            // so we can now respond to the requester
-            // @todo we probably want to have a different front end and back
-            // end latency for split packets
-            accessAndRespond(dram_pkt->pkt, frontendLatency + backendLatency);
-            delete dram_pkt->burstHelper;
-            dram_pkt->burstHelper = NULL;
-        }
-    } else {
-        // it is not a split packet
-        accessAndRespond(dram_pkt->pkt, frontendLatency + backendLatency);
-    }
+void
+DRAMCtrl::processRespondEvent()
+{
+    //DPRINTF(DRAM,
+    //        "processRespondEvent(): Some req has reached its readyTime\n");
+
+    DRAMPacket* dram_pkt = respQueue.front();
 
-    delete respQueue.front();
-    respQueue.pop_front();
+    DPRINTF(DRAM,
+            "processRespondEvent(): req: %lu has reached its readyTime at: %d\n",dram_pkt->addr, (dram_pkt->readyTime - dram_pkt->entryTime) );
+
+    //if(reram && dram_pkt->isWrite())
+    //{
+		//removed write from queue, decrement count
+	//	--dram_pkt->rankRef.writeEntries;
+
+		//if (!dram_pkt->rankRef.writeDoneEvent.scheduled()) {
+		//	schedule(dram_pkt->rankRef.writeDoneEvent, dram_pkt->readyTime);
+			// New event, increment count
+		//++dram_pkt->rankRef.outstandingEvents;
+
+		//} else if (dram_pkt->rankRef.writeDoneEvent.when() <
+		//		   dram_pkt->readyTime) {
+
+		//	reschedule(dram_pkt->rankRef.writeDoneEvent, dram_pkt->readyTime);
+		//}
+
+	    //assert(dram_pkt->rankRef.outstandingEvents > 0);
+	    // read response received, decrement count
+	    //--dram_pkt->rankRef.outstandingEvents;
+
+	    //accessAndRespond(dram_pkt->pkt, frontendLatency + backendLatency);
+
+    //}
+    //else
+    //{
+		// if a read has reached its ready-time, decrement the number of reads
+		// At this point the packet has been handled and there is a possibility
+		// to switch to low-power mode if no other packet is available
+		--dram_pkt->rankRef.readEntries;
+		//DPRINTF(DRAM, "number of read entries for rank %d is %d\n",
+		//        dram_pkt->rank, dram_pkt->rankRef.readEntries);
+    //}
+	// counter should at least indicate one outstanding request
+	// for this read
+	assert(dram_pkt->rankRef.outstandingEvents > 0);
+	// read response received, decrement count
+	--dram_pkt->rankRef.outstandingEvents;
+
+	// at this moment should not have transitioned to a low-power state
+	assert((dram_pkt->rankRef.pwrState != PWR_SREF) &&
+		   (dram_pkt->rankRef.pwrState != PWR_PRE_PDN) &&
+		   (dram_pkt->rankRef.pwrState != PWR_ACT_PDN));
+
+	// track if this is the last packet before idling
+	// and that there are no outstanding commands to this rank
+	if (dram_pkt->rankRef.isQueueEmpty() &&
+		dram_pkt->rankRef.outstandingEvents == 0) {
+		// verify that there are no events scheduled
+		assert(!dram_pkt->rankRef.activateEvent.scheduled());
+		assert(!dram_pkt->rankRef.prechargeEvent.scheduled());
+
+		// if coming from active state, schedule power event to
+		// active power-down else go to precharge power-down
+		DPRINTF(DRAMState, "Rank %d sleep at tick %d; current power state is "
+				"%d\n", dram_pkt->rank, curTick(), dram_pkt->rankRef.pwrState);
+
+		// default to ACT power-down unless already in IDLE state
+		// could be in IDLE if PRE issued before data returned
+		PowerState next_pwr_state = PWR_ACT_PDN;
+		if (dram_pkt->rankRef.pwrState == PWR_IDLE) {
+			next_pwr_state = PWR_PRE_PDN;
+		}
+
+		dram_pkt->rankRef.powerDownSleep(next_pwr_state, curTick());
+	}
+
+	//if((!reram) && dram_pkt->isRead())
+	//{
+		if (dram_pkt->burstHelper) {
+			// it is a split packet
+			dram_pkt->burstHelper->burstsServiced++;
+			if (dram_pkt->burstHelper->burstsServiced ==
+				dram_pkt->burstHelper->burstCount) {
+				// we have now serviced all children packets of a system packet
+				// so we can now respond to the requester
+				// @todo we probably want to have a different front end and back
+				// end latency for split packets
+				accessAndRespond(dram_pkt->pkt, frontendLatency + backendLatency);
+				delete dram_pkt->burstHelper;
+				dram_pkt->burstHelper = NULL;
+			}
+		} else {
+			// it is not a split packet
+			accessAndRespond(dram_pkt->pkt, frontendLatency + backendLatency);
+		}
+	//}
+	////else{
+	//	DPRINTF(DRAMState, "response addr: %lu at %d", curTick());
+	//	accessAndRespond(dram_pkt->pkt, frontendLatency + backendLatency);
+	//}
+
+	delete respQueue.front();
+	respQueue.pop_front();
 
     if (!respQueue.empty()) {
         assert(respQueue.front()->readyTime >= curTick());
@@ -781,9 +1201,9 @@ DRAMCtrl::chooseNext(DRAMPacketQueue& queue, Tick extra_col_delay)
             DRAMPacket* dram_pkt = *(queue.begin());
             if (ranks[dram_pkt->rank]->inRefIdleState()) {
                 ret = queue.begin();
-                DPRINTF(DRAM, "Single request, going to a free rank\n");
+                //DPRINTF(DRAM, "Single request, going to a free rank\n");
             } else {
-                DPRINTF(DRAM, "Single request, going to a busy rank\n");
+                //DPRINTF(DRAM, "Single request, going to a busy rank\n");
             }
         } else if (memSchedPolicy == Enums::fcfs) {
             // check if there is a packet going to a free rank
@@ -840,16 +1260,16 @@ DRAMCtrl::chooseNextFRFCFS(DRAMPacketQueue& queue, Tick extra_col_delay)
         const Tick col_allowed_at = dram_pkt->isRead() ? bank.rdAllowedAt :
                                                          bank.wrAllowedAt;
 
-        DPRINTF(DRAM, "%s checking packet in bank %d\n",
-                __func__, dram_pkt->bankRef.bank);
+        //DPRINTF(DRAM, "%s checking packet in bank %d\n",
+        //        __func__, dram_pkt->bankRef.bank);
 
         // check if rank is not doing a refresh and thus is available, if not,
         // jump to the next packet
         if (dram_pkt->rankRef.inRefIdleState()) {
 
-            DPRINTF(DRAM,
-                    "%s bank %d - Rank %d available\n", __func__,
-                    dram_pkt->bankRef.bank, dram_pkt->rankRef.rank);
+            //DPRINTF(DRAM,
+            //        "%s bank %d - Rank %d available\n", __func__,
+            //        dram_pkt->bankRef.bank, dram_pkt->rankRef.rank);
 
             // check if it is a row hit
             if (bank.openRow == dram_pkt->row) {
@@ -861,7 +1281,7 @@ DRAMCtrl::chooseNextFRFCFS(DRAMPacketQueue& queue, Tick extra_col_delay)
                     // commands that can issue seamlessly, without
                     // additional delay, such as same rank accesses
                     // and/or different bank-group accesses
-                    DPRINTF(DRAM, "%s Seamless row buffer hit\n", __func__);
+                    //DPRINTF(DRAM, "%s Seamless row buffer hit\n", __func__);
                     selected_pkt_it = i;
                     // no need to look through the remaining queue entries
                     break;
@@ -872,7 +1292,7 @@ DRAMCtrl::chooseNextFRFCFS(DRAMPacketQueue& queue, Tick extra_col_delay)
                     // the current one
                     selected_pkt_it = i;
                     found_prepped_pkt = true;
-                    DPRINTF(DRAM, "%s Prepped row buffer hit\n", __func__);
+                    //DPRINTF(DRAM, "%s Prepped row buffer hit\n", __func__);
                 }
             } else if (!found_earliest_pkt) {
                 // if we have not initialised the bank status, do it
@@ -901,13 +1321,13 @@ DRAMCtrl::chooseNextFRFCFS(DRAMPacketQueue& queue, Tick extra_col_delay)
                 }
             }
         } else {
-            DPRINTF(DRAM, "%s bank %d - Rank %d not available\n", __func__,
-                    dram_pkt->bankRef.bank, dram_pkt->rankRef.rank);
+            //DPRINTF(DRAM, "%s bank %d - Rank %d not available\n", __func__,
+            //        dram_pkt->bankRef.bank, dram_pkt->rankRef.rank);
         }
     }
 
     if (selected_pkt_it == queue.end()) {
-        DPRINTF(DRAM, "%s no available ranks found\n", __func__);
+        //DPRINTF(DRAM, "%s no available ranks found\n", __func__);
     }
 
     return selected_pkt_it;
@@ -916,13 +1336,16 @@ DRAMCtrl::chooseNextFRFCFS(DRAMPacketQueue& queue, Tick extra_col_delay)
 void
 DRAMCtrl::accessAndRespond(PacketPtr pkt, Tick static_latency)
 {
-    DPRINTF(DRAM, "Responding to Address %lld.. ",pkt->getAddr());
+    //DPRINTF(DRAM, "Responding to Address %lld.. ",pkt->getAddr());
 
     bool needsResponse = pkt->needsResponse();
     // do the actual memory access which also turns the packet into a
     // response
     access(pkt);
-
+    //if(pkt->isWrite()) {
+    //	DPRINTF(DRAM, " Needs response %d.. ",needsResponse);
+    //	needsResponse = 0;
+    //}
     // turn packet around to go back to requester if response expected
     if (needsResponse) {
         // access already turned the packet into a response
@@ -936,6 +1359,16 @@ DRAMCtrl::accessAndRespond(PacketPtr pkt, Tick static_latency)
         // Here we reset the timing of the packet before sending it out.
         pkt->headerDelay = pkt->payloadDelay = 0;
 
+		if(modified_words_flags.find(pkt->getBlockAddr(64)) == modified_words_flags.end())
+		{
+			modified_words_flags[pkt->getBlockAddr(64)] = new vector<bool>;
+			for(int i=0; i<32; i++)
+				modified_words_flags[pkt->getBlockAddr(64)]->push_back(0);
+		}
+
+		for(int i=0; i<32; i++)
+			pkt->modifiedWordsFlag[i] = 0; //(*(modified_words_flags[pkt->getBlockAddr(64)]))[i];
+
         // queue the packet in the response queue to be sent out after
         // the static latency has passed
         port.schedTimingResp(pkt, response_time, true);
@@ -956,7 +1389,7 @@ DRAMCtrl::activateBank(Rank& rank_ref, Bank& bank_ref,
 {
     assert(rank_ref.actTicks.size() == activationLimit);
 
-    DPRINTF(DRAM, "Activate at tick %d\n", act_tick);
+    //DPRINTF(DRAM, "Activate at tick %d\n", act_tick);
 
     // update the open row
     assert(bank_ref.openRow == Bank::NO_ROW);
@@ -971,9 +1404,9 @@ DRAMCtrl::activateBank(Rank& rank_ref, Bank& bank_ref,
     ++rank_ref.numBanksActive;
     assert(rank_ref.numBanksActive <= banksPerRank);
 
-    DPRINTF(DRAM, "Activate bank %d, rank %d at tick %lld, now got %d active\n",
-            bank_ref.bank, rank_ref.rank, act_tick,
-            ranks[rank_ref.rank]->numBanksActive);
+    //DPRINTF(DRAM, "Activate bank %d, rank %d at tick %lld, now got %d active\n",
+    //        bank_ref.bank, rank_ref.rank, act_tick,
+    //        ranks[rank_ref.rank]->numBanksActive);
 
     rank_ref.cmdList.push_back(Command(MemCommand::ACT, bank_ref.bank,
                                act_tick));
@@ -1031,9 +1464,9 @@ DRAMCtrl::activateBank(Rank& rank_ref, Bank& bank_ref,
         // oldest in our window of X
         if (rank_ref.actTicks.back() &&
            (act_tick - rank_ref.actTicks.back()) < tXAW) {
-            DPRINTF(DRAM, "Enforcing tXAW with X = %d, next activate "
-                    "no earlier than %llu\n", activationLimit,
-                    rank_ref.actTicks.back() + tXAW);
+            //DPRINTF(DRAM, "Enforcing tXAW with X = %d, next activate "
+            //        "no earlier than %llu\n", activationLimit,
+            //        rank_ref.actTicks.back() + tXAW);
             for (int j = 0; j < banksPerRank; j++)
                 // next activate must not happen before end of window
                 rank_ref.banks[j].actAllowedAt =
@@ -1073,9 +1506,9 @@ DRAMCtrl::prechargeBank(Rank& rank_ref, Bank& bank, Tick pre_at, bool trace)
     assert(rank_ref.numBanksActive != 0);
     --rank_ref.numBanksActive;
 
-    DPRINTF(DRAM, "Precharging bank %d, rank %d at tick %lld, now got "
-            "%d active\n", bank.bank, rank_ref.rank, pre_at,
-            rank_ref.numBanksActive);
+    //DPRINTF(DRAM, "Precharging bank %d, rank %d at tick %lld, now got "
+    //        "%d active\n", bank.bank, rank_ref.rank, pre_at,
+    //        rank_ref.numBanksActive);
 
     if (trace) {
 
@@ -1099,11 +1532,1280 @@ DRAMCtrl::prechargeBank(Rank& rank_ref, Bank& bank, Tick pre_at, bool trace)
     }
 }
 
+Tick
+DRAMCtrl::getDelayDueToBitFlips(PacketPtr pkt)
+{
+	//PacketPtr pkt = dram_pkt->pkt;
+	Tick delay = tWR;
+
+	uint512_t encrypted_data;
+	std::vector<uint8_t> actual_data_bytes(pkt->getSize());
+	std::vector<uint8_t> stored_data_bytes(pkt->getSize());
+	std::vector<uint8_t> updated_data_bytes(pkt->getSize());
+	uint512_t actual_data=0, stored_data=0, updated_data=0;
+
+	getMemoryData(pkt, &actual_data_bytes[0], &stored_data_bytes[0]);
+	pkt->writeData(&updated_data_bytes[0]);
+
+	if(modified_words_flags.find(pkt->getBlockAddr(64)) == modified_words_flags.end())
+	{
+		modified_words_flags[pkt->getBlockAddr(64)] = new vector<bool>;
+		for(int i=0; i<32; i++) modified_words_flags[pkt->getBlockAddr(64)]->push_back(0);
+	}
+	for(int i=0; i<32; i++) {
+		bool temp = (*(modified_words_flags[pkt->getBlockAddr(64)]))[i];
+		(*(modified_words_flags[pkt->getBlockAddr(64)]))[i] = temp | pkt->modifiedWordsFlag[i];
+	}
+
+	if(pkt->hasData())
+	{
+		for (int i=0; i<pkt->getSize(); i++) {
+			actual_data = actual_data<<8 | actual_data_bytes[i];
+			stored_data = stored_data<<8 | stored_data_bytes[(pkt->getSize())-1 - i];
+			updated_data = updated_data<<8 | updated_data_bytes[i];
+		}
+	}
+	uint512_t compData = (actual_data) ^ (updated_data);
+	if(compData == 0) {writesWithOutWordChanges += 1;}//pkt->dlyDueToBitFips = 0;return true;}
+
+	totalCacheLineWrites += 1;
+	int wordsModifiedThisWrite = 0;
+	int wordsModifiedThisWriteIndex = 0;
+	for(int i=0; i<32; i++) { if(pkt->modifiedWordsFlag[i]) { totalModifiedWords += 1; wordsModifiedThisWrite++; wordsModifiedThisWriteIndex = i;}}
+	if(!wordsModifiedThisWrite) { actualWritesWithOutWordChanges += 1; pkt->dlyDueToBitFips = 0;return false; }
+
+	// received the counter value. Update it and encrypt the data and send the updated counter to the cache.
+	switch(encryptionMethod) {
+	case 0: {
+		//DCW No Encryption
+		encrypted_data = updated_data;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		uint512_t x = (stored_data) ^ (encrypted_data);
+		unsigned bit_flips = 0;
+		while (x > 0) {	bit_flips = bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; }
+		bitFlips += bit_flips;
+		int write_slots = 1 + (bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		numWriteReqs++;
+	}
+		break;
+
+	case 1: {
+		//DCW With Encryption
+		uint512_t PAD = gen512(); //rand_value(gen); //counter ^ AES;
+		encrypted_data = PAD ^ (updated_data);
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		uint512_t x = (stored_data) ^ (encrypted_data);
+		unsigned bit_flips = 0;
+		while (x > 0) {	bit_flips = bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; }
+		bitFlips += bit_flips;
+		int write_slots = 1 +(bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		numWriteReqs++;
+	}
+		break;
+
+	case 2: {
+		//FNW No Encryption
+		encrypted_data = updated_data;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		uint512_t x = (stored_data) ^ (encrypted_data);
+		unsigned bit_flips = 0;
+		while (x > 0) {	bit_flips = bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; }
+		if(bit_flips > 512/2)
+			bit_flips = 512-bit_flips;
+
+		bitFlips += bit_flips;
+		int write_slots = 1 + (bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		numWriteReqs++;
+	}
+		break;
+
+	case 3: {
+		//FNW with Encryption
+		uint512_t PAD = gen512(); //rand_value(gen); //counter ^ AES;
+		encrypted_data = PAD ^ (updated_data);
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		uint512_t x = (stored_data) ^ (encrypted_data);
+		unsigned bit_flips = 0;
+		while (x > 0) {	bit_flips = bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; }
+		if(bit_flips > 512/2)
+			bit_flips = 512-bit_flips;
+
+		bitFlips += bit_flips;
+		int write_slots = 1 + (bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		numWriteReqs++;
+	}
+		break;
+
+	case 4: {
+		//TSECME
+		unsigned bit_flips = 0;
+		unsigned best_flips = 0;
+		int best_flips_index = 0;
+		uint512_t best_flips_encrypted_data = 0;
+		int counter = 0;
+		uint512_t PAD;
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+
+		while(counter < 16) {
+			counter++;
+			bit_flips = 0;
+			PAD = gen512();
+
+			// PAD = PAD << (512-modified_words*16); //2 bytes(16bits per word)
+			encrypted_data = PAD ^ (updated_data);
+
+			uint512_t x = (stored_data) ^ (encrypted_data);
+			while (x > 0) {	bit_flips = bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+			if (best_flips == 0) {
+				best_flips =  bit_flips;
+				best_flips_encrypted_data = encrypted_data;
+				best_flips_index = counter;
+			}
+			else {
+				best_flips = (best_flips < bit_flips) ? best_flips : bit_flips;
+				best_flips_encrypted_data = (best_flips < bit_flips) ? best_flips_encrypted_data : encrypted_data;
+				best_flips_index = (best_flips < bit_flips) ? best_flips_index : counter;
+			}
+		}
+
+		encrypted_data = best_flips_encrypted_data;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		totalEncryptValSkpd += best_flips_index;
+		totalEncryptValTested += counter;
+		bitFlips += best_flips;
+		int write_slots = 1 + (best_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		pkt->payloadDelay += 25000;
+		numWriteReqs++;
+	}
+		break;
+
+	case 5: {
+
+		int counter = 0;
+		uint512_t bestPAD = 0;
+		uint512_t PAD;
+		int pad_hamming_distance = 0;
+		int best_pad_hamming_distance = 0;
+		int best_pad_index = 0;
+		int best_data_bit_flips = 0;
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			trailPAD[cacheline] = leadPAD[cacheline] = gen512();
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+
+		while(counter < 16) {
+			counter++;
+			pad_hamming_distance = 0;
+			PAD = gen512();
+
+			uint512_t x = (trailPAD[cacheline]) ^ (PAD);
+			while (x > 0) {	pad_hamming_distance = pad_hamming_distance + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+			if (best_pad_hamming_distance == 0) {
+				bestPAD = PAD;
+				best_pad_index = counter;
+				best_pad_hamming_distance = pad_hamming_distance;
+			}
+			else {
+				bestPAD = (best_pad_hamming_distance < pad_hamming_distance) ? bestPAD : PAD;
+				best_pad_index = (best_pad_hamming_distance < pad_hamming_distance) ? best_pad_index : counter;
+				best_pad_hamming_distance = (best_pad_hamming_distance < pad_hamming_distance) ? best_pad_hamming_distance : pad_hamming_distance;
+			}
+		}
+
+		trailPAD[cacheline] = bestPAD;
+		encrypted_data = bestPAD ^ updated_data;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		best_data_bit_flips = 0;
+		uint512_t x = stored_data ^ encrypted_data;
+		while (x > 0) {	best_data_bit_flips = best_data_bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+		totalEncryptValSkpd += best_pad_index;
+		totalEncryptValTested += counter;
+		bitFlips += best_data_bit_flips;
+		int write_slots = 1 + (best_data_bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		pkt->payloadDelay += 25000;
+		numWriteReqs++;
+	}
+		break;
+
+	case 6: {
+
+		uint128_t best_PAD[4] = {0};
+		uint128_t trail_PAD[4] = {0};
+		int best_pad_hamming_distance[4] = {0};
+		int best_pad_index[4] = {0};
+		int best_data_bit_flips = {0};
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			trailPAD[cacheline] = leadPAD[cacheline] = gen512();
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+
+		for (int i=0; i<4; i++) {
+			trail_PAD[i] = (uint128_t)(trailPAD[cacheline] >> 128*i);
+
+			int counter = 0;
+			while(counter < 16) {
+				counter++;
+				uint128_t PAD;
+
+				int pad_hamming_distance = 0;
+				PAD = gen128();
+
+				uint128_t x = (trail_PAD[i]) ^ (PAD);
+				while (x > 0) {	pad_hamming_distance = pad_hamming_distance + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+				if (best_pad_hamming_distance[i] == 0) {
+					best_PAD[i] = PAD;
+					best_pad_index[i] = counter;
+					best_pad_hamming_distance[i] = pad_hamming_distance;
+				}
+				else {
+					best_PAD[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_PAD[i] : PAD;
+					best_pad_index[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_pad_index[i] : counter;
+					best_pad_hamming_distance[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_pad_hamming_distance[i] : pad_hamming_distance;
+				}
+
+				std::cout << " subPAD: " << i << " phd: " << pad_hamming_distance;
+			}
+
+			std::cout << " subPAD: " << i << " bphd: " << best_pad_hamming_distance[i];
+		}
+		std::cout << "" << endl;
+
+		for (int i=0; i<4; i++) {
+			trailPAD[cacheline] = (trailPAD[cacheline] << 128) | best_PAD[4-i-1];
+		}
+
+		encrypted_data = trailPAD[cacheline] ^ updated_data;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		best_data_bit_flips = 0;
+		uint512_t x = stored_data ^ encrypted_data;
+		while (x > 0) {	best_data_bit_flips = best_data_bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+		//totalEncryptValSkpd += best_pad_index;
+		//totalEncryptValTested += counter;
+		bitFlips += best_data_bit_flips;
+		int write_slots = 1 + (best_data_bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		pkt->payloadDelay += 25000;
+		numWriteReqs++;
+	}
+		break;
+
+	case 7: {
+
+		uint64_t best_PAD[8] = {0};
+		uint64_t trail_PAD[8] = {0};
+		int best_pad_hamming_distance[8] = {0};
+		int best_pad_index[8] = {0};
+		int best_data_bit_flips = {0};
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			trailPAD[cacheline] = leadPAD[cacheline] = gen512();
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+
+		for (int i=0; i<8; i++) {
+			trail_PAD[i] = (uint64_t)(trailPAD[cacheline] >> 64*i);
+
+			int counter = 0;
+			while(counter < 16) {
+				counter++;
+				uint64_t PAD[8];
+
+				int pad_hamming_distance = 0;
+				PAD[i] = gen64();
+
+				uint64_t x = (trail_PAD[i]) ^ (PAD[i]);
+				while (x > 0) {	pad_hamming_distance = pad_hamming_distance + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+				if (best_pad_hamming_distance[i] == 0) {
+					best_PAD[i] = PAD[i];
+					best_pad_index[i] = counter;
+					best_pad_hamming_distance[i] = pad_hamming_distance;
+				}
+				else {
+					best_PAD[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_PAD[i] : PAD[i];
+					best_pad_index[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_pad_index[i] : counter;
+					best_pad_hamming_distance[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_pad_hamming_distance[i] : pad_hamming_distance;
+				}
+			}
+		}
+
+		for (int i=0; i<8; i++) {
+			trailPAD[cacheline] = (trailPAD[cacheline] << 64) | best_PAD[8-i-1];
+		}
+
+		encrypted_data = trailPAD[cacheline] ^ updated_data;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		best_data_bit_flips = 0;
+		uint512_t x = stored_data ^ encrypted_data;
+		while (x > 0) {	best_data_bit_flips = best_data_bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+		//totalEncryptValSkpd += best_pad_index;
+		//totalEncryptValTested += counter;
+		bitFlips += best_data_bit_flips;
+		int write_slots = 1 + (best_data_bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		pkt->payloadDelay += 25000;
+		numWriteReqs++;
+	}
+		break;
+
+	case 8: {
+
+		uint32_t best_PAD[16] = {0};
+		uint32_t trail_PAD[16] = {0};
+		int best_pad_hamming_distance[16] = {0};
+		int best_pad_index[16] = {0};
+		int best_data_bit_flips = {0};
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			trailPAD[cacheline] = leadPAD[cacheline] = gen512();
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+
+		for (int i=0; i<16; i++) {
+			trail_PAD[i] = (uint32_t)(trailPAD[cacheline] >> 32*i);
+
+			int counter = 0;
+			while(counter < 16) {
+				counter++;
+				uint32_t PAD[16];
+
+				int pad_hamming_distance = 0;
+				PAD[i] = gen32();
+
+				uint32_t x = (trail_PAD[i]) ^ (PAD[i]);
+				while (x > 0) {	pad_hamming_distance = pad_hamming_distance + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+				if (best_pad_hamming_distance[i] == 0) {
+					best_PAD[i] = PAD[i];
+					best_pad_index[i] = counter;
+					best_pad_hamming_distance[i] = pad_hamming_distance;
+				}
+				else {
+					best_PAD[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_PAD[i] : PAD[i];
+					best_pad_index[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_pad_index[i] : counter;
+					best_pad_hamming_distance[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_pad_hamming_distance[i] : pad_hamming_distance;
+				}
+			}
+		}
+
+		for (int i=0; i<16; i++) {
+			trailPAD[cacheline] = (trailPAD[cacheline] << 32) | best_PAD[16-i-1];
+		}
+
+		encrypted_data = trailPAD[cacheline] ^ updated_data;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		best_data_bit_flips = 0;
+		uint512_t x = stored_data ^ encrypted_data;
+		while (x > 0) {	best_data_bit_flips = best_data_bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+		//totalEncryptValSkpd += best_pad_index;
+		//totalEncryptValTested += counter;
+		bitFlips += best_data_bit_flips;
+		int write_slots = 1 + (best_data_bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		pkt->payloadDelay += 25000;
+		numWriteReqs++;
+	}
+		break;
+
+	case 9: {
+		//DEUCE-N
+
+		uint512_t PAD = gen512();
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+
+		if (!(cacheline_epoch[cacheline])) {
+			encrypted_data = PAD ^ (updated_data);
+			if(cacheline_stats[cacheline] != 1) numOfTimesTotalCacheLineEncrp += 1;		// stat after first write
+
+			// clear all the flags that were set for previous writes 		(prev_modified_words[cacheline])->clear(); pkt->clearModifiedWordsFlag();
+			for(int i=0; i<32; i++)	(*(modified_words_flags[cacheline]))[i] = 0;
+		}
+		else
+		{
+			vector<uint16_t> modifiedWordsPAD(32);
+			vector<uint16_t> modifiedWords(32);
+			for(int i=0; i<32; i++)
+			{
+				if((*(modified_words_flags[cacheline]))[i] == 1) {
+					uint16_t temp1 = (uint64_t)(updated_data >> (512 - (i+1)*16)); modifiedWords[i] = temp1;
+					uint16_t temp2 = (uint64_t)(PAD >> (512 - (i+1)*16)); modifiedWordsPAD[i] = temp2;
+				}
+				else {
+					uint16_t temp1 = (uint64_t)(stored_data >> (512 - (i+1)*16)); modifiedWords[i] = temp1;
+					uint16_t temp2 = 0x0000; modifiedWordsPAD[i] = temp2;
+				}
+			}
+
+			uint512_t temp_PAD = 0;
+			uint512_t temp_updated_data = 0;
+			for(int i=0; i<32; i++)
+			{
+				temp_PAD = (temp_PAD << 16) | modifiedWordsPAD[i];
+				temp_updated_data = (temp_updated_data << 16) | modifiedWords[i];
+			}
+			// PAD = PAD << (512-modified_words*16); //2 bytes(16bits per word)
+			encrypted_data = temp_PAD ^ (temp_updated_data);
+			numOfTimesPartialCacheLineEncrp += 1;
+		}
+		cacheline_epoch[cacheline] = (cacheline_epoch[cacheline]+1)%deuce_epoch; // epoch interval is 32
+		if(cacheline_stats[cacheline] > 4) {
+			if(cacheline_stats_4.find(cacheline) == cacheline_stats_4.end()) {
+				cacheline_stats_4[cacheline] = 0;
+				numCacheLinesWritten1_4 += 1;
+				numCacheLinesWritten_4 += 1;
+			}
+		}
+		if(cacheline_stats[cacheline] > 8) {
+			if(cacheline_stats_8.find(cacheline) == cacheline_stats_8.end()) {
+				cacheline_stats_8[cacheline] = 0;
+				numCacheLinesWritten1_8 += 1;
+				numCacheLinesWritten_8 += 1;
+			}
+		}
+		if(cacheline_stats[cacheline] > 16) {
+			if(cacheline_stats_16.find(cacheline) == cacheline_stats_16.end()) {
+				cacheline_stats_16[cacheline] = 0;
+				numCacheLinesWritten1_16 += 1;
+				numCacheLinesWritten_16 += 1;
+			}
+		}
+		if(cacheline_stats[cacheline] > 32) {
+			if(cacheline_stats_32.find(cacheline) == cacheline_stats_32.end()) {
+				cacheline_stats_32[cacheline] = 0;
+				numCacheLinesWritten1_32 += 1;
+				numCacheLinesWritten_32 += 1;
+			}
+		}
+		if(cacheline_stats[cacheline] > 64) {
+			if(cacheline_stats_64.find(cacheline) == cacheline_stats_64.end()) {
+				cacheline_stats_64[cacheline] = 0;
+				numCacheLinesWritten1_64 += 1;
+				numCacheLinesWritten_64 += 1;
+			}
+		}
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		uint512_t x = (stored_data) ^ (encrypted_data);
+		unsigned bit_flips = 0;
+		while (x > 0) {	bit_flips = bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; }
+
+		bitFlips += bit_flips;
+		int write_slots = 1 + (bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		numWriteReqs++;
+	}
+		break;
+
+	case 10: {
+		//DEUCE-N + TSECME
+
+		unsigned bit_flips = 0;
+		unsigned best_flips = 0;
+		unsigned max_flips = 0;
+		int best_flips_index = 0;
+		uint512_t best_flips_encrypted_data = 0;
+		int counter = 0;
+		int found = 0;
+		uint512_t PAD;
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+
+		uint32_t current_epoch = cacheline_epoch[cacheline];
+		while(current_epoch) {
+			counter++;
+			found = 1;
+			bit_flips = 0;
+			PAD = gen512();
+			vector<uint16_t> modifiedWordsPAD(32);
+			vector<uint16_t> modifiedWords(32);
+
+			for(int i=0; i<32; i++)
+			{
+				if((*(modified_words_flags[cacheline]))[i] == 1) {
+					uint16_t temp1 = (uint64_t)(updated_data >> (512 - (i+1)*16)); modifiedWords[i] = temp1;
+					uint16_t temp2 = (uint64_t)(PAD >> (512 - (i+1)*16)); modifiedWordsPAD[i] = temp2;
+				}
+				else {
+					uint16_t temp1 = (uint64_t)(stored_data >> (512 - (i+1)*16)); modifiedWords[i] = temp1;
+					uint16_t temp2 = 0x0000; modifiedWordsPAD[i] = temp2;
+				}
+			}
+
+			uint512_t temp_PAD = 0;
+			uint512_t temp_updated_data = 0;
+			for(int i=0; i<32; i++)
+			{
+				temp_PAD = (temp_PAD << 16) | modifiedWordsPAD[i];
+				temp_updated_data = (temp_updated_data << 16) | modifiedWords[i];
+			}
+			// PAD = PAD << (512-modified_words*16); //2 bytes(16bits per word)
+			encrypted_data = temp_PAD ^ (temp_updated_data);
+
+			uint512_t x = (stored_data) ^ (encrypted_data);
+			while (x > 0) {	bit_flips = bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+			if (best_flips == 0) {
+				best_flips =  bit_flips;
+				best_flips_encrypted_data = encrypted_data;
+				best_flips_index = counter;
+				max_flips = bit_flips;
+			}
+			else {
+				best_flips = (best_flips < bit_flips) ? best_flips : bit_flips;
+				best_flips_encrypted_data = (best_flips < bit_flips) ? best_flips_encrypted_data : encrypted_data;
+				best_flips_index = (best_flips < bit_flips) ? best_flips_index : counter;
+				max_flips = (max_flips >= bit_flips) ? max_flips : bit_flips;
+			}
+
+			if (counter >= 16) break;
+
+			current_epoch = (current_epoch+1)%deuce_epoch; // epoch interval is 32
+
+		}
+
+		if (!found) {
+			cacheline_epoch[cacheline] = (cacheline_epoch[cacheline]+1)%deuce_epoch; // epoch interval is 32
+			counter++;
+			PAD = gen512();
+			best_flips_encrypted_data = PAD ^ (updated_data);
+			best_flips_index = 1;
+
+			int bit_flips = 0;
+			uint512_t x = (stored_data) ^ (best_flips_encrypted_data);
+			while (x > 0) { bit_flips = bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+			max_flips = bit_flips;
+			best_flips = bit_flips;
+
+			if(cacheline_stats[cacheline] != 1) numOfTimesTotalCacheLineEncrp += 1;
+
+			for(int i=0; i<32; i++)	(*(modified_words_flags[cacheline]))[i] = 0;
+		}
+		else {
+			cacheline_epoch[cacheline] = (cacheline_epoch[cacheline] + best_flips_index)%deuce_epoch;
+			numOfTimesPartialCacheLineEncrp += 1;
+		}
+
+		encrypted_data = best_flips_encrypted_data;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		totalEncryptValSkpd += best_flips_index;
+		totalEncryptValTested += counter;
+		bitFlips += best_flips;
+		int write_slots = 1 + (best_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		pkt->payloadDelay += 25000;
+		numWriteReqs++;
+	}
+		break;
+
+	case 11: {
+		//DEUCE-N + TSECME
+
+		uint512_t best_flips_encrypted_data = 0;
+		int counter = 0;
+		int found = 0;
+		uint512_t bestPAD = 0;
+		uint512_t best_PAD =0;
+		uint512_t PAD;
+		int pad_hamming_distance = 0;
+		int best_pad_hamming_distance = 0;
+		int best_pad_index = 0;
+		int best_data_bit_flips = 0;
+#ifdef DEBUG
+		int max_pad_hamming_distance = 0;
+		int max_data_bit_flips = 0;
+#endif
+		vector<uint16_t> lead_PAD(32);
+		vector<uint16_t> trail_PAD(32);
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			trailPAD[cacheline] = leadPAD[cacheline] = gen512();
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+
+		uint32_t current_epoch = cacheline_epoch[cacheline];
+
+		for(int i=0; i<32; i++) { trail_PAD[i] = (uint16_t)(trailPAD[cacheline] >> (512 - (i+1)*16)); }
+		for(int i=0; i<32; i++) { lead_PAD[i] = (uint16_t)(leadPAD[cacheline] >> (512 - (i+1)*16)); }
+
+		//cout << "previous PAD: " << std::hex << prevPAD[cacheline] << endl;
+		while(current_epoch) {
+			counter++;
+			found = 1;
+			pad_hamming_distance = 0;
+			PAD = gen512();
+			uint512_t compare_PAD = 0;
+			vector<uint16_t> modifiedWordsPAD(32);
+
+			for(int i=0; i<32; i++)
+			{
+				if((*(modified_words_flags[cacheline]))[i] == 1) {
+					modifiedWordsPAD[i] = (uint64_t)(PAD >> (512 - (i+1)*16));
+					compare_PAD = (compare_PAD << 16) | lead_PAD[i];
+				}
+				else {
+					modifiedWordsPAD[i] = trail_PAD[i];
+					compare_PAD = (compare_PAD << 16) | trail_PAD[i];
+				}
+			}
+
+			uint512_t test_PAD = 0;
+			for(int i=0; i<32; i++) test_PAD = (test_PAD << 16) | modifiedWordsPAD[i];
+
+			uint512_t x = (test_PAD) ^ (compare_PAD);
+			while (x > 0) {	pad_hamming_distance = pad_hamming_distance + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+#ifdef DEBUG
+			int stat_data_bit_flips = 0;
+			uint512_t stat_encrypted_data = (test_PAD) ^ updated_data;
+			uint512_t bf = (stored_data) ^ (stat_encrypted_data);
+			while (bf > 0) { stat_data_bit_flips = stat_data_bit_flips + ((bf & 1) == 1 ? 1 : 0); bf >>= 1; };
+#endif
+
+			if (best_pad_hamming_distance == 0) {
+				best_pad_hamming_distance =  pad_hamming_distance;
+				bestPAD = PAD;
+				best_PAD = test_PAD;
+				best_pad_index = counter;
+#ifdef DEBUG
+				max_pad_hamming_distance = pad_hamming_distance;
+				best_data_bit_flips = stat_data_bit_flips;
+				max_data_bit_flips = stat_data_bit_flips;
+#endif
+			}
+			else {
+				bestPAD = (best_pad_hamming_distance < pad_hamming_distance) ? bestPAD : PAD;
+				best_PAD = (best_pad_hamming_distance < pad_hamming_distance) ? best_PAD : test_PAD;
+				best_pad_index = (best_pad_hamming_distance < pad_hamming_distance) ? best_pad_index : counter;
+
+#ifdef DEBUG
+				max_data_bit_flips = (max_pad_hamming_distance > pad_hamming_distance) ? max_data_bit_flips : ((max_data_bit_flips > stat_data_bit_flips) ? max_data_bit_flips : stat_data_bit_flips);
+				//best_data_bit_flips = (best_pad_hamming_distance < pad_hamming_distance) ? best_data_bit_flips : stat_data_bit_flips;
+				max_pad_hamming_distance = (max_pad_hamming_distance > pad_hamming_distance) ? max_pad_hamming_distance : pad_hamming_distance;
+#endif
+
+				best_pad_hamming_distance = (best_pad_hamming_distance < pad_hamming_distance) ? best_pad_hamming_distance : pad_hamming_distance;
+			}
+
+			if (counter >= 16) break;
+
+			current_epoch = (current_epoch+1)%deuce_epoch; // epoch interval is 32
+
+		}
+
+		if (!found) {
+			cacheline_epoch[cacheline] = (cacheline_epoch[cacheline]+1)%deuce_epoch; // epoch interval is 32
+			counter++;
+			pad_hamming_distance = 0;
+			bestPAD = best_PAD = gen512();
+			best_flips_encrypted_data = best_PAD ^ (updated_data);
+			best_pad_index = 1;
+
+			//uint512_t x = (stored_data) ^ (encrypted_data);
+			uint512_t x = bestPAD ^ trailPAD[cacheline];
+			while (x > 0) {	pad_hamming_distance = pad_hamming_distance + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+#ifdef DEBUG
+			int stat_data_bit_flips = 0;
+			uint512_t bf = (stored_data) ^ (best_flips_encrypted_data);
+			while (bf > 0) { stat_data_bit_flips = stat_data_bit_flips + ((bf & 1) == 1 ? 1 : 0); bf >>= 1; };
+			max_pad_hamming_distance = pad_hamming_distance;
+			max_data_bit_flips = stat_data_bit_flips;
+#endif
+
+			trailPAD[cacheline] = leadPAD[cacheline] = bestPAD;
+			if(cacheline_stats[cacheline] != 1)
+				numOfTimesTotalCacheLineEncrp += 1;
+
+			for(int i=0; i<32; i++) (*(modified_words_flags[cacheline]))[i] = 0;
+		}
+		else {
+			best_flips_encrypted_data = best_PAD ^ (updated_data);
+			cacheline_epoch[cacheline] = (cacheline_epoch[cacheline] + best_pad_index)%deuce_epoch;
+			leadPAD[cacheline] = bestPAD;
+			numOfTimesPartialCacheLineEncrp += 1;
+		}
+
+		encrypted_data = best_flips_encrypted_data;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		best_data_bit_flips = 0;
+		uint512_t x = stored_data ^ encrypted_data;
+		while (x > 0) {	best_data_bit_flips = best_data_bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+		totalEncryptValSkpd += best_pad_index;
+		totalEncryptValTested += counter;
+		bitFlips += best_data_bit_flips;
+		int write_slots = 1 + (best_data_bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		pkt->payloadDelay += 25000;
+		numWriteReqs++;
+	}
+		break;
+
+	case 12: {
+
+		uint512_t best_flips_encrypted_data = 0;
+		int counter = 0;
+		int found = 0;
+		uint512_t bestPAD = 0;
+		uint512_t best_PAD =0;
+		uint512_t PAD;
+		int pad_hamming_distance = 0;
+		int best_pad_hamming_distance = 0;
+		int best_pad_index = 0;
+		int best_data_bit_flips = 0;
+#ifdef DEBUG
+		int max_pad_hamming_distance = 0;
+		int max_data_bit_flips = 0;
+#endif
+		vector<uint16_t> lead_PAD(32);
+		vector<uint16_t> trail_PAD(32);
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			trailPAD[cacheline] = leadPAD[cacheline] = gen512();
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+
+		uint32_t current_epoch = cacheline_epoch[cacheline];
+
+		for(int i=0; i<32; i++) { trail_PAD[i] = (uint16_t)(trailPAD[cacheline] >> (512 - (i+1)*16)); }
+		for(int i=0; i<32; i++) { lead_PAD[i] = (uint16_t)(leadPAD[cacheline] >> (512 - (i+1)*16)); }
+
+		//cout << "previous PAD: " << std::hex << prevPAD[cacheline] << endl;
+		while(current_epoch) {
+			counter++;
+			found = 1;
+			pad_hamming_distance = 0;
+			PAD = gen512();
+			uint512_t compare_PAD = 0;
+			vector<uint16_t> modifiedWordsPAD(32);
+
+			for(int i=0; i<32; i++)
+			{
+				if((*(modified_words_flags[cacheline]))[i] == 1) {
+					modifiedWordsPAD[i] = (uint64_t)(PAD >> (512 - (i+1)*16));
+					compare_PAD = (compare_PAD << 16) | lead_PAD[i];
+				}
+				else {
+					modifiedWordsPAD[i] = trail_PAD[i];
+					compare_PAD = (compare_PAD << 16) | trail_PAD[i];
+				}
+			}
+
+			uint512_t test_PAD = 0;
+			for(int i=0; i<32; i++) test_PAD = (test_PAD << 16) | modifiedWordsPAD[i];
+
+			uint512_t x = (test_PAD) ^ (compare_PAD);
+			while (x > 0) {	pad_hamming_distance = pad_hamming_distance + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+#ifdef DEBUG
+			int stat_data_bit_flips = 0;
+			uint512_t stat_encrypted_data = (test_PAD) ^ updated_data;
+			uint512_t bf = (stored_data) ^ (stat_encrypted_data);
+			while (bf > 0) { stat_data_bit_flips = stat_data_bit_flips + ((bf & 1) == 1 ? 1 : 0); bf >>= 1; };
+#endif
+
+			if (best_pad_hamming_distance == 0) {
+				best_pad_hamming_distance =  pad_hamming_distance;
+				bestPAD = PAD;
+				best_PAD = test_PAD;
+				best_pad_index = counter;
+#ifdef DEBUG
+				max_pad_hamming_distance = pad_hamming_distance;
+				best_data_bit_flips = stat_data_bit_flips;
+				max_data_bit_flips = stat_data_bit_flips;
+#endif
+			}
+			else {
+				bestPAD = (best_pad_hamming_distance < pad_hamming_distance) ? bestPAD : PAD;
+				best_PAD = (best_pad_hamming_distance < pad_hamming_distance) ? best_PAD : test_PAD;
+				best_pad_index = (best_pad_hamming_distance < pad_hamming_distance) ? best_pad_index : counter;
+
+#ifdef DEBUG
+				max_data_bit_flips = (max_pad_hamming_distance > pad_hamming_distance) ? max_data_bit_flips : ((max_data_bit_flips > stat_data_bit_flips) ? max_data_bit_flips : stat_data_bit_flips);
+				//best_data_bit_flips = (best_pad_hamming_distance < pad_hamming_distance) ? best_data_bit_flips : stat_data_bit_flips;
+				max_pad_hamming_distance = (max_pad_hamming_distance > pad_hamming_distance) ? max_pad_hamming_distance : pad_hamming_distance;
+#endif
+
+				best_pad_hamming_distance = (best_pad_hamming_distance < pad_hamming_distance) ? best_pad_hamming_distance : pad_hamming_distance;
+			}
+
+			if (counter >= 16) break;
+
+			current_epoch = (current_epoch+1)%deuce_epoch; // epoch interval is 32
+
+		}
+
+		if (!found) {
+			cacheline_epoch[cacheline] = (cacheline_epoch[cacheline]+1)%deuce_epoch; // epoch interval is 32
+			counter++;
+			pad_hamming_distance = 0;
+			bestPAD = best_PAD = gen512();
+			best_flips_encrypted_data = best_PAD ^ (updated_data);
+			best_pad_index = 1;
+
+			//uint512_t x = (stored_data) ^ (encrypted_data);
+			uint512_t x = bestPAD ^ trailPAD[cacheline];
+			while (x > 0) {	pad_hamming_distance = pad_hamming_distance + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+#ifdef DEBUG
+			int stat_data_bit_flips = 0;
+			uint512_t bf = (stored_data) ^ (best_flips_encrypted_data);
+			while (bf > 0) { stat_data_bit_flips = stat_data_bit_flips + ((bf & 1) == 1 ? 1 : 0); bf >>= 1; };
+			max_pad_hamming_distance = pad_hamming_distance;
+			max_data_bit_flips = stat_data_bit_flips;
+#endif
+
+			trailPAD[cacheline] = leadPAD[cacheline] = bestPAD;
+			if(cacheline_stats[cacheline] != 1)
+				numOfTimesTotalCacheLineEncrp += 1;
+
+			for(int i=0; i<32; i++) (*(modified_words_flags[cacheline]))[i] = 0;
+		}
+		else {
+			best_flips_encrypted_data = best_PAD ^ (updated_data);
+			cacheline_epoch[cacheline] = (cacheline_epoch[cacheline] + best_pad_index)%deuce_epoch;
+			leadPAD[cacheline] = bestPAD;
+			numOfTimesPartialCacheLineEncrp += 1;
+		}
+
+		encrypted_data = best_flips_encrypted_data;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		best_data_bit_flips = 0;
+		uint512_t x = stored_data ^ encrypted_data;
+		while (x > 0) {	best_data_bit_flips = best_data_bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+		totalEncryptValSkpd += best_pad_index;
+		totalEncryptValTested += counter;
+		bitFlips += best_data_bit_flips;
+		int write_slots = 1 + (best_data_bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		pkt->payloadDelay += 25000;
+		numWriteReqs++;
+	}
+		break;
+
+	case 13: {
+		//DEUCE1-N
+
+		uint512_t PAD = gen512();
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+
+		if (!(cacheline_epoch[cacheline])) {
+			encrypted_data = PAD ^ (updated_data);
+			if(cacheline_stats[cacheline] != 1) numOfTimesTotalCacheLineEncrp += 1;		// stat after first write
+
+			// clear all the flags that were set for previous writes 		(prev_modified_words[cacheline])->clear(); pkt->clearModifiedWordsFlag();
+			for(int i=0; i<32; i++)	(*(modified_words_flags[cacheline]))[i] = 0;
+		}
+		else
+		{
+			vector<uint16_t> modifiedWordsPAD(32);
+			vector<uint16_t> modifiedWords(32);
+			for(int i=0; i<32; i++)
+			{
+				if((*(modified_words_flags[cacheline]))[i] == 1) {
+					uint16_t temp1 = (uint64_t)(updated_data >> (512 - (i+1)*16)); modifiedWords[i] = temp1;
+					uint16_t temp2 = (uint64_t)(PAD >> (512 - (i+1)*16)); modifiedWordsPAD[i] = temp2;
+				}
+				else {
+					uint16_t temp1 = (uint64_t)(stored_data >> (512 - (i+1)*16)); modifiedWords[i] = temp1;
+					uint16_t temp2 = 0x0000; modifiedWordsPAD[i] = temp2;
+				}
+			}
+
+			uint512_t temp_PAD = 0;
+			uint512_t temp_updated_data = 0;
+			for(int i=0; i<32; i++)
+			{
+				temp_PAD = (temp_PAD << 16) | modifiedWordsPAD[i];
+				temp_updated_data = (temp_updated_data << 16) | modifiedWords[i];
+			}
+			// PAD = PAD << (512-modified_words*16); //2 bytes(16bits per word)
+			encrypted_data = temp_PAD ^ (temp_updated_data);
+			numOfTimesPartialCacheLineEncrp += 1;
+		}
+		cacheline_epoch[cacheline] = (cacheline_epoch[cacheline]+1)%deuce_epoch;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		bool storedInCounterCache = false;
+		if(wordsModifiedThisWrite == 1) {
+			if(!cc->hasData(cacheline)) {
+				cc->setData(cacheline, wordsModifiedThisWriteIndex);
+				storedInCounterCache = true;
+			}
+			else {
+				int word = cc->getData(cacheline);
+				if(word == wordsModifiedThisWriteIndex) {
+					storedInCounterCache = true;
+				}
+				else {
+					cc->setData(cacheline, wordsModifiedThisWriteIndex);
+				}
+			}
+		}
+
+		if(storedInCounterCache) {
+			int write_slots = 0;
+			totalWriteSlots += write_slots;
+			delay = tWR*write_slots;
+			numWriteReqs++;
+			writesSavedAtCC += 1;
+			return false;
+		}
+		else {
+			if(cacheline_stats[cacheline] == 1) {
+				pkt->dlyDueToBitFips = tWR*2;
+				return tWR*2;
+			}
+
+			uint512_t x = (stored_data) ^ (encrypted_data);
+			unsigned bit_flips = 0;
+			while (x > 0) {	bit_flips = bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; } bitFlips += bit_flips;
+			int write_slots = 1 + (bit_flips/64);
+			totalWriteSlots += write_slots;
+			delay = tWR*write_slots;
+			numWriteReqs++;
+		}
+	}
+		break;
+
+	case 14: {
+
+		uint256_t best_PAD[2] = {0};
+		uint256_t trail_PAD[2] = {0};
+		int best_pad_hamming_distance[2] = {0};
+		int best_pad_index[2] = {0};
+		int best_data_bit_flips = {0};
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			trailPAD[cacheline] = leadPAD[cacheline] = gen512();
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+
+		for (int i=0; i<2; i++) {
+			trail_PAD[i] = (uint256_t)(trailPAD[cacheline] >> 256*i);
+
+			int counter = 0;
+			while(counter < 16) {
+				counter++;
+				uint256_t PAD[2];
+
+				int pad_hamming_distance = 0;
+				PAD[i] = gen256();
+
+				uint256_t x = (trail_PAD[i]) ^ (PAD[i]);
+				while (x > 0) {	pad_hamming_distance = pad_hamming_distance + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+				if (best_pad_hamming_distance[i] == 0) {
+					best_PAD[i] = PAD[i];
+					best_pad_index[i] = counter;
+					best_pad_hamming_distance[i] = pad_hamming_distance;
+				}
+				else {
+					best_PAD[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_PAD[i] : PAD[i];
+					best_pad_index[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_pad_index[i] : counter;
+					best_pad_hamming_distance[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_pad_hamming_distance[i] : pad_hamming_distance;
+				}
+			}
+		}
+
+		for (int i=0; i<2; i++) {
+			trailPAD[cacheline] = (trailPAD[cacheline] << 256) | best_PAD[2-i-1];
+		}
+
+		encrypted_data = trailPAD[cacheline] ^ updated_data;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		best_data_bit_flips = 0;
+		uint512_t x = stored_data ^ encrypted_data;
+		while (x > 0) {	best_data_bit_flips = best_data_bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+		//totalEncryptValSkpd += best_pad_index;
+		//totalEncryptValTested += counter;
+		bitFlips += best_data_bit_flips;
+		int write_slots = 1 + (best_data_bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		pkt->payloadDelay += 25000;
+		numWriteReqs++;
+	}
+		break;
+
+	case 15: {
+
+		uint16_t best_PAD[32] = {0};
+		uint16_t trail_PAD[32] = {0};
+		int best_pad_hamming_distance[32] = {0};
+		int best_pad_index[32] = {0};
+		int best_data_bit_flips = {0};
+
+		uint64_t cacheline = pkt->getBlockAddr(64);
+		if(cacheline_epoch.find(cacheline) == cacheline_epoch.end()) {
+			cacheline_epoch[cacheline] = 0;
+			cacheline_stats[cacheline] = 0;
+			trailPAD[cacheline] = leadPAD[cacheline] = gen512();
+			numCacheLinesWritten += 1;
+		}
+		cacheline_stats[cacheline] += 1;
+
+		for (int i=0; i<32; i++) {
+			trail_PAD[i] = (uint16_t)(trailPAD[cacheline] >> 16*i);
+
+			int counter = 0;
+			while(counter < 16) {
+				counter++;
+				uint16_t PAD[32];
+
+				int pad_hamming_distance = 0;
+				PAD[i] = gen16();
+
+				uint16_t x = (trail_PAD[i]) ^ (PAD[i]);
+				while (x > 0) {	pad_hamming_distance = pad_hamming_distance + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+				if (best_pad_hamming_distance[i] == 0) {
+					best_PAD[i] = PAD[i];
+					best_pad_index[i] = counter;
+					best_pad_hamming_distance[i] = pad_hamming_distance;
+				}
+				else {
+					best_PAD[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_PAD[i] : PAD[i];
+					best_pad_index[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_pad_index[i] : counter;
+					best_pad_hamming_distance[i] = (best_pad_hamming_distance[i] < pad_hamming_distance) ? best_pad_hamming_distance[i] : pad_hamming_distance;
+				}
+			}
+		}
+
+		for (int i=0; i<32; i++) {
+			trailPAD[cacheline] = (trailPAD[cacheline] << 16) | best_PAD[32-i-1];
+		}
+
+		encrypted_data = trailPAD[cacheline] ^ updated_data;
+		storeEncryptedData(pkt,(uint8_t *)&encrypted_data);
+
+		if(cacheline_stats[cacheline] == 1) {
+			pkt->dlyDueToBitFips = tWR*2;
+			return true;
+		}
+
+		best_data_bit_flips = 0;
+		uint512_t x = stored_data ^ encrypted_data;
+		while (x > 0) {	best_data_bit_flips = best_data_bit_flips + ((x & 1) == 1 ? 1 : 0); x >>= 1; };
+
+		//totalEncryptValSkpd += best_pad_index;
+		//totalEncryptValTested += counter;
+		bitFlips += best_data_bit_flips;
+		int write_slots = 1 + (best_data_bit_flips/64);
+		totalWriteSlots += write_slots;
+		delay = tWR*write_slots;
+		pkt->payloadDelay += 25000;
+		numWriteReqs++;
+	}
+		break;
+
+	default:
+		cout << "ERRORRRRR!!!" << endl;
+		break;
+	}
+
+	pkt->dlyDueToBitFips = delay;
+
+	return true;
+}
+
 void
 DRAMCtrl::doDRAMAccess(DRAMPacket* dram_pkt)
 {
-    DPRINTF(DRAM, "Timing access to addr %lld, rank/bank/row %d %d %d\n",
-            dram_pkt->addr, dram_pkt->rank, dram_pkt->bank, dram_pkt->row);
+    //DPRINTF(DRAM, "Timing access to addr %lld, rank/bank/row %d %d %d\n",
+    //        dram_pkt->addr, dram_pkt->rank, dram_pkt->bank, dram_pkt->row);
 
     // get the rank
     Rank& rank = dram_pkt->rankRef;
@@ -1119,18 +2821,23 @@ DRAMCtrl::doDRAMAccess(DRAMPacket* dram_pkt)
     // get the bank
     Bank& bank = dram_pkt->bankRef;
 
+    DPRINTF(DRAM, "doDRAM Access: rank/bank/row %d %d %d preAllowedAt: %d, curTick: %d, diff: \n",
+        	                dram_pkt->rank, dram_pkt->bank, dram_pkt->row, bank.preAllowedAt, curTick(), bank.preAllowedAt-curTick());
+
     // for the state we need to track if it is a row hit or not
     bool row_hit = true;
 
     // Determine the access latency and update the bank state
-    if (bank.openRow == dram_pkt->row) {
+    //if (bank.openRow == dram_pkt->row) {
         // nothing to do
-    } else {
-        row_hit = false;
+    //} else {
+    //    row_hit = false;
 
         // If there is a page open, precharge it.
         if (bank.openRow != Bank::NO_ROW) {
             prechargeBank(rank, bank, std::max(bank.preAllowedAt, curTick()));
+            DPRINTF(DRAM, "doDRAM Access: rank/bank/row %d %d %d prechargeBank at: %d, curTick: %d, diff: \n",
+					dram_pkt->rank, dram_pkt->bank, dram_pkt->row, bank.preAllowedAt, curTick(), (std::max(bank.preAllowedAt, curTick())) - curTick());
         }
 
         // next we need to account for the delay in activating the
@@ -1140,7 +2847,7 @@ DRAMCtrl::doDRAMAccess(DRAMPacket* dram_pkt)
         // Record the activation and deal with all the global timing
         // constraints caused be a new activation (tRRD and tXAW)
         activateBank(rank, bank, act_tick, dram_pkt->row);
-    }
+    //}
 
     // respect any constraints on the command (e.g. tRCD or tCCD)
     const Tick col_allowed_at = dram_pkt->isRead() ?
@@ -1153,6 +2860,11 @@ DRAMCtrl::doDRAMAccess(DRAMPacket* dram_pkt)
     // update the packet ready time
     dram_pkt->readyTime = cmd_at + tCL + tBURST;
 
+    DPRINTF(DRAM, "doDRAM Access: rank/bank/row %d %d %d preAllowedAt: %d, actAllowedAt: %d, col_allowed_at: %d, cmd_at: %d, readyTime: %d , readQueue: %d, writeQueue: %d\n",
+        	                dram_pkt->rank, dram_pkt->bank, dram_pkt->row, bank.preAllowedAt-curTick(),
+        	                bank.actAllowedAt-curTick(), col_allowed_at-curTick(), cmd_at-curTick(), dram_pkt->readyTime-curTick(),
+        	                totalReadQueueSize, totalWriteQueueSize);
+
     // update the time for the next read/write burst for each
     // bank (add a max with tCCD/tCCD_L/tCCD_L_WR here)
     Tick dly_to_rd_cmd;
@@ -1197,12 +2909,50 @@ DRAMCtrl::doDRAMAccess(DRAMPacket* dram_pkt)
     // Save rank of current access
     activeRank = dram_pkt->rank;
 
-    // If this is a write, we also need to respect the write recovery
-    // time before a precharge, in the case of a read, respect the
-    // read to precharge constraint
-    bank.preAllowedAt = std::max(bank.preAllowedAt,
+    if(bitFlipsDelay) {
+    	// If this is a write, we also need to respect the write recovery
+    	// time before a precharge, in the case of a read, respect the
+    	// read to precharge constraint
+    	bank.preAllowedAt = std::max(bank.preAllowedAt,
                                  dram_pkt->isRead() ? cmd_at + tRTP :
-                                 dram_pkt->readyTime + tWR);
+                                 dram_pkt->readyTime + dram_pkt->dly_due_to_bit_flips); //tWR);
+    }
+    else if(reram) {
+    	//std::vector<uint8_t> actual_data_bytes(pkt->getSize());
+    	//std::vector<uint8_t> stored_data_bytes(pkt->getSize());
+    	//std::vector<uint8_t> updated_data_bytes(pkt->getSize());
+    	//uint512_t actual_data=0, updated_data=0;
+
+    	//getMemoryData(pkt, &actual_data_bytes[0], &stored_data_bytes[0]);
+    	//pkt->writeData(&updated_data_bytes[0]);
+
+    	//if(pkt->hasData())
+    	//{
+    	//	for (int i=0; i<pkt->getSize(); i++) {
+    	//		actual_data = actual_data<<8 | actual_data_bytes[i];
+    	//		updated_data = updated_data<<8 | updated_data_bytes[i];
+    	//	}
+    	//}
+    	//uint512_t compData = (actual_data) ^ (updated_data);
+    	//if(compData == 0) {writesWithOutWordChanges += 1;}//pkt->dlyDueToBitFips = 0;return true;}
+
+    	//DPRINTF(DRAM, "doDRAM Access: rank/bank/row %d %d %d preAllowedAt: %d, actAllowedAt: %d, col_allowed_at: %d, cmd_at: %d, readyTime: %d\n",
+        //        dram_pkt->rank, dram_pkt->bank, dram_pkt->row, bank.preAllowedAt-curTick(),
+        //        bank.actAllowedAt-curTick(), col_allowed_at-curTick(), cmd_at-curTick(), dram_pkt->readyTime-curTick());
+
+    	int user_data = 224;
+    	int subArrayRow_delay = 0.4*((dram_pkt->row)%512);
+    	int maxSubArrayRow_delay = 0; //0.4*(512)*1000;
+    	int subArrayCol_delay = 0.4*user_data*1000;
+    	bank.preAllowedAt = std::max(bank.preAllowedAt,
+                                 dram_pkt->isRead() ? cmd_at + tRTP :
+                                 dram_pkt->readyTime + tWR + maxSubArrayRow_delay+ subArrayCol_delay);
+
+    	if(!dram_pkt->isRead())
+    		DPRINTF(DRAM, "ReRAM access to addr %lld, rank/bank/row %d %d %d, delay: %d, max delay: %d, subArrayVol_delay: %d, tWR: %d\n",
+                dram_pkt->addr, dram_pkt->rank, dram_pkt->bank, dram_pkt->row, subArrayRow_delay, maxSubArrayRow_delay, subArrayCol_delay, tWR);
+
+    }
 
     // increment the bytes accessed and the accesses per row
     bank.bytesAccessed += burstSize;
@@ -1228,33 +2978,79 @@ DRAMCtrl::doDRAMAccess(DRAMPacket* dram_pkt)
         bool got_more_hits = false;
         bool got_bank_conflict = false;
 
-        // either look at the read queue or write queue
-        const std::vector<DRAMPacketQueue>& queue =
-                dram_pkt->isRead() ? readQueue : writeQueue;
-
-        for (uint8_t i = 0; i < numPriorities(); ++i) {
-            auto p = queue[i].begin();
-            // keep on looking until we find a hit or reach the end of the queue
-            // 1) if a hit is found, then both open and close adaptive policies keep
-            // the page open
-            // 2) if no hit is found, got_bank_conflict is set to true if a bank
-            // conflict request is waiting in the queue
-            // 3) make sure we are not considering the packet that we are
-            // currently dealing with
-            while (!got_more_hits && p != queue[i].end()) {
-                if (dram_pkt != (*p)) {
-                    bool same_rank_bank = (dram_pkt->rank == (*p)->rank) &&
-                                          (dram_pkt->bank == (*p)->bank);
-
-                    bool same_row = dram_pkt->row == (*p)->row;
-                    got_more_hits |= same_rank_bank && same_row;
-                    got_bank_conflict |= same_rank_bank && !same_row;
-                }
-                ++p;
-            }
-
-            if (got_more_hits)
-                break;
+        if(nvm) {
+        	// either look at the read queue or write queue
+			if(dram_pkt->isRead()) {
+				const std::vector<DRAMPacketQueue>& queue = readQueue;
+
+				for (uint8_t i = 0; i < numPriorities(); ++i) {
+					auto p = queue[i].begin();
+					// keep on looking until we find a hit or reach the end of the queue
+					// 1) if a hit is found, then both open and close adaptive policies keep
+					// the page open
+					// 2) if no hit is found, got_bank_conflict is set to true if a bank
+					// conflict request is waiting in the queue
+					// 3) make sure we are not considering the packet that we are
+					// currently dealing with
+					while (!got_more_hits && p != queue[i].end()) {
+						if (dram_pkt != (*p)) {
+							bool same_rank_bank = (dram_pkt->rank == (*p)->rank) &&
+												  (dram_pkt->bank == (*p)->bank);
+
+							bool same_row = dram_pkt->row == (*p)->row;
+							got_more_hits |= same_rank_bank && same_row;
+							got_bank_conflict |= same_rank_bank && !same_row;
+						}
+						++p;
+					}
+
+					if (got_more_hits)
+						break;
+				}
+			}
+			else {
+				auto p = writeBuffer.begin();
+				while (!got_more_hits && p != writeBuffer.end()) {
+					if (dram_pkt != (*p)) {
+						bool same_rank_bank = (dram_pkt->rank == (*p)->rank) &&
+											  (dram_pkt->bank == (*p)->bank);
+
+						bool same_row = dram_pkt->row == (*p)->row;
+						got_more_hits |= same_rank_bank && same_row;
+						got_bank_conflict |= same_rank_bank && !same_row;
+					}
+					++p;
+				}
+			}
+        } else {
+			// either look at the read queue or write queue
+			const std::vector<DRAMPacketQueue>& queue =
+					dram_pkt->isRead() ? readQueue : writeQueue;
+
+			for (uint8_t i = 0; i < numPriorities(); ++i) {
+				auto p = queue[i].begin();
+				// keep on looking until we find a hit or reach the end of the queue
+				// 1) if a hit is found, then both open and close adaptive policies keep
+				// the page open
+				// 2) if no hit is found, got_bank_conflict is set to true if a bank
+				// conflict request is waiting in the queue
+				// 3) make sure we are not considering the packet that we are
+				// currently dealing with
+				while (!got_more_hits && p != queue[i].end()) {
+					if (dram_pkt != (*p)) {
+						bool same_rank_bank = (dram_pkt->rank == (*p)->rank) &&
+											  (dram_pkt->bank == (*p)->bank);
+
+						bool same_row = dram_pkt->row == (*p)->row;
+						got_more_hits |= same_rank_bank && same_row;
+						got_bank_conflict |= same_rank_bank && !same_row;
+					}
+					++p;
+				}
+
+				if (got_more_hits)
+					break;
+			}
         }
 
         // auto pre-charge when either
@@ -1275,8 +3071,8 @@ DRAMCtrl::doDRAMAccess(DRAMPacket* dram_pkt)
     // Update bus state to reflect when previous command was issued
     nextBurstAt = cmd_at + tBURST;
 
-    DPRINTF(DRAM, "Access to %lld, ready at %lld next burst at %lld.\n",
-            dram_pkt->addr, dram_pkt->readyTime, nextBurstAt);
+    //DPRINTF(DRAM, "Access to %lld, ready at %lld next burst at %lld.\n",
+    //        dram_pkt->addr, dram_pkt->readyTime, nextBurstAt);
 
     dram_pkt->rankRef.cmdList.push_back(Command(command, dram_pkt->bank,
                                         cmd_at));
@@ -1329,8 +3125,469 @@ DRAMCtrl::doDRAMAccess(DRAMPacket* dram_pkt)
 }
 
 void
+DRAMCtrl::insertEntryNVMWriteBuffer(DRAMPacket* dram_pkt)
+{
+	//cout << "WB: inserting address: " << dram_pkt->addr << endl;
+	writeBuffer.push_back(dram_pkt);
+	isInWriteBuffer.insert(burstAlign(dram_pkt->addr));
+
+	nvm_totalWriteBufferSize++;
+	if(nvm_totalWriteBufferSize >= nvm_wb_max_threshold)
+		nvm_wb_flushing = true;
+
+	return;
+}
+
+void
+DRAMCtrl::eraseEntryNVMWriteBuffer(DRAMPacket* dram_pkt)
+{
+	//cout << "WB: erasing address: " << dram_pkt->addr << endl;
+	isInWriteBuffer.erase(burstAlign(dram_pkt->addr));
+	//writeBuffer.erase(dram_pkt);
+
+	nvm_totalWriteBufferSize--;
+	if(nvm_totalWriteBufferSize < nvm_wb_min_threshold)
+		nvm_wb_flushing = false;
+
+	return;
+}
+
+void
+DRAMCtrl::try_flush_wb()
+{
+	//int MAX_WRITES = params->max_writes;
+	bool found = false;
+	DRAMPacketQueue::iterator to_write;
+
+	//change the bus state
+    //bool switched_cmd_type = busStateNext == READ ? TRUE : FALSE;
+
+	if(nvm_wb_flushing || (totalReadQueueSize == 0 && totalWriteQueueSize == 0 && nvm_totalWriteBufferSize > 0))
+	{
+		// Figure out which write request goes next
+		// If we are changing command type, incorporate the minimum
+		// bus turnaround delay which will be tCS (different rank) case
+		to_write = chooseNext((writeBuffer), 0);
+
+		if (to_write != writeBuffer.end()) {
+			// candidate read found
+			found = true;
+		}
+	}
+
+    // if no write to an available rank is found then return
+    // at this point. There could be writes to the available ranks
+    // which are above the required threshold. However, to
+    // avoid adding more complexity to the code, return and wait
+    // for a refresh event to kick things into action again.
+    if (!found) {
+        //DPRINTF(DRAM, "No Writes Found in Write Buffer\n");
+        return;
+    }
+
+    auto dram_pkt = *to_write;
+
+    //cout << "nvm_wb_flushing: " << nvm_wb_flushing << " totalReadQueueSize: " << totalReadQueueSize << " nvm_totalWriteBufferSize: " << nvm_totalWriteBufferSize  << " address: " << dram_pkt->addr << endl;
+
+	doDRAMAccess(dram_pkt);
+
+	// removed write from queue, decrement count
+	// --dram_pkt->rankRef.writeEntries;
+
+	// Schedule write done event to decrement event count
+	// after the readyTime has been reached
+	// Only schedule latest write event to minimize events
+	// required; only need to ensure that final event scheduled covers
+	// the time that writes are outstanding and bus is active
+	// to holdoff power-down entry events
+	if (!dram_pkt->rankRef.writeDoneEvent.scheduled()) {
+		schedule(dram_pkt->rankRef.writeDoneEvent, dram_pkt->readyTime);
+		// New event, increment count
+		++dram_pkt->rankRef.outstandingEvents;
+
+	} else if (dram_pkt->rankRef.writeDoneEvent.when() <
+			   dram_pkt->readyTime) {
+
+		reschedule(dram_pkt->rankRef.writeDoneEvent, dram_pkt->readyTime);
+	}
+
+	eraseEntryNVMWriteBuffer(dram_pkt);
+
+	writeBuffer.erase(to_write);
+
+	// log the response
+	//logResponse(MemCtrl::WRITE, dram_pkt->masterId(),
+	//			dram_pkt->qosValue(), dram_pkt->getAddr(), 1,
+	//			dram_pkt->readyTime - dram_pkt->entryTime);
+
+	delete dram_pkt;
+
+	return;
+}
+
+void
+DRAMCtrl::processNVMNextReqEvent()
+{
+    // transition is handled by QoS algorithm if enabled
+    if (turnPolicy) {
+        // select bus state - only done if QoS algorithms are in use
+        busStateNext = selectNextBusState();
+    }
+
+    // detect bus state change
+    bool switched_cmd_type = (busState != busStateNext);
+    // record stats
+    recordTurnaroundStats();
+
+    //DPRINTF(DRAM, "QoS Turnarounds selected state %s %s\n",
+    //        (busState==MemCtrl::READ)?"READ":"WRITE",
+    //        switched_cmd_type?"[turnaround triggered]":"");
+
+    if (switched_cmd_type) {
+        if (busState == READ) {
+            //DPRINTF(DRAM,
+            //        "Switching to writes after %d reads with %d reads "
+            //        "waiting\n", readsThisTime, totalReadQueueSize);
+            rdPerTurnAround.sample(readsThisTime);
+            readsThisTime = 0;
+        } else {
+            //DPRINTF(DRAM,
+            //        "Switching to reads after %d writes with %d writes "
+            //        "waiting\n", writesThisTime, totalWriteQueueSize);
+            wrPerTurnAround.sample(writesThisTime);
+            writesThisTime = 0;
+        }
+    }
+
+    // updates current state
+    busState = busStateNext;
+
+    // check ranks for refresh/wakeup - uses busStateNext, so done after turnaround
+    // decisions
+    int busyRanks = 0;
+    for (auto r : ranks) {
+        if (!r->inRefIdleState()) {
+            if (r->pwrState != PWR_SREF) {
+                // rank is busy refreshing
+                DPRINTF(DRAMState, "Rank %d is not available\n", r->rank);
+                busyRanks++;
+
+                // let the rank know that if it was waiting to drain, it
+                // is now done and ready to proceed
+                r->checkDrainDone();
+            }
+
+            // check if we were in self-refresh and haven't started
+            // to transition out
+            if ((r->pwrState == PWR_SREF) && r->inLowPowerState) {
+                DPRINTF(DRAMState, "Rank %d is in self-refresh\n", r->rank);
+                // if we have commands queued to this rank and we don't have
+                // a minimum number of active commands enqueued,
+                // exit self-refresh
+                if (r->forceSelfRefreshExit()) {
+                    DPRINTF(DRAMState, "rank %d was in self refresh and"
+                           " should wake up\n", r->rank);
+                    //wake up from self-refresh
+                    r->scheduleWakeUpEvent(tXS);
+                    // things are brought back into action once a refresh is
+                    // performed after self-refresh
+                    // continue with selection for other ranks
+                }
+            }
+        }
+    }
+
+    if (busyRanks == ranksPerChannel) {
+        // if all ranks are refreshing wait for them to finish
+        // and stall this state machine without taking any further
+        // action, and do not schedule a new nextReqEvent
+        return;
+    }
+
+    // when we get here it is either a read or a write
+    if (busState == READ) {
+
+        // track if we should switch or not
+        bool switch_to_writes = false;
+
+        if (totalReadQueueSize == 0) {
+            // In the case there is no read request to go next,
+            // trigger writes if we have passed the low threshold (or
+            // if we are draining)
+            if (!(totalWriteQueueSize == 0) &&
+                (drainState() == DrainState::Draining ||
+                 totalWriteQueueSize > writeLowThreshold)) {
+
+                //DPRINTF(DRAM, "Switching to writes due to read queue empty\n");
+                switch_to_writes = true;
+            } else {
+                // check if we are drained
+                // not done draining until in PWR_IDLE state
+                // ensuring all banks are closed and
+                // have exited low power states
+                if (drainState() == DrainState::Draining &&
+                    respQueue.empty() && allRanksDrained()) {
+
+                    DPRINTF(Drain, "DRAM controller done draining\n");
+                    signalDrainDone();
+                }
+
+                // nothing to do, not even any point in scheduling an
+                // event for the next request
+                return;
+            }
+        } else {
+
+            bool read_found = false;
+            DRAMPacketQueue::iterator to_read;
+            uint8_t prio = numPriorities();
+
+            for (auto queue = readQueue.rbegin();
+                 queue != readQueue.rend(); ++queue) {
+
+                prio--;
+
+                DPRINTF(QOS,
+                        "DRAM controller checking READ queue [%d] priority [%d elements]\n",
+                        prio, queue->size());
+
+                // Figure out which read request goes next
+                // If we are changing command type, incorporate the minimum
+                // bus turnaround delay which will be tCS (different rank) case
+                to_read = chooseNext((*queue), switched_cmd_type ? tCS : 0);
+
+                if (to_read != queue->end()) {
+                    // candidate read found
+                    read_found = true;
+                    break;
+                }
+            }
+
+            // if no read to an available rank is found then return
+            // at this point. There could be writes to the available ranks
+            // which are above the required threshold. However, to
+            // avoid adding more complexity to the code, return and wait
+            // for a refresh event to kick things into action again.
+            if (!read_found) {
+                //DPRINTF(DRAM, "No Reads Found - exiting\n");
+                return;
+            }
+
+            auto dram_pkt = *to_read;
+
+            bool found = false;
+
+            //cout << "WB: checking for address: " << dram_pkt->addr << endl;
+            if(isInWriteBuffer.find(burstAlign(dram_pkt->addr)) != isInWriteBuffer.end())
+            {
+            	dram_pkt->readyTime = curTick() + 1;
+            	//cout << "WB serviced read address: " << dram_pkt->addr << " write buffer length: " << nvm_totalWriteBufferSize <<"("<<isInWriteBuffer.size()<<")"<< endl;
+            	servicedByWrB++;
+            	found = true;
+            }
+            else {
+            	assert(dram_pkt->rankRef.inRefIdleState());
+            	doDRAMAccess(dram_pkt);
+            }
+
+            // sanity check
+            assert(dram_pkt->size <= burstSize);
+            assert(dram_pkt->readyTime >= curTick());
+
+            // log the response
+            logResponse(MemCtrl::READ, (*to_read)->masterId(),
+                        dram_pkt->qosValue(), dram_pkt->getAddr(), 1,
+                        dram_pkt->readyTime - dram_pkt->entryTime);
+
+            if(found) {
+                --dram_pkt->rankRef.readEntries;
+                //DPRINTF(DRAM, "number of read entries for rank %d is %d\n",
+                //        dram_pkt->rank, dram_pkt->rankRef.readEntries);
+
+				if (dram_pkt->burstHelper) {
+					// it is a split packet
+					dram_pkt->burstHelper->burstsServiced++;
+					if (dram_pkt->burstHelper->burstsServiced ==
+						dram_pkt->burstHelper->burstCount) {
+						// we have now serviced all children packets of a system packet
+						// so we can now respond to the requester
+						// @todo we probably want to have a different front end and back
+						// end latency for split packets
+						accessAndRespond(dram_pkt->pkt, frontendLatency);
+						delete dram_pkt->burstHelper;
+						dram_pkt->burstHelper = NULL;
+					}
+				} else {
+					// it is not a split packet
+					accessAndRespond(dram_pkt->pkt, frontendLatency);
+				}
+            }
+            else {
+            	// Every respQueue which will generate an event, increment count
+				++dram_pkt->rankRef.outstandingEvents;
+
+				// Insert into response queue. It will be sent back to the
+				// requester at its readyTime
+				if (respQueue.empty()) {
+					assert(!respondEvent.scheduled());
+					schedule(respondEvent, dram_pkt->readyTime);
+				} else {
+					assert(respQueue.back()->readyTime <= dram_pkt->readyTime);
+					assert(respondEvent.scheduled());
+				}
+
+				respQueue.push_back(dram_pkt);
+            }
+
+            // we have so many writes that we have to transition
+            if (totalWriteQueueSize > writeHighThreshold) {
+                switch_to_writes = true;
+            }
+
+            // remove the request from the queue - the iterator is no longer valid .
+            readQueue[dram_pkt->qosValue()].erase(to_read);
+        }
+
+        // switching to writes, either because the read queue is empty
+        // and the writes have passed the low threshold (or we are
+        // draining), or because the writes hit the hight threshold
+        if (switch_to_writes) {
+            // transition to writing
+            busStateNext = WRITE;
+        }
+    } else {
+
+        bool write_found = false;
+        DRAMPacketQueue::iterator to_write;
+        uint8_t prio = numPriorities();
+
+        for (auto queue = writeQueue.rbegin();
+             queue != writeQueue.rend(); ++queue) {
+
+            prio--;
+
+            DPRINTF(QOS,
+                    "DRAM controller checking WRITE queue [%d] priority [%d elements]\n",
+                    prio, queue->size());
+
+            // If we are changing command type, incorporate the minimum
+            // bus turnaround delay
+            to_write = chooseNext((*queue),
+                                  switched_cmd_type ? std::min(tRTW, tCS) : 0);						// not required for NVM
+
+            if (to_write != queue->end()) {
+                write_found = true;
+                break;
+            }
+        }
+
+        // if there are no writes to a rank that is available to service
+        // requests (i.e. rank is in refresh idle state) are found then
+        // return. There could be reads to the available ranks. However, to
+        // avoid adding more complexity to the code, return at this point and
+        // wait for a refresh event to kick things into action again.
+        if (!write_found) {
+            //DPRINTF(DRAM, "No Writes Found - exiting\n");
+            return;
+        }
+
+        auto dram_pkt = *to_write;
+
+        bool removed = false, erase = false;
+
+        // check if found it can be merged with write buffer request
+        if(isInWriteBuffer.find(burstAlign(dram_pkt->addr)) != isInWriteBuffer.end())
+        {
+        	erase = true;
+        	removed = true;
+        }
+        else if(nvm_totalWriteBufferSize < nvm_write_buffer_size)
+        {
+			//DPRINTF(DRAM, "Adding to write buffer\n");
+			insertEntryNVMWriteBuffer(dram_pkt);
+
+			//cout << "WB insert write address: " << dram_pkt->addr << " wb size: " << nvm_totalWriteBufferSize << endl;
+			// Update stats
+			avgWrBLen = nvm_totalWriteBufferSize;
+
+			removed = true;
+        }
+
+        //assert(dram_pkt->rankRef.inRefIdleState());
+        // sanity check
+        //assert(dram_pkt->size <= burstSize);
+        //doDRAMAccess(dram_pkt);
+
+        if(removed) {
+
+        	dram_pkt->readyTime = curTick();
+
+        	// removed write from queue, decrement count
+            --dram_pkt->rankRef.writeEntries;
+			//isInWriteQueue.erase(burstAlign(dram_pkt->addr));
+            unordered_multiset<Addr>::iterator it = isInWriteQueue.find(burstAlign(dram_pkt->addr));
+            isInWriteQueue.erase(it);
+
+			// log the response
+			logResponse(MemCtrl::WRITE, dram_pkt->masterId(),
+						dram_pkt->qosValue(), dram_pkt->getAddr(), 1,
+						dram_pkt->readyTime - dram_pkt->entryTime);
+
+			// remove the request from the queue - the iterator is no longer valid
+			writeQueue[dram_pkt->qosValue()].erase(to_write);
+
+			//accessAndRespond(pkt, frontendLatency);
+        }
+
+        if(erase)
+        	delete dram_pkt;
+
+
+        // If we emptied the write queue, or got sufficiently below the
+        // threshold (using the minWritesPerSwitch as the hysteresis) and
+        // are not draining, or we have reads waiting and have done enough
+        // writes, then switch to reads.
+        bool below_threshold =
+            totalWriteQueueSize + minWritesPerSwitch < writeLowThreshold;
+
+        if (totalWriteQueueSize == 0 ||
+            (below_threshold && drainState() != DrainState::Draining) ||
+            (totalReadQueueSize && writesThisTime >= minWritesPerSwitch)) {
+
+            // turn the bus back around for reads again
+            busStateNext = READ;
+
+            // note that the we switch back to reads also in the idle
+            // case, which eventually will check for any draining and
+            // also pause any further scheduling if there is really
+            // nothing to do
+        }
+    }
+    // It is possible that a refresh to another rank kicks things back into
+    // action before reaching this point.
+    if (!nextReqEvent.scheduled())
+        schedule(nextReqEvent, std::max(nextReqTime, curTick()));
+
+    // If there is space available and we have writes waiting then let
+    // them retry. This is done here to ensure that the retry does not
+    // cause a nextReqEvent to be scheduled before we do so as part of
+    // the next request processing
+    if (retryWrReq && totalWriteQueueSize < writeBufferSize) {
+        retryWrReq = false;
+        port.sendRetryReq();
+    }
+
+}
+
+void
 DRAMCtrl::processNextReqEvent()
 {
+	if(nvm) {
+		processNVMNextReqEvent();
+		try_flush_wb();
+		return;
+	}
+
     // transition is handled by QoS algorithm if enabled
     if (turnPolicy) {
         // select bus state - only done if QoS algorithms are in use
@@ -1342,21 +3599,21 @@ DRAMCtrl::processNextReqEvent()
     // record stats
     recordTurnaroundStats();
 
-    DPRINTF(DRAM, "QoS Turnarounds selected state %s %s\n",
-            (busState==MemCtrl::READ)?"READ":"WRITE",
-            switched_cmd_type?"[turnaround triggered]":"");
+    //DPRINTF(DRAM, "QoS Turnarounds selected state %s %s\n",
+    //        (busState==MemCtrl::READ)?"READ":"WRITE",
+    //        switched_cmd_type?"[turnaround triggered]":"");
 
     if (switched_cmd_type) {
         if (busState == READ) {
-            DPRINTF(DRAM,
-                    "Switching to writes after %d reads with %d reads "
-                    "waiting\n", readsThisTime, totalReadQueueSize);
+            //DPRINTF(DRAM,
+            //        "Switching to writes after %d reads with %d reads "
+            //        "waiting\n", readsThisTime, totalReadQueueSize);
             rdPerTurnAround.sample(readsThisTime);
             readsThisTime = 0;
         } else {
-            DPRINTF(DRAM,
-                    "Switching to reads after %d writes with %d writes "
-                    "waiting\n", writesThisTime, totalWriteQueueSize);
+            //DPRINTF(DRAM,
+            //        "Switching to reads after %d writes with %d writes "
+            //        "waiting\n", writesThisTime, totalWriteQueueSize);
             wrPerTurnAround.sample(writesThisTime);
             writesThisTime = 0;
         }
@@ -1421,7 +3678,7 @@ DRAMCtrl::processNextReqEvent()
                 (drainState() == DrainState::Draining ||
                  totalWriteQueueSize > writeLowThreshold)) {
 
-                DPRINTF(DRAM, "Switching to writes due to read queue empty\n");
+                //DPRINTF(DRAM, "Switching to writes due to read queue empty\n");
                 switch_to_writes = true;
             } else {
                 // check if we are drained
@@ -1472,7 +3729,7 @@ DRAMCtrl::processNextReqEvent()
             // avoid adding more complexity to the code, return and wait
             // for a refresh event to kick things into action again.
             if (!read_found) {
-                DPRINTF(DRAM, "No Reads Found - exiting\n");
+                //DPRINTF(DRAM, "No Reads Found - exiting\n");
                 return;
             }
 
@@ -1506,11 +3763,17 @@ DRAMCtrl::processNextReqEvent()
 
             respQueue.push_back(dram_pkt);
 
-            // we have so many writes that we have to transition
-            if (totalWriteQueueSize > writeHighThreshold) {
-                switch_to_writes = true;
-            }
-
+            //if(!reram) {
+				// we have so many writes that we have to transition
+				if (totalWriteQueueSize > writeHighThreshold) {
+					switch_to_writes = true;
+				}
+            //}
+            //else {
+            //	if (totalWriteQueueSize >= 1) {
+            //		switch_to_writes = true;
+            //	}
+            //}
             // remove the request from the queue - the iterator is no longer valid .
             readQueue[dram_pkt->qosValue()].erase(to_read);
         }
@@ -1554,7 +3817,7 @@ DRAMCtrl::processNextReqEvent()
         // avoid adding more complexity to the code, return at this point and
         // wait for a refresh event to kick things into action again.
         if (!write_found) {
-            DPRINTF(DRAM, "No Writes Found - exiting\n");
+            //DPRINTF(DRAM, "No Writes Found - exiting\n");
             return;
         }
 
@@ -1566,38 +3829,76 @@ DRAMCtrl::processNextReqEvent()
 
         doDRAMAccess(dram_pkt);
 
-        // removed write from queue, decrement count
-        --dram_pkt->rankRef.writeEntries;
-
-        // Schedule write done event to decrement event count
-        // after the readyTime has been reached
-        // Only schedule latest write event to minimize events
-        // required; only need to ensure that final event scheduled covers
-        // the time that writes are outstanding and bus is active
-        // to holdoff power-down entry events
-        if (!dram_pkt->rankRef.writeDoneEvent.scheduled()) {
-            schedule(dram_pkt->rankRef.writeDoneEvent, dram_pkt->readyTime);
-            // New event, increment count
+        /*if(reram) {
+            // Every respQueue which will generate an event, increment count
             ++dram_pkt->rankRef.outstandingEvents;
 
-        } else if (dram_pkt->rankRef.writeDoneEvent.when() <
-                   dram_pkt->readyTime) {
+            // sanity check
+            assert(dram_pkt->size <= burstSize);
+            assert(dram_pkt->readyTime >= curTick());
+
+            // Insert into response queue. It will be sent back to the
+            // requester at its readyTime
+            if (respQueue.empty()) {
+                assert(!respondEvent.scheduled());
+                schedule(respondEvent, dram_pkt->readyTime);
+            } else {
+                assert(respQueue.back()->readyTime <= dram_pkt->readyTime);
+                assert(respondEvent.scheduled());
+            }
+
+            respQueue.push_back(dram_pkt);
+
+			//isInWriteQueue.erase(burstAlign(dram_pkt->addr));
+			unordered_multiset<Addr>::iterator it = isInWriteQueue.find(burstAlign(dram_pkt->addr));
+			isInWriteQueue.erase(it);
+
+			// log the response
+			logResponse(MemCtrl::WRITE, dram_pkt->masterId(),
+						dram_pkt->qosValue(), dram_pkt->getAddr(), 1,
+						dram_pkt->readyTime - dram_pkt->entryTime);
+
+			// remove the request from the queue - the iterator is no longer valid
+			writeQueue[dram_pkt->qosValue()].erase(to_write);
 
-            reschedule(dram_pkt->rankRef.writeDoneEvent, dram_pkt->readyTime);
         }
+        else {*/
+			// removed write from queue, decrement count
+			--dram_pkt->rankRef.writeEntries;
+
+			// Schedule write done event to decrement event count
+			// after the readyTime has been reached
+			// Only schedule latest write event to minimize events
+			// required; only need to ensure that final event scheduled covers
+			// the time that writes are outstanding and bus is active
+			// to holdoff power-down entry events
+			if (!dram_pkt->rankRef.writeDoneEvent.scheduled()) {
+				schedule(dram_pkt->rankRef.writeDoneEvent, dram_pkt->readyTime);
+				// New event, increment count
+				++dram_pkt->rankRef.outstandingEvents;
 
-        isInWriteQueue.erase(burstAlign(dram_pkt->addr));
+			} else if (dram_pkt->rankRef.writeDoneEvent.when() <
+					   dram_pkt->readyTime) {
 
-        // log the response
-        logResponse(MemCtrl::WRITE, dram_pkt->masterId(),
-                    dram_pkt->qosValue(), dram_pkt->getAddr(), 1,
-                    dram_pkt->readyTime - dram_pkt->entryTime);
+				reschedule(dram_pkt->rankRef.writeDoneEvent, dram_pkt->readyTime);
+			}
 
+			//isInWriteQueue.erase(burstAlign(dram_pkt->addr));
+			unordered_multiset<Addr>::iterator it = isInWriteQueue.find(burstAlign(dram_pkt->addr));
+			isInWriteQueue.erase(it);
 
-        // remove the request from the queue - the iterator is no longer valid
-        writeQueue[dram_pkt->qosValue()].erase(to_write);
+			// log the response
+			logResponse(MemCtrl::WRITE, dram_pkt->masterId(),
+						dram_pkt->qosValue(), dram_pkt->getAddr(), 1,
+						dram_pkt->readyTime - dram_pkt->entryTime);
 
-        delete dram_pkt;
+
+			// remove the request from the queue - the iterator is no longer valid
+			writeQueue[dram_pkt->qosValue()].erase(to_write);
+
+			//writeResponseQueue.push_back(dram_pkt);
+			delete dram_pkt;
+        //}
 
         // If we emptied the write queue, or got sufficiently below the
         // threshold (using the minWritesPerSwitch as the hysteresis) and
@@ -1606,18 +3907,26 @@ DRAMCtrl::processNextReqEvent()
         bool below_threshold =
             totalWriteQueueSize + minWritesPerSwitch < writeLowThreshold;
 
-        if (totalWriteQueueSize == 0 ||
-            (below_threshold && drainState() != DrainState::Draining) ||
-            (totalReadQueueSize && writesThisTime >= minWritesPerSwitch)) {
-
-            // turn the bus back around for reads again
-            busStateNext = READ;
-
-            // note that the we switch back to reads also in the idle
-            // case, which eventually will check for any draining and
-            // also pause any further scheduling if there is really
-            // nothing to do
-        }
+        //if(!reram) {
+			if (totalWriteQueueSize == 0 ||
+				(below_threshold && drainState() != DrainState::Draining) ||
+				(totalReadQueueSize && writesThisTime >= minWritesPerSwitch)) {
+
+				// turn the bus back around for reads again
+				busStateNext = READ;
+
+				// note that the we switch back to reads also in the idle
+				// case, which eventually will check for any draining and
+				// also pause any further scheduling if there is really
+				// nothing to do
+			}
+        //}
+		//else {
+		//	if (totalWriteQueueSize == 0 ||
+		//		(drainState() != DrainState::Draining) ||
+		//		(totalReadQueueSize && writesThisTime >= 1))
+		//		busStateNext = READ;
+		//}
     }
     // It is possible that a refresh to another rank kicks things back into
     // action before reaching this point.
@@ -1795,7 +4104,7 @@ DRAMCtrl::Rank::checkDrainDone()
     // if this rank was waiting to drain it is now able to proceed to
     // precharge
     if (refreshState == REF_DRAIN) {
-        DPRINTF(DRAM, "Refresh drain done, now precharging\n");
+        //DPRINTF(DRAM, "Refresh drain done, now precharging\n");
 
         refreshState = REF_PD_EXIT;
 
@@ -1886,6 +4195,9 @@ DRAMCtrl::Rank::processWriteDoneEvent()
 void
 DRAMCtrl::Rank::processRefreshEvent()
 {
+	//if(nvm)
+	//	return;
+
     // when first preparing the refresh, remember when it was due
     if ((refreshState == REF_IDLE) || (refreshState == REF_SREF_EXIT)) {
         // remember when the refresh is due
@@ -1898,7 +4210,7 @@ DRAMCtrl::Rank::processRefreshEvent()
         // power down and self-refresh are not entered
         ++outstandingEvents;
 
-        DPRINTF(DRAM, "Refresh due\n");
+        //DPRINTF(DRAM, "Refresh due\n");
     }
 
     // let any scheduled read or write to the same rank go ahead,
@@ -1911,7 +4223,7 @@ DRAMCtrl::Rank::processRefreshEvent()
             && (memory.nextReqEvent.scheduled())) {
             // hand control over to the request loop until it is
             // evaluated next
-            DPRINTF(DRAM, "Refresh awaiting draining\n");
+            //DPRINTF(DRAM, "Refresh awaiting draining\n");
 
             return;
         } else {
@@ -1924,7 +4236,7 @@ DRAMCtrl::Rank::processRefreshEvent()
         // if rank was sleeping and we have't started exit process,
         // wake-up for refresh
         if (inLowPowerState) {
-            DPRINTF(DRAM, "Wake Up for refresh\n");
+            //DPRINTF(DRAM, "Wake Up for refresh\n");
             // save state and return after refresh completes
             scheduleWakeUpEvent(memory.tXP);
             return;
@@ -1939,7 +4251,7 @@ DRAMCtrl::Rank::processRefreshEvent()
         if (numBanksActive != 0) {
             // at the moment, we use a precharge all even if there is
             // only a single bank open
-            DPRINTF(DRAM, "Precharging all\n");
+            //DPRINTF(DRAM, "Precharging all\n");
 
             // first determine when we can precharge
             Tick pre_at = curTick();
@@ -1973,7 +4285,7 @@ DRAMCtrl::Rank::processRefreshEvent()
         } else if ((pwrState == PWR_IDLE) && (outstandingEvents == 1))  {
             // Banks are closed, have transitioned to IDLE state, and
             // no outstanding ACT,RD/WR,Auto-PRE sequence scheduled
-            DPRINTF(DRAM, "All banks already precharged, starting refresh\n");
+            //DPRINTF(DRAM, "All banks already precharged, starting refresh\n");
 
             // go ahead and kick the power state machine into gear since
             // we are already idle
@@ -2266,8 +4578,8 @@ DRAMCtrl::Rank::processPowerEvent()
 
         // completed refresh event, ensure next request is scheduled
         if (!memory.nextReqEvent.scheduled()) {
-            DPRINTF(DRAM, "Scheduling next request after refreshing"
-                           " rank %d\n", rank);
+            //DPRINTF(DRAM, "Scheduling next request after refreshing"
+            //               " rank %d\n", rank);
             schedule(memory.nextReqEvent, curTick());
         }
     }
@@ -2405,7 +4717,7 @@ DRAMCtrl::Rank::updatePowerStats()
 void
 DRAMCtrl::Rank::computeStats()
 {
-    DPRINTF(DRAM,"Computing stats due to a dump callback\n");
+    //DPRINTF(DRAM,"Computing stats due to a dump callback\n");
 
     // Update the stats
     updatePowerStats();
@@ -2508,6 +4820,158 @@ DRAMCtrl::regStats()
 
     registerResetCallback(new MemResetCallback(this));
 
+
+
+
+//       MTmsr
+ //       .name("mtmissrate")
+  //      .desc("miss rate mt cache");
+//
+//
+//       MTmisses
+//        .name("mtmisses")
+//       .desc("miss count mt cache");
+
+
+//       MThits
+//        .name("mthits")
+//        .desc("hitcoutn mt cache");
+//
+
+//       CtLife
+//        .name("ctlife")
+//        .desc("miss rate counter cache");
+
+
+//       Ctreadonly
+//        .name("ctreadonly")
+//        .desc("miss rate counter cache");
+
+
+//       Ctmodify
+//        .name("Ctmodify")
+//        .desc("miss rate counter cache");
+
+    llcMissrate
+		.name(name() + ".llcMissrate")
+		.desc("Last Level Cache miss rate");
+
+    numWriteReqs
+        .name(name() + ".numWriteReqs")
+        .desc("Number of write requests sent to NVM");
+
+    bitFlips
+        .name(name() + ".bitFlips")
+        .desc("Number of bit flips");
+
+    avgBitFlipsPerWrite
+        .name(name() + ".avgBitFlipsPerWrite")
+        .desc("Number of average bit flips per write")
+        .precision(2);
+
+    avgBitFlipsPerWrite = bitFlips / (numWriteReqs);
+
+    totalWriteSlots
+        .name(name() + ".totalWriteSlots")
+        .desc("Number of total write slots used to perform a write operation");
+
+    avgWriteSlots
+        .name(name() + ".avgWriteSlots")
+        .desc("Number of average write slots used to perform a write operation")
+        .precision(2);
+
+    avgWriteSlots = totalWriteSlots / (numWriteReqs);
+
+    totalEncryptValSkpd
+        .name(name() + ".totalEncryptValSkpd")
+        .desc("Number of total encrypted values skipped");
+
+    avgEncryptValSkpd
+        .name(name() + ".avgEncryptValSkpd")
+        .desc("Number of average encrypted values skipped")
+        .precision(2);
+
+    avgEncryptValSkpd = totalEncryptValSkpd / (numWriteReqs);
+
+    totalEncryptValTested
+        .name(name() + ".totalEncryptValTested")
+        .desc("Number of total encrypted values tested");
+
+    avgEncryptValTested
+        .name(name() + ".avgEncryptValTested")
+        .desc("Number of average encrypted values tested")
+        .precision(2);
+
+    avgEncryptValTested = totalEncryptValTested / (numWriteReqs);
+
+    numOfTimesTotalCacheLineEncrp
+    	.name(name() + ".numOfTimesTotalCacheLineEncrp")
+        .desc("Number of times total cacheline is encrypted");
+
+    numOfTimesPartialCacheLineEncrp
+		.name(name() + ".numOfTimesPartialCacheLineEncrp")
+		.desc("Number of times partial cacheline is encrypted");
+
+    numCacheLinesWritten
+		.name(name() + ".numCacheLinesWritten")
+		.desc("Number of cache blocks written");
+
+    numCacheLinesWritten_4
+		.name(name() + ".numCacheLinesWritten_4")
+		.desc("Number of cache blocks written");
+
+    numCacheLinesWritten_8
+		.name(name() + ".numCacheLinesWritten_8")
+		.desc("Number of cache blocks written");
+
+    numCacheLinesWritten_16
+		.name(name() + ".numCacheLinesWritten_16")
+		.desc("Number of cache blocks written");
+
+    numCacheLinesWritten_32
+		.name(name() + ".numCacheLinesWritten_32")
+		.desc("Number of cache blocks written");
+
+    numCacheLinesWritten_64
+		.name(name() + ".numCacheLinesWritten_64")
+		.desc("Number of cache blocks written");
+
+    totalModifiedWords
+		.name(name() + ".totalModifiedWords")
+		.desc("Number of total modified words");
+
+    totalCacheLineWrites
+		.name(name() + ".totalCacheLineWrites")
+		.desc("Number of total writes");
+
+    avgModifiedWordsPerWrite
+        .name(name() + ".avgModifiedWordsPerWrite")
+        .desc("Number of average modified words per write")
+        .precision(2);
+
+    avgModifiedWordsPerWrite = totalModifiedWords / (totalCacheLineWrites);
+
+    writesSavedAtCC
+		.name(name() + ".writesSavedAtCC")
+		.desc("Number of writes due to counter cache");
+
+    writesWithOutWordChanges
+		.name(name() + ".writesWithOutWordChanges")
+		.desc("Number of same writes to the memory");
+
+    actualWritesWithOutWordChanges
+		.name(name() + ".actualWritesWithOutWordChanges")
+		.desc("Number of same writes to the memory");
+
+    servicedByWrB
+        .name(name() + ".servicedByWrB")
+        .desc("Number of DRAM read bursts serviced by the write buffer");
+
+    avgWrBLen
+        .name(name() + ".avgWrBLen")
+        .desc("Average write buffer length")
+        .precision(2);
+
     readReqs
         .name(name() + ".readReqs")
         .desc("Number of read requests accepted");
@@ -2877,8 +5341,8 @@ DRAMCtrl::drain()
         for (auto r : ranks) {
             // force self-refresh exit, which in turn will issue auto-refresh
             if (r->pwrState == PWR_SREF) {
-                DPRINTF(DRAM,"Rank%d: Forcing self-refresh wakeup in drain\n",
-                        r->rank);
+                //DPRINTF(DRAM,"Rank%d: Forcing self-refresh wakeup in drain\n",
+                //        r->rank);
                 r->scheduleWakeUpEvent(tXS);
             }
         }
@@ -2969,3 +5433,4 @@ DRAMCtrlParams::create()
 {
     return new DRAMCtrl(this);
 }
+
diff --git a/src/mem/dram_ctrl.hh b/src/mem/dram_ctrl.hh
index a5f2fbe..05454bb 100644
--- a/src/mem/dram_ctrl.hh
+++ b/src/mem/dram_ctrl.hh
@@ -58,6 +58,10 @@
 #include <string>
 #include <unordered_set>
 #include <vector>
+#include <map>
+#include <random>
+#include <boost/multiprecision/cpp_int.hpp>
+#include <boost/random.hpp>
 
 #include "base/callback.hh"
 #include "base/statistics.hh"
@@ -95,6 +99,10 @@
  * similar to that described in "Optimized Active and Power-Down Mode
  * Refresh Control in 3D-DRAMs" by Jung et al, VLSI-SoC, 2014.
  */
+
+using namespace boost::multiprecision;
+using namespace boost::random;
+
 class DRAMCtrl : public QoS::MemCtrl
 {
 
@@ -642,7 +650,8 @@ class DRAMCtrl : public QoS::MemCtrl
         Tick readyTime;
 
         /** This comes from the outside world */
-        const PacketPtr pkt;
+        //const PacketPtr pkt;
+        PacketPtr pkt;
 
         /** MasterID associated with the packet */
         const MasterID _masterId;
@@ -689,6 +698,11 @@ class DRAMCtrl : public QoS::MemCtrl
         uint8_t _qosValue;
 
         /**
+         *	delay due to bit flips
+         */
+        Tick dly_due_to_bit_flips;
+
+        /**
          * Set the packet QoS value
          * (interface compatibility with Packet)
          */
@@ -753,6 +767,17 @@ class DRAMCtrl : public QoS::MemCtrl
      * processRespondEvent is called; no parameters are allowed
      * in these methods
      */
+    void processSendPacketEvent();
+    EventFunctionWrapper XbarSendPacketEvent;
+
+    void processFakeEvent();
+    EventFunctionWrapper XbarFakeEvent;
+
+    Tick getDelayDueToBitFlips(PacketPtr pkt);
+    void insertEntryNVMWriteBuffer(DRAMPacket *dram_pkt);
+    void eraseEntryNVMWriteBuffer(DRAMPacket *dram_pkt);
+    void try_flush_wb();
+    void processNVMNextReqEvent();
     void processNextReqEvent();
     EventFunctionWrapper nextReqEvent;
 
@@ -934,7 +959,48 @@ class DRAMCtrl : public QoS::MemCtrl
      * location we never have more than one address to the same burst
      * address.
      */
-    std::unordered_set<Addr> isInWriteQueue;
+    //std::unordered_set<Addr> isInWriteQueue;
+    std::unordered_multiset<Addr> isInWriteQueue;
+
+    /*
+     * Enable Non-Volatile Memory
+     */
+    uint32_t nvm;
+
+    /*
+     * Non-Volatile Memory Write Buffer size
+     */
+    uint32_t nvm_write_buffer_size;
+
+    uint64_t nvm_totalWriteBufferSize;
+
+    bool nvm_wb_flushing;
+
+    uint64_t nvm_wb_max_threshold;
+
+    uint64_t nvm_wb_min_threshold;
+
+    /**
+     * The controller's write buffer for NVM
+     */
+    DRAMPacketQueue writeBuffer;
+
+    uint32_t reram;
+
+    uint32_t dynamicRowReMapping;
+
+    uint32_t enableEncryption;
+
+    uint32_t bitFlipsDelay;
+
+    /**
+     * To avoid iterating over the write buffer to check for
+     * overlapping transactions, maintain a set of burst addresses
+     * that are currently queued. Since we merge writes to the same
+     * location we never have more than one address to the same burst
+     * address.
+     */
+    std::unordered_set<Addr> isInWriteBuffer;
 
     /**
      * Response queue where read packets wait after we're done working
@@ -951,6 +1017,22 @@ class DRAMCtrl : public QoS::MemCtrl
      */
     std::vector<Rank*> ranks;
 
+    /*
+     * Random value for encryption
+     */
+    typedef independent_bits_engine<mt19937, 512, uint512_t> generator512_type;
+    generator512_type gen512;
+    typedef independent_bits_engine<mt19937, 256, uint256_t> generator256_type;
+    generator256_type gen256;
+    typedef independent_bits_engine<mt19937, 128, uint128_t> generator128_type;
+	generator128_type gen128;
+    typedef independent_bits_engine<mt19937, 64, uint64_t> generator64_type;
+	generator64_type gen64;
+	typedef independent_bits_engine<mt19937, 32, uint32_t> generator32_type;
+	generator32_type gen32;
+	typedef independent_bits_engine<mt19937, 16, uint16_t> generator16_type;
+	generator16_type gen16;
+
     /**
      * The following are basic design parameters of the memory
      * controller, and are initialized based on parameter values.
@@ -1052,6 +1134,34 @@ class DRAMCtrl : public QoS::MemCtrl
     Tick nextReqTime;
 
     // All statistics that the model needs to capture
+
+    Stats::Scalar llcMissrate;
+
+    Stats::Scalar bitFlips;
+    Stats::Formula avgBitFlipsPerWrite;
+    Stats::Scalar totalWriteSlots;
+    Stats::Formula avgWriteSlots;
+    Stats::Scalar totalEncryptValSkpd;
+    Stats::Formula avgEncryptValSkpd;
+    Stats::Scalar totalEncryptValTested;
+    Stats::Formula avgEncryptValTested;
+    Stats::Scalar numOfTimesTotalCacheLineEncrp;
+    Stats::Scalar numOfTimesPartialCacheLineEncrp;
+    Stats::Scalar numCacheLinesWritten;
+    Stats::Scalar numCacheLinesWritten_4;
+    Stats::Scalar numCacheLinesWritten_8;
+    Stats::Scalar numCacheLinesWritten_16;
+    Stats::Scalar numCacheLinesWritten_32;
+    Stats::Scalar numCacheLinesWritten_64;
+    Stats::Scalar totalModifiedWords;
+    Stats::Scalar totalCacheLineWrites;
+    Stats::Formula avgModifiedWordsPerWrite;
+    Stats::Scalar writesSavedAtCC;
+    Stats::Scalar writesWithOutWordChanges;
+    Stats::Scalar actualWritesWithOutWordChanges;
+    Stats::Scalar numWriteReqs;
+
+
     Stats::Scalar readReqs;
     Stats::Scalar writeReqs;
     Stats::Scalar readBursts;
@@ -1062,6 +1172,7 @@ class DRAMCtrl : public QoS::MemCtrl
     Stats::Scalar bytesReadSys;
     Stats::Scalar bytesWrittenSys;
     Stats::Scalar servicedByWrQ;
+    Stats::Scalar servicedByWrB;
     Stats::Scalar mergedWrBursts;
     Stats::Scalar neitherReadNorWrite;
     Stats::Vector perBankRdBursts;
@@ -1120,6 +1231,7 @@ class DRAMCtrl : public QoS::MemCtrl
     // Average queue lengths
     Stats::Average avgRdQLen;
     Stats::Average avgWrQLen;
+    Stats::Average avgWrBLen;
 
     // Row hit count and rate
     Stats::Scalar readRowHits;
@@ -1140,6 +1252,10 @@ class DRAMCtrl : public QoS::MemCtrl
     /** The time when stats were last reset used to calculate average power */
     Tick lastStatsResetTick;
 
+    uint64_t encryptionMethod;
+    uint64_t tsecme_encrption_thresh;
+    uint64_t deuce_epoch;
+
     /**
      * Upstream caches need this packet until true is returned, so
      * hold it for deletion until a subsequent call
@@ -1170,6 +1286,8 @@ class DRAMCtrl : public QoS::MemCtrl
 
   public:
 
+
+
     void regStats() override;
 
     DRAMCtrl(const DRAMCtrlParams* p);
@@ -1182,7 +1300,26 @@ class DRAMCtrl : public QoS::MemCtrl
     virtual void init() override;
     virtual void startup() override;
     virtual void drainResume() override;
-
+/*Stats::Scalar MTmsr;
+Stats::Scalar MTmisses;
+Stats::Scalar MThits;
+Stats::Scalar CtLife;
+Stats::Scalar Ctreadonly;
+Stats::Scalar Ctmodify;
+    Stats::Scalar CacheMissRate;
+    Stats::Scalar mtmissrate;
+    Stats::Scalar MTshadowCount;
+    Stats::Scalar CTRshadow;
+    Stats::Scalar CTROverflow;
+    Stats::Scalar L1Overflow;
+    Stats::Scalar L2Overflow;
+    Stats::Scalar L3Overflow;
+    Stats::Scalar L4Overflow;
+    Stats::Scalar L5Overflow;
+    Stats::Scalar L6Overflow;
+    Stats::Scalar L7Overflow;
+    Stats::Scalar L8Overflow;
+*/
     /**
      * Return true once refresh is complete for all ranks and there are no
      * additional commands enqueued.  (only evaluated when draining)
@@ -1199,7 +1336,12 @@ class DRAMCtrl : public QoS::MemCtrl
     Tick recvAtomic(PacketPtr pkt);
     void recvFunctional(PacketPtr pkt);
     bool recvTimingReq(PacketPtr pkt);
+    bool actualRcvTiming(PacketPtr pkt);
+    void sendDummyRead();
+    void sendDummyWrite();
+    void sendDummyWrite(uint64_t extra_latency);
 
 };
 
 #endif //__MEM_DRAM_CTRL_HH__
+
diff --git a/src/mem/packet.cc b/src/mem/packet.cc
index 866bc90..500f2d3 100644
--- a/src/mem/packet.cc
+++ b/src/mem/packet.cc
@@ -91,7 +91,7 @@ MemCmd::commandInfo[] =
     { SET2(IsWrite, IsResponse), InvalidCmd, "WriteResp" },
     /* WritebackDirty */
     { SET5(IsWrite, IsRequest, IsEviction, HasData, FromCache),
-            InvalidCmd, "WritebackDirty" },
+    		InvalidCmd, "WritebackDirty" },
     /* WritebackClean - This allows the upstream cache to writeback a
      * line to the downstream cache without it being considered
      * dirty. */
@@ -438,3 +438,4 @@ Packet::PrintReqState::printObj(Printable *obj)
     printLabels();
     obj->print(os, verbosity, curPrefix());
 }
+
diff --git a/src/mem/packet.hh b/src/mem/packet.hh
index 0f45a7b..d5fd3d9 100644
--- a/src/mem/packet.hh
+++ b/src/mem/packet.hh
@@ -379,6 +379,8 @@ class Packet : public Printable
      */
     uint32_t payloadDelay;
 
+    uint64_t dlyDueToBitFips;
+
     /**
      * A virtual base opaque structure used to hold state associated
      * with the packet (e.g., an MSHR), specific to a MemObject that
@@ -782,6 +784,9 @@ class Packet : public Printable
             size = req->getSize();
             flags.set(VALID_SIZE);
         }
+
+        for(int i=0; i<32; i++)
+			modifiedWordsFlag[i] = 0;
     }
 
     /**
@@ -802,6 +807,9 @@ class Packet : public Printable
         }
         size = _blkSize;
         flags.set(VALID_SIZE);
+
+        for(int i=0; i<32; i++)
+        	modifiedWordsFlag[i] = 0;
     }
 
     /**
@@ -842,6 +850,9 @@ class Packet : public Printable
                 allocate();
             }
         }
+
+        for(int i=0; i<32; i++)
+			modifiedWordsFlag[i] = pkt->modifiedWordsFlag[i];
     }
 
     /**
@@ -1171,6 +1182,61 @@ class Packet : public Printable
         }
     }
 
+    bool modifiedWordsFlag[32];
+
+    /** set modified Words Flag*/
+    void
+    setModifiedWordsFlagFromBlock(bool *mwf) {
+    	for(int i=0; i<32; i++) {
+    		modifiedWordsFlag[i] = mwf[i];
+    	}
+    	//std::cout << "setModifiedWordsFlagFromBlock" << std::endl;
+    }
+
+    void
+    setModifiedWordsFlagInBlock(bool *mwf) {
+    	for(int i=0; i<32; i++) {
+    		mwf[i] = modifiedWordsFlag[i];
+    	}
+    	//std::cout << "setModifiedWordsFlagInBlock" << std::endl;
+    }
+
+    void
+    writeModifiedWordsFlagToBlock(uint8_t *blk_data, int blkSize, bool *mwf)
+    {
+    	//std::vector<uint16_t> pkt_data_words(getSize());
+    	//std::vector<uint16_t> blk_data_words(getSize());
+
+    	//std::memcpy(&blk_data_words[0], blk_data + getOffset(blkSize), getSize());
+    	//std::memcpy(&pkt_data_words[0], getConstPtr<uint8_t>(), getSize());
+
+    	int word = getOffset(64)/2;
+		for (int i=0; i<getSize()/2; i++){
+			mwf[word] = 1;
+			word++;
+		}
+
+		//std::cout << "address: " << getBlockAddr(64) <<"("<< getAddr()<<")"<<"("<< getOffset(64)<<")"<<"("<< getSize()<<")";
+		//for (int i=0; i<32; i++) {
+		//	std::cout << ":" << mwf[i];
+		//}
+		//std::cout << "" << std::endl;
+		//std::cout << "address: " << getBlockAddr(64) <<"("<< getAddr()<<")"<<"("<< getOffset(64)<<")"<<"("<< getSize()<<")";
+		//for (int i=0; i<32; i++) {
+		//	std::cout << ":" << blk_data_words[i] << "-" << pkt_data_words[i];
+		//}
+		//std::cout << "" << std::endl;
+    	//std::cout << "writeModifiedWordsFlagToBlock" << std::endl;
+    }
+
+    /** Clear Modified Words Flag*/
+    void
+    clearModifiedWordsFlag() {
+    	for(int i=0; i<32; i++)
+    		modifiedWordsFlag[i] = 1;
+    	//std::cout << "clearModifiedWordsFlag" << std::endl;
+    }
+
     /** @} */
 
   private: // Private data accessor methods
@@ -1267,3 +1333,4 @@ class Packet : public Printable
 };
 
 #endif //__MEM_PACKET_HH
+
diff --git a/src/mem/physical.cc b/src/mem/physical.cc
index 2806204..804eefc 100644
--- a/src/mem/physical.cc
+++ b/src/mem/physical.cc
@@ -212,16 +212,29 @@ PhysicalMemory::createBackingStore(AddrRange range,
               range.to_string());
     }
 
+    uint8_t* pmemEncrypt = (uint8_t*) mmap(NULL, range.size(),
+                                    PROT_READ | PROT_WRITE,
+                                    map_flags, -1, 0);
+
+    if (pmemEncrypt == (uint8_t*) MAP_FAILED) {
+        perror("mmap");
+        fatal("Could not mmap %d bytes for range %s!\n", range.size(),
+              range.to_string());
+    }
+
     // remember this backing store so we can checkpoint it and unmap
     // it appropriately
     backingStore.emplace_back(range, pmem,
                               conf_table_reported, in_addr_map, kvm_map);
 
+    backingStore.emplace_back(range, pmemEncrypt,
+                              conf_table_reported, in_addr_map, kvm_map);
+
     // point the memories to their backing store
     for (const auto& m : _memories) {
         DPRINTF(AddrRanges, "Mapping memory %s to backing store\n",
                 m->name());
-        m->setBackingStore(pmem);
+        m->setBackingStore(pmem,pmemEncrypt);
     }
 }
 
@@ -454,3 +467,4 @@ PhysicalMemory::unserializeStore(CheckpointIn &cp)
         fatal("Close failed on physical memory checkpoint file '%s'\n",
               filename);
 }
+
